## Transformer
<div align="center">
<img width="300" height="400" alt="image" src="https://github.com/user-attachments/assets/8d064b02-6166-47ec-bfc6-fb031f94192c" />  
</div>

- é‡è¦æ€§ï¼šTransformer æ˜¯ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„æ ¸å¿ƒï¼Œé©±åŠ¨äº† ChatGPT ç­‰å¤§æ¨¡å‹ï¼Œä»£è¡¨æ·±åº¦å­¦ä¹ çš„å‰æ²¿ã€‚
- æ ¸å¿ƒæ¦‚å¿µï¼š
Transformer ä½¿ç”¨â€œæ³¨æ„åŠ›æœºåˆ¶â€ï¼ˆAttentionï¼‰ï¼Œå…³æ³¨è¾“å…¥ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼ˆå¦‚å¥å­ä¸­çš„å…³é”®å•è¯ï¼‰ã€‚  
æ¯” RNN æ›´é«˜æ•ˆï¼Œé€‚åˆå¤„ç†é•¿åºåˆ—ã€‚  
- åº”ç”¨ï¼šèŠå¤©æœºå™¨äººï¼ˆå¦‚ Grokï¼‰ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€‚
 ä¸ºä»€ä¹ˆæ•™ï¼šTransformer ä»£è¡¨ AI çš„æœ€æ–°è¿›å±•ã€‚


## Transformerçš„æ•°å­¦æè¿°
Transformeræ¶æ„æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„æ ¸å¿ƒæ¨¡å‹ï¼Œæœ€åˆç”±Vaswaniç­‰äººåœ¨2017å¹´è®ºæ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºã€‚ä»¥ä¸‹æ˜¯å…¶æ•°å­¦æè¿°ï¼Œæ¶µç›–ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬è¾“å…¥è¡¨ç¤ºã€æ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç ã€å‰é¦ˆç½‘ç»œå’Œå±‚å½’ä¸€åŒ–ç­‰ã€‚    
### 1. æ•´ä½“æ¶æ„  
Transformerç”±ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ç»„æˆï¼Œæ¯éƒ¨åˆ†åŒ…å«å¤šä¸ªå †å çš„å±‚ï¼ˆé€šå¸¸æ˜¯ $ N $ å±‚ï¼‰ã€‚ç¼–ç å™¨å¤„ç†è¾“å…¥åºåˆ—ï¼Œè§£ç å™¨ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œå–ä»£äº†ä¼ ç»Ÿå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„åºåˆ—å¤„ç†æ–¹å¼ã€‚  
è¾“å…¥è¡¨ç¤º  
è¾“å…¥åºåˆ—ï¼ˆå¦‚å•è¯æˆ–æ ‡è®°ï¼‰é¦–å…ˆè¢«è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼š
<img width="1390" height="474" alt="image" src="https://github.com/user-attachments/assets/fb62a0e3-59d9-41e0-b322-d4d5c2c06904" />


* **è¯åµŒå…¥**ï¼šå°†æ¯ä¸ªè¯æ˜ å°„ä¸ºå›ºå®šç»´åº¦çš„å‘é‡ $x_i \in \mathbb{R}^d$ï¼Œé€šå¸¸é€šè¿‡åµŒå…¥çŸ©é˜µ
  $E \in \mathbb{R}^{|V|\times d}$ å®ç°ï¼Œå…¶ä¸­ $|V|$ æ˜¯è¯æ±‡è¡¨å¤§å°ï¼Œ $d$ æ˜¯åµŒå…¥ç»´åº¦ã€‚

* **ä½ç½®ç¼–ç **ï¼šç”±äº Transformer ä¸å…·å¤‡åºåˆ—é¡ºåºä¿¡æ¯ï¼Œéœ€åŠ å…¥ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ä»¥æ•æ‰è¯çš„ä½ç½®ã€‚
  ä½ç½®ç¼–ç  $PE$ å¯é€šè¿‡å›ºå®šå…¬å¼ç”Ÿæˆï¼š

  $$
  PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad 
  PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
  $$

ä½ è¯´å¾—å¾ˆå¯¹ ğŸ‘Œ
æˆ‘æ¥å¸®ä½ ä¿®æ­£ä½ç½®ç¼–ç å…¬å¼çš„ **LaTeX è¡¨è¾¾**ã€‚

åœ¨åŸå§‹ Transformer è®ºæ–‡ä¸­ï¼Œä½ç½®ç¼–ç å…¬å¼æ˜¯è¿™æ ·çš„ï¼š

$$
PE_{(pos, 2i)} = \sin\!\left(\frac{pos}{10000^{\frac{2i}{d}}}\right), 
\quad
PE_{(pos, 2i+1)} = \cos\!\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
$$

---

### å®Œæ•´ LaTeX ä»£ç ï¼ˆå¯ç›´æ¥æ”¾è¿›æ–‡æ¡£é‡Œï¼‰

```latex
\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right), 
\quad 
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\]
```

è¿™æ ·å°±å’Œå…¬å¼ä¸­ **åˆ†æ¯æŒ‡æ•°éƒ¨åˆ†è¦æœ‰åˆ†æ•° $\frac{2i}{d}$** ä¸€è‡´äº†ï¼Œè€Œä¸æ˜¯ä¹‹å‰å†™çš„ $10000^{2i/d}$ é‚£ç§æ­§ä¹‰å½¢å¼ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠ **è¯åµŒå…¥ + ä½ç½®ç¼–ç  + æœ€ç»ˆè¾“å…¥** ä¸‰ä¸ªå…¬å¼æ•´ç†æˆä¸€ä¸ªå®Œæ•´çš„ LaTeX å°èŠ‚ï¼Ÿ


  å…¶ä¸­ $pos$ æ˜¯è¯åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œ $i$ æ˜¯ç»´åº¦ç´¢å¼•ã€‚æœ€ç»ˆè¾“å…¥ä¸ºï¼š

 $$
x_i = E_i + PE(pos_i)
$$

å…¶ä¸­ï¼š

* $E_i$ è¡¨ç¤ºè¯ $w_i$ çš„åµŒå…¥å‘é‡ï¼ˆå³ä»åµŒå…¥çŸ©é˜µ $E$ ä¸­æŸ¥åˆ°çš„ç»“æœï¼‰ï¼›
* $PE(pos_i)$ è¡¨ç¤ºä½ç½®ç¼–ç å‘é‡ï¼›
* ä¸¤è€…é€å…ƒç´ ç›¸åŠ åä½œä¸º Transformer çš„è¾“å…¥ã€‚







- ç¼–ç å™¨  
æ¯ä¸ªç¼–ç å™¨å±‚åŒ…å«ä¸¤ä¸ªä¸»è¦å­æ¨¡å—ï¼š 

å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰  
å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Neural Network, FFNï¼‰  

æ¯ä¸ªå­æ¨¡å—åæ¥æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰ã€‚  
è§£ç å™¨  
è§£ç å™¨ä¸ç¼–ç å™¨ç±»ä¼¼ï¼Œä½†å¤šäº†æ©ç è‡ªæ³¨æ„åŠ›ï¼ˆMasked Self-Attentionï¼Œç”¨äºé¿å…æœªæ¥ä¿¡æ¯çš„æ³„éœ²ï¼‰å’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼ˆEncoder-Decoder Attentionï¼‰ã€‚  

### 2. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ 
è‡ªæ³¨æ„åŠ›æ˜¯Transformerçš„æ ¸å¿ƒï¼Œå…è®¸æ¨¡å‹åœ¨å¤„ç†æ¯ä¸ªè¯æ—¶å…³æ³¨åºåˆ—ä¸­çš„å…¶ä»–è¯ã€‚  
ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Product Attentionï¼‰
<img width="1109" height="644" alt="image" src="https://github.com/user-attachments/assets/0ebc34f6-2940-498e-a1f2-541c907b8197" />
å¤šå¤´æœºåˆ¶
<img width="1180" height="634" alt="image" src="https://github.com/user-attachments/assets/8f46e5e3-645e-4d3a-9fad-2fa525721a79" />
### 3. å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFFNï¼‰ 
æ¯ä¸ªç¼–ç å™¨å’Œè§£ç å™¨å±‚åŒ…å«ä¸€ä¸ªé€ä½ç½®çš„å‰é¦ˆç½‘ç»œï¼Œåº”ç”¨äºæ¯ä¸ªè¯çš„å‘é‡ï¼š

$$
\mathrm{FFN}(x) = \mathrm{ReLU}(x W_1 + b_1) W_2 + b_2
$$  

å…¶ä¸­ $W_1 \in \mathbb{R}^{d \times d_{ff}}$, $W_2 \in \mathbb{R}^{d_{ff} \times d}$ï¼Œ $d_{ff}$ é€šå¸¸è¿œå¤§äº $d$ï¼ˆå¦‚ $d_{ff} = 4d$ï¼‰ã€‚

  
### 4. æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–  
æ¯ä¸ªå­æ¨¡å—ï¼ˆè‡ªæ³¨æ„åŠ›æˆ–FFNï¼‰åæ¥æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼š  


$$
y = \mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))
$$  

å…¶ä¸­ Sublayer æ˜¯æ³¨æ„åŠ›æˆ– FFNï¼ŒLayerNorm å®šä¹‰ä¸ºï¼š  

$$
\mathrm{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$  

$\mu$ å’Œ $\sigma^2$ æ˜¯è¾“å…¥å‘é‡çš„å‡å€¼å’Œæ–¹å·®ï¼Œ $\gamma, \beta$ æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚


### 5. ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›  
è§£ç å™¨ä¸­çš„é¢å¤–æ³¨æ„åŠ›å±‚ä½¿ç”¨ç¼–ç å™¨çš„è¾“å‡ºK, Vå’Œè§£ç å™¨çš„Qï¼š  
  
   

$\mathrm{Attention}(Q_{\text{dec}}, K_{\text{enc}}, V_{\text{enc}})$
  
è¿™å…è®¸è§£ç å™¨å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸Šä¸‹æ–‡ã€‚  
 
### 6. è¾“å‡ºå±‚


è§£ç å™¨æœ€åä¸€å±‚é€šè¿‡çº¿æ€§å˜æ¢å’Œ softmax ç”Ÿæˆè¾“å‡ºæ¦‚ç‡ï¼š  

$$
P(y_i) = \mathrm{softmax}(z W_{\text{out}} + b_{\text{out}})
$$  

å…¶ä¸­ $z$ æ˜¯è§£ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºï¼Œ $W_{\text{out}} \in \mathbb{R}^{d \times |V|}$ã€‚





### 7. è¾“å‡ºå±‚
è§£ç å™¨æœ€åä¸€å±‚é€šè¿‡çº¿æ€§å˜æ¢å’Œsoftmaxç”Ÿæˆè¾“å‡ºæ¦‚ç‡ï¼š


$P(y_i) = \mathrm{softmax}(z W_{\text{out}} + b_{\text{out}})$  

å…¶ä¸­ $z$ æ˜¯è§£ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºï¼Œ $W_{\text{out}} \in \mathbb{R}^{d \times |V|}$ã€‚
  
### 8. æŸå¤±å‡½æ•° 
è®­ç»ƒæ—¶é€šå¸¸ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ–æ­£ç¡®è¾“å‡ºåºåˆ—çš„æ¦‚ç‡ï¼š


$\mathcal{L} = -\sum_{i=1}^{T} \log P(y_i \mid y_{<i}, X)$  

å…¶ä¸­ $T$ æ˜¯è¾“å‡ºåºåˆ—é•¿åº¦ï¼Œ $y_{<i}$ æ˜¯å·²ç”Ÿæˆçš„è¯ã€‚

### 9. æ€»ç»“  
Transformerçš„æ•°å­¦æ ¸å¿ƒåœ¨äºï¼š   

è‡ªæ³¨æ„åŠ›ï¼šé€šè¿‡Q, K, Væ•æ‰åºåˆ—å†…å…³ç³»ã€‚  
å¤šå¤´æœºåˆ¶ï¼šå¹¶è¡Œæ•æ‰å¤šç§è¯­ä¹‰å…³ç³»ã€‚  
ä½ç½®ç¼–ç ï¼šå¼¥è¡¥åºåˆ—é¡ºåºä¿¡æ¯ã€‚  
æ®‹å·®ä¸å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒå¹¶åŠ é€Ÿæ”¶æ•›ã€‚  

## ä¸€ä¸ªåªæœ‰ç¼–ç å™¨çš„Transformer

**å®Œæ•´çš„Transformer vs ç¼–ç å™¨Transformer**

---

### è¡¨æ ¼å†…å®¹

| ç»„ä»¶   | å®Œæ•´Transformer | ç¼–ç å™¨Transformer |
| ---- | ------------- | -------------- |
| ç¼–ç å™¨  | âœ…             | âœ…              |
| è§£ç å™¨  | âœ…             | âŒ              |
| é€‚ç”¨ä»»åŠ¡ | ç¿»è¯‘ã€æ‘˜è¦         | åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ        |
| è¾“å…¥è¾“å‡º | åºåˆ—â†’åºåˆ—         | åºåˆ—â†’ç±»åˆ«          |

---



```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import math

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼Œç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç å±‚ï¼Œä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯"""
    def __init__(self, d_model, max_len=5000):
        """
        åˆå§‹åŒ–ä½ç½®ç¼–ç 
        
        å‚æ•°:
            d_model (int): æ¨¡å‹ç»´åº¦
            max_len (int): æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤5000ï¼‰
        """
        super(PositionalEncoding, self).__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(max_len, d_model)
        pe = torch.zeros(max_len, d_model)
        # ç”Ÿæˆä½ç½®ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º(max_len, 1)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        # è®¡ç®—é™¤æ•°é¡¹ï¼Œç”¨äºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        # è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç ï¼Œåˆ†åˆ«å¡«å……åˆ°å¶æ•°å’Œå¥‡æ•°åˆ—
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶è½¬ç½®ï¼Œå½¢çŠ¶å˜ä¸º(max_len, 1, d_model)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        # å°†ä½ç½®ç¼–ç æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼Œæ·»åŠ ä½ç½®ç¼–ç 
        
        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(seq_len, batch_size, d_model)
        
        è¿”å›:
            torch.Tensor: æ·»åŠ ä½ç½®ç¼–ç åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸å˜
        """
        return x + self.pe[:x.size(0), :]

class SimpleTransformer(nn.Module):
    """ç®€å•çš„Transformeræ¨¡å‹ï¼Œç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡"""
    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, num_classes=2, max_len=100):
        """
        åˆå§‹åŒ–SimpleTransformeræ¨¡å‹
        
        å‚æ•°:
            vocab_size (int): è¯æ±‡è¡¨å¤§å°
            d_model (int): æ¨¡å‹ç»´åº¦ï¼ˆé»˜è®¤64ï¼‰
            nhead (int): å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°ï¼ˆé»˜è®¤4ï¼‰
            num_layers (int): Transformerç¼–ç å™¨å±‚æ•°ï¼ˆé»˜è®¤2ï¼‰
            num_classes (int): åˆ†ç±»ç±»åˆ«æ•°ï¼ˆé»˜è®¤2ï¼‰
            max_len (int): æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤100ï¼‰
        """
        super(SimpleTransformer, self).__init__()
        
        self.d_model = d_model  # ä¿å­˜æ¨¡å‹ç»´åº¦ï¼Œç”¨äºåç»­ç¼©æ”¾
        # è¯åµŒå…¥å±‚ï¼Œå°†è¯ç´¢å¼•è½¬æ¢ä¸ºd_modelç»´å‘é‡
        self.embedding = nn.Embedding(vocab_size, d_model)
        # ä½ç½®ç¼–ç å±‚ï¼Œæ·»åŠ åºåˆ—ä½ç½®ä¿¡æ¯
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        # å®šä¹‰å•ä¸ªTransformerç¼–ç å™¨å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,  # æ¨¡å‹ç»´åº¦
            nhead=nhead,  # æ³¨æ„åŠ›å¤´æ•°
            dim_feedforward=d_model * 4,  # å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦ï¼ˆé€šå¸¸ä¸ºd_modelçš„4å€ï¼‰
            dropout=0.1,  # dropoutæ¦‚ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            batch_first=True  # è¾“å…¥æ ¼å¼ä¸º(batch_size, seq_len, d_model)
        )
        # å †å å¤šä¸ªTransformerç¼–ç å™¨å±‚
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´ï¼Œå°†Transformerè¾“å‡ºæ˜ å°„åˆ°åˆ†ç±»ç»“æœ
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),  # çº¿æ€§å±‚ï¼Œé™ç»´
            nn.ReLU(),  # ReLUæ¿€æ´»å‡½æ•°ï¼Œå¢åŠ éçº¿æ€§
            nn.Dropout(0.1),  # dropoutå±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(d_model // 2, num_classes)  # è¾“å‡ºåˆ†ç±»ç»“æœ
        )
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len)
        
        è¿”å›:
            torch.Tensor: åˆ†ç±»è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
        """
        batch_size, seq_len = x.shape
        
        # è¯åµŒå…¥å¹¶ç¼©æ”¾ï¼ˆä¹˜ä»¥sqrt(d_model)ä»¥ç¨³å®šè®­ç»ƒï¼‰
        x = self.embedding(x) * math.sqrt(self.d_model)  # å½¢çŠ¶: (batch_size, seq_len, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # è½¬æ¢ä¸º(seq_len, batch_size, d_model)
        x = self.pos_encoding(x)  # æ·»åŠ ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # è½¬æ¢å›(batch_size, seq_len, d_model)
        
        # é€šè¿‡Transformerç¼–ç å™¨å¤„ç†åºåˆ—
        x = self.transformer_encoder(x)  # å½¢çŠ¶: (batch_size, seq_len, d_model)
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼Œå¾—åˆ°å›ºå®šé•¿åº¦è¡¨ç¤º
        x = x.mean(dim=1)  # å½¢çŠ¶: (batch_size, d_model)
        
        # é€šè¿‡åˆ†ç±»å¤´è¾“å‡ºåˆ†ç±»ç»“æœ
        x = self.classifier(x)  # å½¢çŠ¶: (batch_size, num_classes)
        return x

class SimpleDataset(Dataset):
    """ç®€å•çš„æ•°æ®é›†ç±»ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®"""
    def __init__(self, sequences, labels, vocab_size=1000, max_len=50):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        å‚æ•°:
            sequences (list): è¾“å…¥åºåˆ—åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—åˆ—è¡¨ï¼‰
            labels (list): æ ‡ç­¾åˆ—è¡¨
            vocab_size (int): è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤1000ï¼‰
            max_len (int): æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤50ï¼‰
        """
        self.sequences = sequences
        self.labels = labels
        self.vocab_size = vocab_size
        self.max_len = max_len
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.sequences)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        å‚æ•°:
            idx (int): æ ·æœ¬ç´¢å¼•
        
        è¿”å›:
            tuple: (åºåˆ—å¼ é‡, æ ‡ç­¾å¼ é‡)
        """
        sequence = self.sequences[idx]
        label = self.labels[idx]
        
        # å°†å­—ç¬¦ä¸²åºåˆ—è½¬æ¢ä¸ºæ•°å­—åºåˆ—
        if isinstance(sequence, str):
            tokens = [ord(c) % self.vocab_size for c in sequence[:self.max_len]]
        else:
            tokens = list(sequence[:self.max_len])
        
        # å¡«å……æˆ–æˆªæ–­åºåˆ—åˆ°å›ºå®šé•¿åº¦
        if len(tokens) < self.max_len:
            tokens = tokens + [0] * (self.max_len - len(tokens))
        else:
            tokens = tokens[:self.max_len]
        
        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.long)

def generate_synthetic_data(num_samples=1000, seq_len=30, vocab_size=1000, num_classes=2):
    """ç”Ÿæˆåˆæˆæ•°æ®ç”¨äºæ¼”ç¤º"""
    np.random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    
    sequences = []
    labels = []
    
    for i in range(num_samples):
        # ç”Ÿæˆéšæœºåºåˆ—
        seq = np.random.randint(1, vocab_size, seq_len)
        sequences.append(seq)
        
        # è®¡ç®—åºåˆ—ç‰¹å¾ç”¨äºç”Ÿæˆæ ‡ç­¾
        freq_1 = np.sum(seq == 1) / seq_len  # æ•°å­—1çš„é¢‘ç‡
        freq_2 = np.sum(seq == 2) / seq_len  # æ•°å­—2çš„é¢‘ç‡
        freq_3 = np.sum(seq == 3) / seq_len  # æ•°å­—3çš„é¢‘ç‡
        variance = np.var(seq)  # åºåˆ—æ–¹å·®
        # è®¡ç®—æœ€å¤§è¿ç»­ç›¸åŒæ•°å­—é•¿åº¦
        max_consecutive = 1
        current_consecutive = 1
        for j in range(1, len(seq)):
            if seq[j] == seq[j-1]:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 1
        
        # ç»¼åˆç‰¹å¾ç”Ÿæˆæ ‡ç­¾
        score = (freq_1 * 0.3 + freq_2 * 0.2 + freq_3 * 0.1 + 
                (variance / 1000) * 0.2 + (max_consecutive / seq_len) * 0.2)
        label = 1 if score > 0.5 else 0
        labels.append(label)
    
    return sequences, labels

def train_model(model, train_loader, val_loader, num_epochs=15, device='cpu'):
    """è®­ç»ƒæ¨¡å‹"""
    model.to(device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡0.0005
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)  # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ¯5ä¸ªepoché™ä½å­¦ä¹ ç‡
    
    # åˆå§‹åŒ–è®°å½•åˆ—è¡¨
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    print("å¼€å§‹è®­ç»ƒ...")
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)  # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            output = model(data)  # å‰å‘ä¼ æ’­
            loss = criterion(output, target)  # è®¡ç®—æŸå¤±
            loss.backward()  # åå‘ä¼ æ’­
            optimizer.step()  # æ›´æ–°å‚æ•°
            
            train_loss += loss.item()
            _, predicted = output.max(1)  # è·å–é¢„æµ‹ç±»åˆ«
            train_total += target.size(0)
            train_correct += predicted.eq(target).sum().item()
        
        train_loss /= len(train_loader)  # è®¡ç®—å¹³å‡æŸå¤±
        train_accuracy = 100. * train_correct / train_total  # è®¡ç®—å‡†ç¡®ç‡
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()
        
        val_loss /= len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        
        scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
        
        # è®°å½•æŒ‡æ ‡
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}%')
        print(f'  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%')
        print()
    
    return train_losses, val_losses, train_accuracies, val_accuracies

def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies):
    """ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±åŠå‡†ç¡®ç‡æ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue', linewidth=2, marker='o', markersize=4)
    ax1.plot(val_losses, label='éªŒè¯æŸå¤±', color='red', linewidth=2, marker='s', markersize=4)
    ax1.set_xlabel('è½®æ¬¡', fontsize=12)
    ax1.set_ylabel('æŸå¤±', fontsize=12)
    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(bottom=0)
    
    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue', linewidth=2, marker='o', markersize=4)
    ax2.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='red', linewidth=2, marker='s', markersize=4)
    ax2.set_xlabel('è½®æ¬¡', fontsize=12)
    ax2.set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 100)
    
    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')  # ä¿å­˜å›¾åƒ
    plt.show()

def predict(model, sequences, device='cpu'):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for sequence in sequences:
            # å¤„ç†å•ä¸ªåºåˆ—
            if isinstance(sequence, str):
                tokens = [ord(c) % 1000 for c in sequence[:50]]  # å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—åºåˆ—
            else:
                tokens = list(sequence[:50])  # ç¡®ä¿ä¸ºåˆ—è¡¨æ ¼å¼
            
            # å¡«å……æˆ–æˆªæ–­åºåˆ—
            if len(tokens) < 50:
                tokens = tokens + [0] * (50 - len(tokens))
            else:
                tokens = tokens[:50]
            
            # è½¬æ¢ä¸ºå¼ é‡
            x = torch.tensor([tokens], dtype=torch.long).to(device)
            
            # è¿›è¡Œé¢„æµ‹
            output = model(x)
            prob = F.softmax(output, dim=1)  # è®¡ç®—ç±»åˆ«æ¦‚ç‡
            pred_class = output.argmax(dim=1).item()  # è·å–é¢„æµ‹ç±»åˆ«
            confidence = prob.max().item()  # è·å–æœ€å¤§æ¦‚ç‡
            
            predictions.append({
                'sequence': sequence,
                'predicted_class': pred_class,
                'confidence': confidence,
                'probabilities': prob.cpu().numpy()[0]
            })
    
    return predictions

def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ•°æ®ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹"""
    # è®¾ç½®è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ç”Ÿæˆåˆæˆæ•°æ®
    print("ç”Ÿæˆåˆæˆæ•°æ®...")
    sequences, labels = generate_synthetic_data(num_samples=2000, seq_len=30)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ80%è®­ç»ƒï¼Œ20%éªŒè¯ï¼‰
    split_idx = int(0.8 * len(sequences))
    train_sequences = sequences[:split_idx]
    train_labels = labels[:split_idx]
    val_sequences = sequences[split_idx:]
    val_labels = labels[split_idx:]
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = SimpleDataset(train_sequences, train_labels)
    val_dataset = SimpleDataset(val_sequences, val_labels)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleTransformer(
        vocab_size=1000,
        d_model=64,
        nhead=4,
        num_layers=2,
        num_classes=2,
        max_len=50
    )
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹
    train_losses, val_losses, train_accuracies, val_accuracies = train_model(
        model, train_loader, val_loader, num_epochs=15, device=device
    )
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)
    
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'simple_transformer_model.pth')
    print("æ¨¡å‹å·²ä¿å­˜åˆ°: simple_transformer_model.pth")
    
    # æµ‹è¯•é¢„æµ‹
    print("\næµ‹è¯•é¢„æµ‹åŠŸèƒ½...")
    test_sequences = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•åºåˆ—",
        "å¦ä¸€ä¸ªæµ‹è¯•åºåˆ—",
        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    ]
    
    predictions = predict(model, test_sequences, device=device)
    
    for pred in predictions:
        print(f"åºåˆ—: {pred['sequence']}")
        print(f"é¢„æµ‹ç±»åˆ«: {pred['predicted_class']}")
        print(f"ç½®ä¿¡åº¦: {pred['confidence']:.4f}")
        print(f"ç±»åˆ«æ¦‚ç‡: {pred['probabilities']}")
        print()

if __name__ == "__main__":
    main()

```
## ä»¥ä¸‹æ˜¯å¯¹å®Œæ•´ä»£ç çš„è¯¦ç»†ä¸­æ–‡æ³¨é‡Šï¼Œæ¶µç›–äº†æ¯ä¸ªç±»å’Œå‡½æ•°çš„åŠŸèƒ½ã€å‚æ•°åŠå®ç°é€»è¾‘ï¼š

```python

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import math

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼Œç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºæ­£å¸¸
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç å±‚ï¼Œä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯"""
    def __init__(self, d_model, max_len=5000):
        """
        åˆå§‹åŒ–ä½ç½®ç¼–ç 
        
        å‚æ•°:
            d_model (int): æ¨¡å‹ç»´åº¦
            max_len (int): æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤5000ï¼‰
        """
        super(PositionalEncoding, self).__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(max_len, d_model)
        pe = torch.zeros(max_len, d_model)
        # ç”Ÿæˆä½ç½®ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º(max_len, 1)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        # è®¡ç®—é™¤æ•°é¡¹ï¼Œç”¨äºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        # è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ä½ç½®ç¼–ç ï¼Œåˆ†åˆ«å¡«å……åˆ°å¶æ•°å’Œå¥‡æ•°åˆ—
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶è½¬ç½®ï¼Œå½¢çŠ¶å˜ä¸º(max_len, 1, d_model)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        # å°†ä½ç½®ç¼–ç æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼Œæ·»åŠ ä½ç½®ç¼–ç 
        
        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(seq_len, batch_size, d_model)
        
        è¿”å›:
            torch.Tensor: æ·»åŠ ä½ç½®ç¼–ç åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸å˜
        """
        return x + self.pe[:x.size(0), :]

class SimpleTransformer(nn.Module):
    """ç®€å•çš„Transformeræ¨¡å‹ï¼Œç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡"""
    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, num_classes=2, max_len=100):
        """
        åˆå§‹åŒ–SimpleTransformeræ¨¡å‹
        
        å‚æ•°:
            vocab_size (int): è¯æ±‡è¡¨å¤§å°
            d_model (int): æ¨¡å‹ç»´åº¦ï¼ˆé»˜è®¤64ï¼‰
            nhead (int): å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°ï¼ˆé»˜è®¤4ï¼‰
            num_layers (int): Transformerç¼–ç å™¨å±‚æ•°ï¼ˆé»˜è®¤2ï¼‰
            num_classes (int): åˆ†ç±»ç±»åˆ«æ•°ï¼ˆé»˜è®¤2ï¼‰
            max_len (int): æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤100ï¼‰
        """
        super(SimpleTransformer, self).__init__()
        
        self.d_model = d_model  # ä¿å­˜æ¨¡å‹ç»´åº¦ï¼Œç”¨äºåç»­ç¼©æ”¾
        # è¯åµŒå…¥å±‚ï¼Œå°†è¯ç´¢å¼•è½¬æ¢ä¸ºd_modelç»´å‘é‡
        self.embedding = nn.Embedding(vocab_size, d_model)
        # ä½ç½®ç¼–ç å±‚ï¼Œæ·»åŠ åºåˆ—ä½ç½®ä¿¡æ¯
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        # å®šä¹‰å•ä¸ªTransformerç¼–ç å™¨å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,  # æ¨¡å‹ç»´åº¦
            nhead=nhead,  # æ³¨æ„åŠ›å¤´æ•°
            dim_feedforward=d_model * 4,  # å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦ï¼ˆé€šå¸¸ä¸ºd_modelçš„4å€ï¼‰
            dropout=0.1,  # dropoutæ¦‚ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            batch_first=True  # è¾“å…¥æ ¼å¼ä¸º(batch_size, seq_len, d_model)
        )
        # å †å å¤šä¸ªTransformerç¼–ç å™¨å±‚
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´ï¼Œå°†Transformerè¾“å‡ºæ˜ å°„åˆ°åˆ†ç±»ç»“æœ
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),  # çº¿æ€§å±‚ï¼Œé™ç»´
            nn.ReLU(),  # ReLUæ¿€æ´»å‡½æ•°ï¼Œå¢åŠ éçº¿æ€§
            nn.Dropout(0.1),  # dropoutå±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(d_model // 2, num_classes)  # è¾“å‡ºåˆ†ç±»ç»“æœ
        )
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len)
        
        è¿”å›:
            torch.Tensor: åˆ†ç±»è¾“å‡ºï¼Œå½¢çŠ¶ä¸º(batch_size, num_classes)
        """
        batch_size, seq_len = x.shape
        
        # è¯åµŒå…¥å¹¶ç¼©æ”¾ï¼ˆä¹˜ä»¥sqrt(d_model)ä»¥ç¨³å®šè®­ç»ƒï¼‰
        x = self.embedding(x) * math.sqrt(self.d_model)  # å½¢çŠ¶: (batch_size, seq_len, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # è½¬æ¢ä¸º(seq_len, batch_size, d_model)
        x = self.pos_encoding(x)  # æ·»åŠ ä½ç½®ç¼–ç 
        x = x.transpose(0, 1)  # è½¬æ¢å›(batch_size, seq_len, d_model)
        
        # é€šè¿‡Transformerç¼–ç å™¨å¤„ç†åºåˆ—
        x = self.transformer_encoder(x)  # å½¢çŠ¶: (batch_size, seq_len, d_model)
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼Œå¾—åˆ°å›ºå®šé•¿åº¦è¡¨ç¤º
        x = x.mean(dim=1)  # å½¢çŠ¶: (batch_size, d_model)
        
        # é€šè¿‡åˆ†ç±»å¤´è¾“å‡ºåˆ†ç±»ç»“æœ
        x = self.classifier(x)  # å½¢çŠ¶: (batch_size, num_classes)
        return x

class SimpleDataset(Dataset):
    """ç®€å•çš„æ•°æ®é›†ç±»ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®"""
    def __init__(self, sequences, labels, vocab_size=1000, max_len=50):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        å‚æ•°:
            sequences (list): è¾“å…¥åºåˆ—åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—åˆ—è¡¨ï¼‰
            labels (list): æ ‡ç­¾åˆ—è¡¨
            vocab_size (int): è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤1000ï¼‰
            max_len (int): æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤50ï¼‰
        """
        self.sequences = sequences
        self.labels = labels
        self.vocab_size = vocab_size
        self.max_len = max_len
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.sequences)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        å‚æ•°:
            idx (int): æ ·æœ¬ç´¢å¼•
        
        è¿”å›:
            tuple: (åºåˆ—å¼ é‡, æ ‡ç­¾å¼ é‡)
        """
        sequence = self.sequences[idx]
        label = self.labels[idx]
        
        # å°†å­—ç¬¦ä¸²åºåˆ—è½¬æ¢ä¸ºæ•°å­—åºåˆ—
        if isinstance(sequence, str):
            tokens = [ord(c) % self.vocab_size for c in sequence[:self.max_len]]
        else:
            tokens = list(sequence[:self.max_len])
        
        # å¡«å……æˆ–æˆªæ–­åºåˆ—åˆ°å›ºå®šé•¿åº¦
        if len(tokens) < self.max_len:
            tokens = tokens + [0] * (self.max_len - len(tokens))
        else:
            tokens = tokens[:self.max_len]
        
        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.long)

def generate_synthetic_data(num_samples=1000, seq_len=30, vocab_size=1000, num_classes=2):
    """ç”Ÿæˆåˆæˆæ•°æ®ç”¨äºæ¼”ç¤º"""
    np.random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    
    sequences = []
    labels = []
    
    for i in range(num_samples):
        # ç”Ÿæˆéšæœºåºåˆ—
        seq = np.random.randint(1, vocab_size, seq_len)
        sequences.append(seq)
        
        # è®¡ç®—åºåˆ—ç‰¹å¾ç”¨äºç”Ÿæˆæ ‡ç­¾
        freq_1 = np.sum(seq == 1) / seq_len  # æ•°å­—1çš„é¢‘ç‡
        freq_2 = np.sum(seq == 2) / seq_len  # æ•°å­—2çš„é¢‘ç‡
        freq_3 = np.sum(seq == 3) / seq_len  # æ•°å­—3çš„é¢‘ç‡
        variance = np.var(seq)  # åºåˆ—æ–¹å·®
        # è®¡ç®—æœ€å¤§è¿ç»­ç›¸åŒæ•°å­—é•¿åº¦
        max_consecutive = 1
        current_consecutive = 1
        for j in range(1, len(seq)):
            if seq[j] == seq[j-1]:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 1
        
        # ç»¼åˆç‰¹å¾ç”Ÿæˆæ ‡ç­¾
        score = (freq_1 * 0.3 + freq_2 * 0.2 + freq_3 * 0.1 + 
                (variance / 1000) * 0.2 + (max_consecutive / seq_len) * 0.2)
        label = 1 if score > 0.5 else 0
        labels.append(label)
    
    return sequences, labels

def train_model(model, train_loader, val_loader, num_epochs=15, device='cpu'):
    """è®­ç»ƒæ¨¡å‹"""
    model.to(device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡0.0005
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)  # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæ¯5ä¸ªepoché™ä½å­¦ä¹ ç‡
    
    # åˆå§‹åŒ–è®°å½•åˆ—è¡¨
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    print("å¼€å§‹è®­ç»ƒ...")
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)  # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            output = model(data)  # å‰å‘ä¼ æ’­
            loss = criterion(output, target)  # è®¡ç®—æŸå¤±
            loss.backward()  # åå‘ä¼ æ’­
            optimizer.step()  # æ›´æ–°å‚æ•°
            
            train_loss += loss.item()
            _, predicted = output.max(1)  # è·å–é¢„æµ‹ç±»åˆ«
            train_total += target.size(0)
            train_correct += predicted.eq(target).sum().item()
        
        train_loss /= len(train_loader)  # è®¡ç®—å¹³å‡æŸå¤±
        train_accuracy = 100. * train_correct / train_total  # è®¡ç®—å‡†ç¡®ç‡
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()
        
        val_loss /= len(val_loader)
        val_accuracy = 100. * val_correct / val_total
        
        scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
        
        # è®°å½•æŒ‡æ ‡
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}%')
        print(f'  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%')
        print()
    
    return train_losses, val_losses, train_accuracies, val_accuracies

def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies):
    """ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±åŠå‡†ç¡®ç‡æ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue', linewidth=2, marker='o', markersize=4)
    ax1.plot(val_losses, label='éªŒè¯æŸå¤±', color='red', linewidth=2, marker='s', markersize=4)
    ax1.set_xlabel('è½®æ¬¡', fontsize=12)
    ax1.set_ylabel('æŸå¤±', fontsize=12)
    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(bottom=0)
    
    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue', linewidth=2, marker='o', markersize=4)
    ax2.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='red', linewidth=2, marker='s', markersize=4)
    ax2.set_xlabel('è½®æ¬¡', fontsize=12)
    ax2.set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 100)
    
    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')  # ä¿å­˜å›¾åƒ
    plt.show()

def predict(model, sequences, device='cpu'):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    model.eval()
    predictions = []
    
    with torch.no_grad():
        for sequence in sequences:
            # å¤„ç†å•ä¸ªåºåˆ—
            if isinstance(sequence, str):
                tokens = [ord(c) % 1000 for c in sequence[:50]]  # å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—åºåˆ—
            else:
                tokens = list(sequence[:50])  # ç¡®ä¿ä¸ºåˆ—è¡¨æ ¼å¼
            
            # å¡«å……æˆ–æˆªæ–­åºåˆ—
            if len(tokens) < 50:
                tokens = tokens + [0] * (50 - len(tokens))
            else:
                tokens = tokens[:50]
            
            # è½¬æ¢ä¸ºå¼ é‡
            x = torch.tensor([tokens], dtype=torch.long).to(device)
            
            # è¿›è¡Œé¢„æµ‹
            output = model(x)
            prob = F.softmax(output, dim=1)  # è®¡ç®—ç±»åˆ«æ¦‚ç‡
            pred_class = output.argmax(dim=1).item()  # è·å–é¢„æµ‹ç±»åˆ«
            confidence = prob.max().item()  # è·å–æœ€å¤§æ¦‚ç‡
            
            predictions.append({
                'sequence': sequence,
                'predicted_class': pred_class,
                'confidence': confidence,
                'probabilities': prob.cpu().numpy()[0]
            })
    
    return predictions

def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ•°æ®ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹"""
    # è®¾ç½®è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ç”Ÿæˆåˆæˆæ•°æ®
    print("ç”Ÿæˆåˆæˆæ•°æ®...")
    sequences, labels = generate_synthetic_data(num_samples=2000, seq_len=30)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ80%è®­ç»ƒï¼Œ20%éªŒè¯ï¼‰
    split_idx = int(0.8 * len(sequences))
    train_sequences = sequences[:split_idx]
    train_labels = labels[:split_idx]
    val_sequences = sequences[split_idx:]
    val_labels = labels[split_idx:]
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = SimpleDataset(train_sequences, train_labels)
    val_dataset = SimpleDataset(val_sequences, val_labels)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleTransformer(
        vocab_size=1000,
        d_model=64,
        nhead=4,
        num_layers=2,
        num_classes=2,
        max_len=50
    )
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹
    train_losses, val_losses, train_accuracies, val_accuracies = train_model(
        model, train_loader, val_loader, num_epochs=15, device=device
    )
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)
    
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'simple_transformer_model.pth')
    print("æ¨¡å‹å·²ä¿å­˜åˆ°: simple_transformer_model.pth")
    
    # æµ‹è¯•é¢„æµ‹
    print("\næµ‹è¯•é¢„æµ‹åŠŸèƒ½...")
    test_sequences = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•åºåˆ—",
        "å¦ä¸€ä¸ªæµ‹è¯•åºåˆ—",
        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    ]
    
    predictions = predict(model, test_sequences, device=device)
    
    for pred in predictions:
        print(f"åºåˆ—: {pred['sequence']}")
        print(f"é¢„æµ‹ç±»åˆ«: {pred['predicted_class']}")
        print(f"ç½®ä¿¡åº¦: {pred['confidence']:.4f}")
        print(f"ç±»åˆ«æ¦‚ç‡: {pred['probabilities']}")
        print()

if __name__ == "__main__":
    main()

```

### ä»£ç æ€»ä½“è¯´æ˜ï¼š
1. **ä»£ç åŠŸèƒ½**ï¼š
   - è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„PyTorchå®ç°ï¼Œç”¨äºåŸºäºTransformerçš„åºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚
   - åŒ…æ‹¬ä½ç½®ç¼–ç ã€Transformeræ¨¡å‹ã€æ•°æ®é›†å¤„ç†ã€æ•°æ®ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒã€ç»“æœå¯è§†åŒ–å’Œé¢„æµ‹åŠŸèƒ½ã€‚

2. **ä¸»è¦ç»„ä»¶**ï¼š
   - **PositionalEncoding**ï¼šå®ç°ç»å…¸çš„æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ï¼Œä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯ã€‚
   - **SimpleTransformer**ï¼šä¸€ä¸ªç®€å•çš„Transformeræ¨¡å‹ï¼ŒåŒ…å«è¯åµŒå…¥ã€ä½ç½®ç¼–ç ã€Transformerç¼–ç å™¨å’Œåˆ†ç±»å¤´ã€‚
   - **SimpleDataset**ï¼šè‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œæ”¯æŒå­—ç¬¦ä¸²å’Œæ•°å­—åºåˆ—ï¼Œå¤„ç†å¡«å……å’Œæˆªæ–­ã€‚
   - **generate_synthetic_data**ï¼šç”Ÿæˆåˆæˆæ•°æ®ï¼ŒåŸºäºåºåˆ—ç‰¹å¾ï¼ˆå¦‚é¢‘ç‡ã€æ–¹å·®ã€è¿ç»­æ€§ï¼‰ç”Ÿæˆæ ‡ç­¾ã€‚
   - **train_model**ï¼šè®­ç»ƒæ¨¡å‹ï¼Œè®°å½•æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ã€‚
   - **plot_training_curves**ï¼šç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±åŠå‡†ç¡®ç‡æ›²çº¿ã€‚
   - **predict**ï¼šå¯¹æ–°åºåˆ—è¿›è¡Œé¢„æµ‹ï¼Œè¿”å›ç±»åˆ«ã€ç½®ä¿¡åº¦å’Œæ¦‚ç‡ã€‚
   - **main**ï¼šä¸»å‡½æ•°ï¼Œåè°ƒæ•°æ®ç”Ÿæˆã€è®­ç»ƒã€ç»˜å›¾å’Œé¢„æµ‹ã€‚

3. **ä½¿ç”¨åœºæ™¯**ï¼š
   - é€‚ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„å¿«é€ŸåŸå‹å¼€å‘ã€‚
   - åˆæˆæ•°æ®ç”¨äºæ¼”ç¤ºï¼Œå®é™…åº”ç”¨å¯æ›¿æ¢ä¸ºçœŸå®æ•°æ®é›†ã€‚
   - å¯é€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°ï¼ˆå¦‚`d_model`ã€`nhead`ç­‰ï¼‰ä¼˜åŒ–æ€§èƒ½ã€‚

## æ³¨é‡Š
### 1. æ•°æ®å‡†å¤‡
```
# ç”Ÿæˆ2000ä¸ªæ ·æœ¬
sequences, labels = generate_synthetic_data(num_samples=2000, seq_len=30)

# 80%è®­ç»ƒï¼Œ20%éªŒè¯
split_idx = int(0.8 * len(sequences))
train_sequences = sequences[:split_idx]  # 1600ä¸ª
val_sequences = sequences[split_idx:]    # 400ä¸ª
```

### 2. è®­ç»ƒå¾ªç¯
```
for epoch in range(15):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    for batch in train_loader:
        optimizer.zero_grad()
        output = model(data)  # å‰å‘ä¼ æ’­
        loss = criterion(output, target)  # è®¡ç®—æŸå¤±
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°å‚æ•°
    
    # éªŒè¯é˜¶æ®µ
    model.eval()
    with torch.no_grad():
        for batch in val_loader:
            output = model(data)
            # è®¡ç®—éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
```
### 3. æŸå¤±å‡½æ•°
```
criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±
optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adamä¼˜åŒ–å™¨
```
### æ€»ç»“
æ•°æ®ï¼šç¨‹åºç”Ÿæˆçš„åˆæˆæ•°æ®ï¼ŒåŸºäºç»Ÿè®¡ç‰¹å¾åˆ†ç±» 
æ¨¡å‹ï¼šåªæœ‰ç¼–ç å™¨çš„Transformerï¼Œç”¨äºåºåˆ—åˆ†ç±»  
è§£ç å™¨ï¼šæ²¡æœ‰ï¼Œå› ä¸ºä¸éœ€è¦ç”Ÿæˆåºåˆ—è¾“å‡º  
æµç¨‹ï¼šæ•°æ®ç”Ÿæˆ â†’ é¢„å¤„ç† â†’ åµŒå…¥ â†’ ä½ç½®ç¼–ç  â†’ è‡ªæ³¨æ„åŠ› â†’ æ± åŒ– â†’ åˆ†ç±»  
è¿™ä¸ªæ¨¡å‹é€‚åˆå­¦ä¹ Transformerçš„åŸºæœ¬æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä½ç½®ç¼–ç ï¼  
