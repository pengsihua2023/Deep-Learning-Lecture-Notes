ä½ è¯´å¾—å¯¹ ğŸ‘
æˆ‘åˆšæ‰å†™çš„

$$
\mathbf{h}_i^{(0)} = \mathbf{E}_{\text{token}}(x_i) + \mathbf{E}_{\text{segment}}(x_i) + \mathbf{E}_{\text{position}}(i)
$$

å½¢å¼ä¸Šå®¹æ˜“å¼•èµ·è¯¯è§£ã€‚å®é™…ä¸Šåœ¨ **BERT çš„è¾“å…¥åµŒå…¥ (input embedding)** é˜¶æ®µï¼Œä¸‰éƒ¨åˆ†åµŒå…¥åº”è¯¥å†™æˆå‘é‡æŸ¥è¡¨å½¢å¼ï¼Œè€Œä¸æ˜¯å‡½æ•°å½¢å¼ã€‚

æ›´å‡†ç¡®çš„è¡¨è¾¾æ˜¯ï¼š

---

### **ä¿®æ­£åçš„è¾“å…¥è¡¨ç¤º**

è®¾ï¼š

* $x_i$ æ˜¯ç¬¬ $i$ ä¸ª token çš„ç´¢å¼•ï¼Œ
* $s_i$ æ˜¯å¯¹åº”çš„å¥å­ç‰‡æ®µ (segment A/B)ï¼Œ
* $i$ æ˜¯ token çš„ä½ç½®ã€‚

åˆ™åˆå§‹è¾“å…¥å‘é‡ä¸ºï¼š

$$
\mathbf{h}_i^{(0)} = \mathbf{W}^{\text{token}}[x_i] + \mathbf{W}^{\text{segment}}[s_i] + \mathbf{W}^{\text{position}}[i]
$$

å…¶ä¸­ï¼š

* $\mathbf{W}^{\text{token}} \in \mathbb{R}^{|V|\times d}$ æ˜¯è¯åµŒå…¥çŸ©é˜µï¼Œ$\mathbf{W}^{\text{token}}[x_i]$ è¡¨ç¤ºè¯è¡¨ä¸­ç´¢å¼•ä¸º $x_i$ çš„å‘é‡ï¼Œ
* $\mathbf{W}^{\text{segment}} \in \mathbb{R}^{2 \times d}$ï¼Œå–å†³äº token å±äºå¥å­ A è¿˜æ˜¯ Bï¼Œ
* $\mathbf{W}^{\text{position}} \in \mathbb{R}^{n \times d}$ï¼Œç¬¬ $i$ è¡Œå¯¹åº”ä½ç½®åµŒå…¥ã€‚

---

ä¹Ÿå°±æ˜¯è¯´ï¼ŒBERT çš„è¾“å…¥å‘é‡æ˜¯ **ä¸‰ç±»åµŒå…¥æŸ¥è¡¨å‘é‡çš„é€å…ƒç´ ç›¸åŠ **ï¼Œè€Œä¸æ˜¯å‡½æ•°æ˜ å°„ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠ **æ•´ä¸ª BERT çš„æ•°å­¦æè¿° LaTeX æ–‡æ¡£**é‡æ–°æ’ç‰ˆï¼ˆåŒ…å«è¾“å…¥ã€Transformer å±‚ã€MLMã€NSPã€æ€»æŸå¤±ï¼‰ï¼Œå†™æˆä¸€ä¸ªå¯ä»¥ç›´æ¥ç¼–è¯‘çš„ `article`ï¼Ÿ


---

## 1. è¾“å…¥è¡¨ç¤º (Input Representation)

æ¯ä¸ªè¾“å…¥ token é¦–å…ˆè¢«æ˜ å°„ä¸ºä¸‰ç§åµŒå…¥çš„å’Œï¼š

$$
\mathbf{h}_i^{(0)} = \mathbf{E}_{\text{token}}(x_i) + \mathbf{E}_{\text{segment}}(x_i) + \mathbf{E}_{\text{position}}(i)
$$

å…¶ä¸­ï¼š

* $\mathbf{E}_{\text{token}}$ æ˜¯ token embeddingï¼Œ
* $\mathbf{E}_{\text{segment}}$ åŒºåˆ†å¥å­ A/Bï¼Œ
* $\mathbf{E}_{\text{position}}$ æ˜¯ä½ç½®ç¼–ç ã€‚

---

## 2. Transformer Encoder å±‚

BERT å †å  $L$ å±‚ Transformer Encoderï¼Œæ¯å±‚åŒ…å« **å¤šå¤´è‡ªæ³¨æ„åŠ› (Multi-Head Self-Attention, MHSA)** å’Œ **å‰é¦ˆç½‘ç»œ (FFN)**ã€‚

### (a) è‡ªæ³¨æ„åŠ› (Scaled Dot-Product Attention)

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

å…¶ä¸­ï¼š

* $Q = H W^Q$ï¼Œ$K = H W^K$ï¼Œ$V = H W^V$ï¼Œ
* $H \in \mathbb{R}^{n \times d}$ è¡¨ç¤ºä¸Šä¸€å±‚çš„éšè—çŠ¶æ€ã€‚

### (b) å¤šå¤´æ³¨æ„åŠ› (Multi-Head)

$$
\text{MHSA}(H) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

å…¶ä¸­ï¼š

$$
\text{head}_j = \text{Attention}(H W_j^Q, H W_j^K, H W_j^V)
$$

### (c) å‰é¦ˆç½‘ç»œ (FFN)

$$
\text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2
$$

### (d) æ®‹å·®ä¸å±‚å½’ä¸€åŒ–

æ¯å±‚éƒ½æœ‰æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼š

$$
H' = \text{LayerNorm}(H + \text{MHSA}(H))
$$

$$
H^{(l)} = \text{LayerNorm}(H' + \text{FFN}(H'))
$$

---

## 3. å †å  L å±‚

æœ€ç»ˆè¾“å‡ºä¸ºï¼š

$$
H^{(L)} = \text{TransformerEncoder}(H^{(0)})
$$

---

## 4. é¢„è®­ç»ƒä»»åŠ¡

### (a) æ©ç è¯­è¨€æ¨¡å‹ (Masked Language Model, MLM)

éšæœºæ©ç  token $x_i$ï¼Œé¢„æµ‹å…¶åŸå§‹è¯ï¼š

$$
\mathcal{L}_{\text{MLM}} = - \sum_{i \in \mathcal{M}} \log P(x_i \mid H^{(L)})
$$

å…¶ä¸­ $\mathcal{M}$ æ˜¯è¢«æ©ç çš„ä½ç½®é›†åˆã€‚

### (b) ä¸‹ä¸€å¥é¢„æµ‹ (Next Sentence Prediction, NSP)

å°† $[CLS]$ çš„è¡¨ç¤º $h_{\text{CLS}}$ è¾“å…¥åˆ°åˆ†ç±»å™¨ï¼Œé¢„æµ‹ä¸¤å¥æ˜¯å¦ç›¸é‚»ï¼š

$$
\mathcal{L}_{\text{NSP}} = - \big[ y \log P(\text{IsNext} \mid h_{\text{CLS}}) + (1-y) \log P(\text{NotNext} \mid h_{\text{CLS}}) \big]
$$

---

## 5. æ€»æŸå¤±å‡½æ•°

$$
\mathcal{L} = \mathcal{L}_{\text{MLM}} + \mathcal{L}_{\text{NSP}}
$$

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™äº› **LaTeX æºç æ•´ç†æˆä¸€ä»½å®Œæ•´çš„å¯ç¼–è¯‘æ–‡æ¡£ï¼ˆå«å¯¼è¨€åŒºä¸å…¬å¼ç¯å¢ƒï¼‰**ï¼Œè¿™æ ·ä½ å¯ä»¥ç›´æ¥æ‹¿å»è·‘ï¼Ÿ


## BERTï¼šBidirectional Encoder Representations from Transformers
<div align="center"> 
<img width="500" height="400" alt="image" src="https://github.com/user-attachments/assets/313fa320-c931-4fb3-8fcb-5d81de615a21" />
</div>

### BERTåŸç†å’Œå…·ä½“é—®é¢˜çš„å®ç°

#### **ä»‹ç»**
BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ Google åœ¨ 2018 å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚BERT çš„æ ¸å¿ƒåœ¨äºä½¿ç”¨ **åŒå‘ Transformer Encoder**ï¼Œé€šè¿‡å¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒæ•è·æ·±å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œç„¶åå¾®è°ƒä»¥é€‚é…ç‰¹å®šä»»åŠ¡ã€‚

#### **åŸç†**
1. **æ ¸å¿ƒæ€æƒ³**ï¼š
   - BERT ä½¿ç”¨ Transformer çš„ Encoder éƒ¨åˆ†ï¼Œé€šè¿‡åŒå‘è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰å»ºæ¨¡å•è¯çš„ä¸Šä¸‹æ–‡ã€‚
   - **é¢„è®­ç»ƒ**ï¼š
     - **Masked Language Model (MLM)**ï¼šéšæœºæ©ç›– 15% çš„è¾“å…¥è¯ï¼Œé¢„æµ‹è¿™äº›è¯ï¼Œå­¦ä¹ è¯­ä¹‰è¡¨ç¤ºã€‚
     - **Next Sentence Prediction (NSP)**ï¼šé¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­ï¼Œå­¦ä¹ å¥å­é—´å…³ç³»ã€‚
   - **å¾®è°ƒ**ï¼šæ›¿æ¢æœ€åä¸€å±‚ï¼ˆå¦‚å…¨è¿æ¥å±‚ï¼‰ï¼Œåœ¨ç‰¹å®šä»»åŠ¡æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚
   - è¾“å…¥åŒ…æ‹¬ tokenã€æ®µè½å’Œä½ç½®åµŒå…¥ï¼Œè¾“å‡ºä¸ºæ¯ä¸ª token çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚

2. **ç½‘ç»œç»“æ„**ï¼š
   - **è¾“å…¥**ï¼š
     - æ–‡æœ¬é€šè¿‡åˆ†è¯å™¨ï¼ˆTokenizerï¼‰è½¬æ¢ä¸º token IDsï¼Œæ·»åŠ  `[CLS]`ï¼ˆåˆ†ç±» tokenï¼‰å’Œ `[SEP]`ï¼ˆåˆ†éš” tokenï¼‰ã€‚
     - åµŒå…¥å±‚ï¼šToken Embeddingï¼ˆè¯å‘é‡ï¼‰ + Segment Embeddingï¼ˆåŒºåˆ†å¥å­ï¼‰ + Position Embeddingï¼ˆä½ç½®ä¿¡æ¯ï¼‰ã€‚
   - **Transformer Encoder**ï¼šå¤šå±‚ï¼ˆBERT-base ä¸º 12 å±‚ï¼‰ï¼ŒåŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œã€‚
   - **è¾“å‡º**ï¼š`[CLS]` token çš„è¡¨ç¤ºç”¨äºåˆ†ç±»ä»»åŠ¡ï¼›æ¯ä¸ª token çš„è¡¨ç¤ºç”¨äºåºåˆ—ä»»åŠ¡ï¼ˆå¦‚ NERï¼‰ã€‚

3. **å…¬å¼**ï¼š
   - è¾“å…¥åµŒå…¥ï¼š
     
     $$
     \text{Input} = \text{Token Embedding} + \text{Segment Embedding} + \text{Position Embedding}
     $$
     
   - è‡ªæ³¨æ„åŠ›ï¼š

$$
\text{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
$$

å…¶ä¸­ \(Q, K, V\) æ˜¯æŸ¥è¯¢ã€é”®ã€å€¼å‘é‡ï¼Œ\(d_k\) æ˜¯ç»´åº¦ã€‚



   - è¾“å‡ºï¼šæœ€åä¸€å±‚ `[CLS]` è¡¨ç¤ºç”¨äºåˆ†ç±»ï¼Œæˆ– token è¡¨ç¤ºç”¨äºåºåˆ—æ ‡æ³¨ã€‚

4. **ä¼˜ç‚¹**ï¼š
   - åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œæ•è·æ·±å±‚è¯­ä¹‰ã€‚
   - é¢„è®­ç»ƒ + å¾®è°ƒï¼Œé€‚é…å¤šç§ NLP ä»»åŠ¡ã€‚
   - é¢„è®­ç»ƒæ¨¡å‹å¼€ç®±å³ç”¨ï¼Œæ€§èƒ½å¼ºå¤§ã€‚

5. **ç¼ºç‚¹**ï¼š
   - è®¡ç®—æˆæœ¬é«˜ï¼Œå‚æ•°é‡å¤§ï¼ˆBERT-baseï¼š110M å‚æ•°ï¼‰ã€‚
   - å¾®è°ƒéœ€è¦å°å¿ƒè°ƒå‚ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

6. **é€‚ç”¨åœºæ™¯**ï¼š
   - æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€NERã€ç¿»è¯‘ç­‰ã€‚

---

#### **å…·ä½“é—®é¢˜å®ç°ï¼šæ–‡æœ¬åˆ†ç±» + æ•°æ®é›†åŠ è½½ + æ³¨æ„åŠ›å¯è§†åŒ–**
ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ PyTorch ä»£ç ç¤ºä¾‹ï¼Œä½¿ç”¨ Hugging Face çš„ `transformers` åº“å®ç° BERT æ¨¡å‹ï¼Œè§£å†³**æ–‡æœ¬åˆ†ç±»ä»»åŠ¡**ï¼ˆä»¥æƒ…æ„Ÿåˆ†æä¸ºä¾‹ï¼Œä½¿ç”¨ IMDb æ•°æ®é›†ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®é›†åŠ è½½ï¼ˆIMDb æ•°æ®é›†ï¼Œç®€åŒ–ç‰ˆæœ¬ï¼‰ã€‚
- BERT æ¨¡å‹å¾®è°ƒã€‚
- æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼ˆæ˜¾ç¤º `[CLS]` token çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼‰ã€‚

##### **ä»£ç ç¤ºä¾‹**
```python
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset

# 1. è‡ªå®šä¹‰æ•°æ®é›†
class IMDbDataset(Dataset):
    def __init__(self, dataset, tokenizer, max_length=128):
        self.texts = dataset['text']
        self.labels = dataset['label']
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            return_tensors='pt',
            padding='max_length',
            truncation=True,
            max_length=self.max_length
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 2. åŠ è½½æ•°æ®é›†å’Œåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
dataset = load_dataset('imdb', split='train[:1000]')  # ä½¿ç”¨ IMDb æ•°æ®é›†å‰ 1000 æ¡
train_dataset = IMDbDataset(dataset, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# 3. åŠ è½½ BERT æ¨¡å‹
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # äºŒåˆ†ç±»ï¼šæ­£/è´Ÿæƒ…æ„Ÿ
model.train()

# 4. è®­ç»ƒï¼ˆå¾®è°ƒï¼‰æ¨¡å‹
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(3):
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader):.4f}")

# 5. æµ‹è¯•å’Œæ³¨æ„åŠ›å¯è§†åŒ–
model.eval()
test_text = "This movie is fantastic!"
inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True, max_length=128)
inputs = {k: v.to(device) for k, v in inputs.items()}

# è·å–æ³¨æ„åŠ›æƒé‡
model_with_attentions = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=True)
model_with_attentions.load_state_dict(model.state_dict())
model_with_attentions.to(device)
model_with_attentions.eval()

with torch.no_grad():
    outputs = model_with_attentions(**inputs)
    logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=1)
    attentions = outputs.attentions  # æ³¨æ„åŠ›æƒé‡ (num_layers, batch_size, num_heads, seq_len, seq_len)

# é¢„æµ‹ç»“æœ
labels = ['Negative', 'Positive']
pred_label = labels[torch.argmax(probs, dim=1).item()]
print(f"Text: {test_text}")
print(f"Predicted sentiment: {pred_label}")
print(f"Probabilities: {probs.tolist()}")

# 6. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼ˆæœ€åä¸€å±‚ï¼Œç¬¬ 1 ä¸ªæ³¨æ„åŠ›å¤´ï¼‰
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
attn = attentions[-1][0, 0].detach().cpu().numpy()  # æœ€åä¸€å±‚ï¼Œç¬¬ 1 å¤´
plt.figure(figsize=(10, 8))
sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens, cmap='viridis')
plt.title('Attention Weights (Last Layer, Head 1)')
plt.xlabel('Tokens')
plt.ylabel('Tokens')
plt.show()
```

##### **ä»£ç è¯´æ˜**
- **æ•°æ®é›†åŠ è½½**ï¼š
  - ä½¿ç”¨ `datasets` åº“åŠ è½½ IMDb æ•°æ®é›†ï¼ˆæƒ…æ„Ÿåˆ†æï¼ŒäºŒåˆ†ç±»ï¼šæ­£/è´Ÿï¼‰ï¼Œå–å‰ 1000 æ¡æ•°æ®ä»¥ç®€åŒ–ã€‚
  - è‡ªå®šä¹‰ `IMDbDataset` ç±»ï¼Œå°†æ–‡æœ¬ç¼–ç ä¸º `input_ids` å’Œ `attention_mask`ï¼Œå¹¶æä¾›æ ‡ç­¾ã€‚
  - `DataLoader` æ‰¹é‡åŠ è½½æ•°æ®ï¼Œ`batch_size=8`ã€‚
- **æ¨¡å‹**ï¼š
  - `BertForSequenceClassification`ï¼šé¢„è®­ç»ƒ BERT æ¨¡å‹ï¼Œé™„åŠ åˆ†ç±»å¤´ï¼ˆå…¨è¿æ¥å±‚ï¼‰ï¼Œ`num_labels=2` è¡¨ç¤ºæ­£/è´Ÿåˆ†ç±»ã€‚
  - ä½¿ç”¨ `bert-base-uncased`ï¼ˆ12 å±‚ï¼Œ768 ç»´ï¼Œ110M å‚æ•°ï¼‰ã€‚
- **è®­ç»ƒ**ï¼š
  - ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ `2e-5`ï¼ˆBERT å¾®è°ƒå¸¸ç”¨å€¼ï¼‰ã€‚
  - è®­ç»ƒ 3 ä¸ª epochï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚
- **æµ‹è¯•**ï¼š
  - å¯¹ç¤ºä¾‹æ–‡æœ¬ â€œThis movie is fantastic!â€ è¿›è¡Œé¢„æµ‹ï¼Œè¾“å‡ºæƒ…æ„Ÿå’Œæ¦‚ç‡ã€‚
- **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼š
  - åŠ è½½å¸¦ `output_attentions=True` çš„æ¨¡å‹ï¼Œè·å–æ³¨æ„åŠ›æƒé‡ã€‚
  - ç»˜åˆ¶æœ€åä¸€å±‚ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å¤´çš„æƒé‡çƒ­å›¾ï¼Œæ˜¾ç¤º token é—´çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚
- **è¾“å‡º**ï¼š
  - é¢„æµ‹æƒ…æ„Ÿï¼ˆæ­£/è´Ÿï¼‰ã€‚
  - æ³¨æ„åŠ›çƒ­å›¾ï¼Œæ˜¾ç¤º `[CLS]` å’Œå…¶ä»– token çš„æ³¨æ„åŠ›æƒé‡ã€‚

#### **æ³¨æ„äº‹é¡¹**
1. **ç¯å¢ƒ**ï¼š
   - å®‰è£…ä¾èµ–ï¼š`pip install transformers datasets torch matplotlib seaborn`.
   - GPU åŠ é€Ÿï¼šå°†æ¨¡å‹å’Œæ•°æ®ç§»åˆ° GPUï¼ˆ`model.to(device)`ï¼‰ã€‚
2. **æ•°æ®é›†**ï¼š
   - IMDb æ•°æ®é›†éœ€ä¸‹è½½ï¼ˆ`datasets` åº“ä¼šè‡ªåŠ¨å¤„ç†ï¼‰ã€‚
   - å¯æ›¿æ¢ä¸ºå…¶ä»–æ•°æ®é›†ï¼ˆå¦‚ SST-2ã€GLUEï¼‰ã€‚
3. **å¾®è°ƒ**ï¼š
   - å®é™…åº”ç”¨ä¸­ï¼Œå»ºè®®ç”¨å®Œæ•´æ•°æ®é›†ï¼ˆå¦‚ IMDb çš„ 25,000 æ¡è®­ç»ƒæ•°æ®ï¼‰ã€‚
   - å¯å†»ç»“éƒ¨åˆ†å±‚ä»¥åŠ é€Ÿï¼š`model.bert.encoder.layer[:8].requires_grad = False`ã€‚
4. **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼š
   - çƒ­å›¾æ˜¾ç¤º token é—´çš„æ³¨æ„åŠ›æƒé‡ï¼Œ`[CLS]` token é€šå¸¸èšåˆå…¨å±€ä¿¡æ¯ã€‚
   - å¯æ‰©å±•åˆ°å¤šå¤´æˆ–å¤šå±‚æ³¨æ„åŠ›åˆ†æã€‚
5. **è®¡ç®—èµ„æº**ï¼š
   - BERT è®¡ç®—å¯†é›†ï¼Œå»ºè®® GPU è¿è¡Œã€‚
   - æ‰¹å¤§å°å’Œåºåˆ—é•¿åº¦ï¼ˆ`max_length`ï¼‰éœ€æ ¹æ®ç¡¬ä»¶è°ƒæ•´ã€‚

#### **æ‰©å±•**
1. **å¤šä»»åŠ¡**ï¼š
   - é—®ç­”ï¼šä½¿ç”¨ `BertForQuestionAnswering`ï¼Œè¾“å…¥é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºç­”æ¡ˆè·¨åº¦ã€‚
   - NERï¼šä½¿ç”¨ `BertForTokenClassification`ï¼Œä¸ºæ¯ä¸ª token é¢„æµ‹æ ‡ç­¾ã€‚
2. **æ›´å¤æ‚å¯è§†åŒ–**ï¼š
   - ç»˜åˆ¶å¤šå¤´æ³¨æ„åŠ›ï¼šå¾ªç¯ `attentions[-1][0, i]`ï¼ˆi ä¸ºæ³¨æ„åŠ›å¤´ï¼‰ã€‚
   - åˆ†æç‰¹å®š token çš„æ³¨æ„åŠ›ï¼šæå– `attn[:, 0, :]`ï¼ˆ`[CLS]` çš„æ³¨æ„åŠ›ï¼‰ã€‚
3. **åé—®é¢˜**ï¼š
   - ç»“åˆè§‚æµ‹æ•°æ®ä¼°è®¡ BERT å‚æ•°ï¼ˆå¦‚æ³¨æ„åŠ›æƒé‡ï¼‰ã€‚

#### **æ€»ç»“**
BERT é€šè¿‡åŒå‘ Transformer å»ºæ¨¡è¯­ä¹‰ï¼Œé¢„è®­ç»ƒåå¾®è°ƒé€‚é…ä»»åŠ¡ã€‚ä¸Šè¿°ä»£ç å±•ç¤ºäº†åœ¨ IMDb æ•°æ®é›†ä¸Šçš„æ–‡æœ¬åˆ†ç±»å®ç°ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€å¾®è°ƒå’Œæ³¨æ„åŠ›å¯è§†åŒ–ã€‚
