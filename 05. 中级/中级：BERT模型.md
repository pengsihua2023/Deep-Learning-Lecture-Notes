## BERTï¼šBidirectional Encoder Representations from Transformers
 
BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ Google åœ¨ 2018 å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚BERT çš„æ ¸å¿ƒåœ¨äºä½¿ç”¨ **åŒå‘ Transformer Encoder**ï¼Œé€šè¿‡å¤§è§„æ¨¡æ— ç›‘ç£é¢„è®­ç»ƒæ•è·æ·±å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œç„¶åå¾®è°ƒä»¥é€‚é…ç‰¹å®šä»»åŠ¡ã€‚
<div align="center"> 
<img width="350" height="280" alt="image" src="https://github.com/user-attachments/assets/313fa320-c931-4fb3-8fcb-5d81de615a21" />
<img width="440" height="175" alt="image" src="https://github.com/user-attachments/assets/e2f218c5-a3cc-487f-a3b2-eb8da6e8eee1" />

</div>
<div align="center">
(æ­¤å›¾å¼•è‡ªInternetã€‚)
</div>


## ğŸ“– BERTæ¨¡å‹çš„æ•°å­¦æè¿°

### 1. è¾“å…¥è¡¨ç¤ºï¼ˆInput Representationï¼‰

å¯¹äºè¾“å…¥åºåˆ—

$$
x = \{x_1, x_2, \dots, x_n\},
$$

BERT çš„è¾“å…¥å‘é‡ç”± **è¯åµŒå…¥**ã€**ä½ç½®åµŒå…¥**å’Œ**SegmentåµŒå…¥**ç»„æˆï¼š

$$
h_i^{(0)} = E(x_i) + P(i) + S(s_i),
$$

å…¶ä¸­ï¼š

* $E(x_i)$ï¼šè¯åµŒå…¥å‘é‡ï¼Œç»´åº¦ä¸º $d$ã€‚
* $P(i)$ï¼šä½ç½®åµŒå…¥ã€‚
* $S(s_i)$ï¼šå¥å­ç‰‡æ®µåµŒå…¥ï¼ˆç”¨äºåŒºåˆ†å¥å­ A/Bï¼‰ã€‚



### 2. Transformer ç¼–ç å™¨å±‚ï¼ˆEncoder Layerï¼‰

BERT ç”± $L$ å±‚ Transformer Encoder å †å è€Œæˆã€‚ç¬¬ $l$ å±‚è¾“å…¥ä¸º $\{h_1^{(l-1)}, \dots, h_n^{(l-1)}\}$ï¼Œè¾“å‡ºä¸º $\{h_1^{(l)}, \dots, h_n^{(l)}\}$ã€‚

- (a) å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰

é¦–å…ˆè®¡ç®—æ¯ä¸ª token çš„æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰ã€å€¼ï¼ˆValueï¼‰å‘é‡ï¼š

$$
Q = H^{(l-1)} W_Q, \quad K = H^{(l-1)} W_K, \quad V = H^{(l-1)} W_V,
$$

å…¶ä¸­ $H^{(l-1)} \in \mathbb{R}^{n \times d}$ï¼ŒæŠ•å½±çŸ©é˜µ $W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$ã€‚

å•å¤´æ³¨æ„åŠ›è®¡ç®—ä¸ºï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V.
$$

å¤šå¤´æ³¨æ„åŠ›ä¸ºï¼š

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O,
$$

$$
\text{head}_i = \text{Attention}(Q W_Q^{(i)}, K W_K^{(i)}, V W_V^{(i)}).
$$

- (b) å‰é¦ˆç½‘ç»œï¼ˆFeed Forward Networkï¼‰

æ¯ä¸ªä½ç½®ç‹¬ç«‹é€šè¿‡ä¸¤å±‚å‰é¦ˆç½‘ç»œï¼š

$$
\text{FFN}(h) = \text{GELU}(h W_1 + b_1) W_2 + b_2.
$$

### 3. æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–ï¼ˆResidual + LayerNormï¼‰

æ¯ä¸ªå­å±‚åæœ‰æ®‹å·®å’Œå½’ä¸€åŒ–ï¼š

$$
\tilde{H}^{(l)} = \text{LayerNorm}(H^{(l-1)} + \text{MultiHead}(Q,K,V)),
$$

$$
H^{(l)} = \text{LayerNorm}(\tilde{H}^{(l)} + \text{FFN}(\tilde{H}^{(l)})).
$$



### 4. é¢„è®­ç»ƒç›®æ ‡ï¼ˆPre-training Objectivesï¼‰

BERT æœ‰ä¸¤ä¸ªä¸»è¦é¢„è®­ç»ƒä»»åŠ¡ï¼š

 - (a) æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Model, MLMï¼‰

éšæœºæ©ç è¾“å…¥çš„ 15% tokenï¼Œé¢„æµ‹è¢«æ©ç çš„è¯ï¼š

$$
\mathcal{L}_ {MLM} = - \sum_{i \in M} \log P(x_i \mid x_{\setminus M}),
$$

å…¶ä¸­ $M$ ä¸ºè¢«æ©ç ä½ç½®é›†åˆã€‚

- (b) ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNext Sentence Prediction, NSPï¼‰

åˆ¤åˆ«è¾“å…¥çš„ä¸¤ä¸ªå¥å­æ˜¯å¦ä¸ºç›¸é‚»å¥å­ï¼š

$$
\mathcal{L}_{NSP} = - \big[ y \log P(\text{IsNext}) + (1-y)\log P(\text{NotNext}) \big].
$$

### (c) æ€»æŸå¤±

$\mathcal{L} = \mathcal{L}_ {MLM} + \mathcal{L}_ {NSP}. $

---


### ğŸ“– é€‚ç”¨åœºæ™¯ï¼š
æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€NERã€ç¿»è¯‘ç­‰ã€‚

---

### ğŸ“– **å…·ä½“é—®é¢˜å®ç°ï¼šæ–‡æœ¬åˆ†ç±» + æ•°æ®é›†åŠ è½½ + æ³¨æ„åŠ›å¯è§†åŒ–**
ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ PyTorch ä»£ç ç¤ºä¾‹ï¼Œä½¿ç”¨ Hugging Face çš„ `transformers` åº“å®ç° BERT æ¨¡å‹ï¼Œè§£å†³**æ–‡æœ¬åˆ†ç±»ä»»åŠ¡**ï¼ˆä»¥æƒ…æ„Ÿåˆ†æä¸ºä¾‹ï¼Œä½¿ç”¨ IMDb æ•°æ®é›†ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®é›†åŠ è½½ï¼ˆIMDb æ•°æ®é›†ï¼Œç®€åŒ–ç‰ˆæœ¬ï¼‰ã€‚
- BERT æ¨¡å‹å¾®è°ƒã€‚
- æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼ˆæ˜¾ç¤º `[CLS]` token çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼‰ã€‚

##### **ä»£ç ç¤ºä¾‹**
```python
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset

# 1. è‡ªå®šä¹‰æ•°æ®é›†
class IMDbDataset(Dataset):
    def __init__(self, dataset, tokenizer, max_length=128):
        self.texts = dataset['text']
        self.labels = dataset['label']
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            return_tensors='pt',
            padding='max_length',
            truncation=True,
            max_length=self.max_length
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 2. åŠ è½½æ•°æ®é›†å’Œåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
dataset = load_dataset('imdb', split='train[:1000]')  # ä½¿ç”¨ IMDb æ•°æ®é›†å‰ 1000 æ¡
train_dataset = IMDbDataset(dataset, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# 3. åŠ è½½ BERT æ¨¡å‹
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # äºŒåˆ†ç±»ï¼šæ­£/è´Ÿæƒ…æ„Ÿ
model.train()

# 4. è®­ç»ƒï¼ˆå¾®è°ƒï¼‰æ¨¡å‹
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(3):
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader):.4f}")

# 5. æµ‹è¯•å’Œæ³¨æ„åŠ›å¯è§†åŒ–
model.eval()
test_text = "This movie is fantastic!"
inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True, max_length=128)
inputs = {k: v.to(device) for k, v in inputs.items()}

# è·å–æ³¨æ„åŠ›æƒé‡
model_with_attentions = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=True)
model_with_attentions.load_state_dict(model.state_dict())
model_with_attentions.to(device)
model_with_attentions.eval()

with torch.no_grad():
    outputs = model_with_attentions(**inputs)
    logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=1)
    attentions = outputs.attentions  # æ³¨æ„åŠ›æƒé‡ (num_layers, batch_size, num_heads, seq_len, seq_len)

# é¢„æµ‹ç»“æœ
labels = ['Negative', 'Positive']
pred_label = labels[torch.argmax(probs, dim=1).item()]
print(f"Text: {test_text}")
print(f"Predicted sentiment: {pred_label}")
print(f"Probabilities: {probs.tolist()}")

# 6. å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼ˆæœ€åä¸€å±‚ï¼Œç¬¬ 1 ä¸ªæ³¨æ„åŠ›å¤´ï¼‰
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
attn = attentions[-1][0, 0].detach().cpu().numpy()  # æœ€åä¸€å±‚ï¼Œç¬¬ 1 å¤´
plt.figure(figsize=(10, 8))
sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens, cmap='viridis')
plt.title('Attention Weights (Last Layer, Head 1)')
plt.xlabel('Tokens')
plt.ylabel('Tokens')
plt.show()
```

###  ğŸ“– **ä»£ç è¯´æ˜**
- **æ•°æ®é›†åŠ è½½**ï¼š
  - ä½¿ç”¨ `datasets` åº“åŠ è½½ IMDb æ•°æ®é›†ï¼ˆæƒ…æ„Ÿåˆ†æï¼ŒäºŒåˆ†ç±»ï¼šæ­£/è´Ÿï¼‰ï¼Œå–å‰ 1000 æ¡æ•°æ®ä»¥ç®€åŒ–ã€‚
  - è‡ªå®šä¹‰ `IMDbDataset` ç±»ï¼Œå°†æ–‡æœ¬ç¼–ç ä¸º `input_ids` å’Œ `attention_mask`ï¼Œå¹¶æä¾›æ ‡ç­¾ã€‚
  - `DataLoader` æ‰¹é‡åŠ è½½æ•°æ®ï¼Œ`batch_size=8`ã€‚
- **æ¨¡å‹**ï¼š
  - `BertForSequenceClassification`ï¼šé¢„è®­ç»ƒ BERT æ¨¡å‹ï¼Œé™„åŠ åˆ†ç±»å¤´ï¼ˆå…¨è¿æ¥å±‚ï¼‰ï¼Œ`num_labels=2` è¡¨ç¤ºæ­£/è´Ÿåˆ†ç±»ã€‚
  - ä½¿ç”¨ `bert-base-uncased`ï¼ˆ12 å±‚ï¼Œ768 ç»´ï¼Œ110M å‚æ•°ï¼‰ã€‚
- **è®­ç»ƒ**ï¼š
  - ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ `2e-5`ï¼ˆBERT å¾®è°ƒå¸¸ç”¨å€¼ï¼‰ã€‚
  - è®­ç»ƒ 3 ä¸ª epochï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚
- **æµ‹è¯•**ï¼š
  - å¯¹ç¤ºä¾‹æ–‡æœ¬ â€œThis movie is fantastic!â€ è¿›è¡Œé¢„æµ‹ï¼Œè¾“å‡ºæƒ…æ„Ÿå’Œæ¦‚ç‡ã€‚
- **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼š
  - åŠ è½½å¸¦ `output_attentions=True` çš„æ¨¡å‹ï¼Œè·å–æ³¨æ„åŠ›æƒé‡ã€‚
  - ç»˜åˆ¶æœ€åä¸€å±‚ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å¤´çš„æƒé‡çƒ­å›¾ï¼Œæ˜¾ç¤º token é—´çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚
- **è¾“å‡º**ï¼š
  - é¢„æµ‹æƒ…æ„Ÿï¼ˆæ­£/è´Ÿï¼‰ã€‚
  - æ³¨æ„åŠ›çƒ­å›¾ï¼Œæ˜¾ç¤º `[CLS]` å’Œå…¶ä»– token çš„æ³¨æ„åŠ›æƒé‡ã€‚

### ğŸ“– **æ³¨æ„äº‹é¡¹**
1. **ç¯å¢ƒ**ï¼š
   - å®‰è£…ä¾èµ–ï¼š`pip install transformers datasets torch matplotlib seaborn`.
   - GPU åŠ é€Ÿï¼šå°†æ¨¡å‹å’Œæ•°æ®ç§»åˆ° GPUï¼ˆ`model.to(device)`ï¼‰ã€‚
2. **æ•°æ®é›†**ï¼š
   - IMDb æ•°æ®é›†éœ€ä¸‹è½½ï¼ˆ`datasets` åº“ä¼šè‡ªåŠ¨å¤„ç†ï¼‰ã€‚
   - å¯æ›¿æ¢ä¸ºå…¶ä»–æ•°æ®é›†ï¼ˆå¦‚ SST-2ã€GLUEï¼‰ã€‚
3. **å¾®è°ƒ**ï¼š
   - å®é™…åº”ç”¨ä¸­ï¼Œå»ºè®®ç”¨å®Œæ•´æ•°æ®é›†ï¼ˆå¦‚ IMDb çš„ 25,000 æ¡è®­ç»ƒæ•°æ®ï¼‰ã€‚
   - å¯å†»ç»“éƒ¨åˆ†å±‚ä»¥åŠ é€Ÿï¼š`model.bert.encoder.layer[:8].requires_grad = False`ã€‚
4. **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼š
   - çƒ­å›¾æ˜¾ç¤º token é—´çš„æ³¨æ„åŠ›æƒé‡ï¼Œ`[CLS]` token é€šå¸¸èšåˆå…¨å±€ä¿¡æ¯ã€‚
   - å¯æ‰©å±•åˆ°å¤šå¤´æˆ–å¤šå±‚æ³¨æ„åŠ›åˆ†æã€‚
5. **è®¡ç®—èµ„æº**ï¼š
   - BERT è®¡ç®—å¯†é›†ï¼Œå»ºè®® GPU è¿è¡Œã€‚
   - æ‰¹å¤§å°å’Œåºåˆ—é•¿åº¦ï¼ˆ`max_length`ï¼‰éœ€æ ¹æ®ç¡¬ä»¶è°ƒæ•´ã€‚

### ğŸ“– **æ‰©å±•**
1. **å¤šä»»åŠ¡**ï¼š
   - é—®ç­”ï¼šä½¿ç”¨ `BertForQuestionAnswering`ï¼Œè¾“å…¥é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºç­”æ¡ˆè·¨åº¦ã€‚
   - NERï¼šä½¿ç”¨ `BertForTokenClassification`ï¼Œä¸ºæ¯ä¸ª token é¢„æµ‹æ ‡ç­¾ã€‚
2. **æ›´å¤æ‚å¯è§†åŒ–**ï¼š
   - ç»˜åˆ¶å¤šå¤´æ³¨æ„åŠ›ï¼šå¾ªç¯ `attentions[-1][0, i]`ï¼ˆi ä¸ºæ³¨æ„åŠ›å¤´ï¼‰ã€‚
   - åˆ†æç‰¹å®š token çš„æ³¨æ„åŠ›ï¼šæå– `attn[:, 0, :]`ï¼ˆ`[CLS]` çš„æ³¨æ„åŠ›ï¼‰ã€‚
3. **åé—®é¢˜**ï¼š
   - ç»“åˆè§‚æµ‹æ•°æ®ä¼°è®¡ BERT å‚æ•°ï¼ˆå¦‚æ³¨æ„åŠ›æƒé‡ï¼‰ã€‚

### ğŸ“– **æ€»ç»“**
BERT é€šè¿‡åŒå‘ Transformer å»ºæ¨¡è¯­ä¹‰ï¼Œé¢„è®­ç»ƒåå¾®è°ƒé€‚é…ä»»åŠ¡ã€‚ä¸Šè¿°ä»£ç å±•ç¤ºäº†åœ¨ IMDb æ•°æ®é›†ä¸Šçš„æ–‡æœ¬åˆ†ç±»å®ç°ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€å¾®è°ƒå’Œæ³¨æ„åŠ›å¯è§†åŒ–ã€‚
