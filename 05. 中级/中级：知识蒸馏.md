
## 基本思想

知识蒸馏的目标是通过让 **学生模型（Student, S）** 模仿 **教师模型（Teacher, T）** 的输出分布，从而在保证精度的前提下降低模型复杂度。

---

## 数学公式

### 1. Softmax 与温度缩放

教师模型输出的 logits 记为 $z^{(T)}$，学生模型输出的 logits 记为 $z^{(S)}$。
通过 softmax 函数并引入温度参数 $T > 1$ 来平滑概率分布：

$$
p_i^{(T)} = \frac{\exp\left(\frac{z_i^{(T)}}{T}\right)}{\sum_j \exp\left(\frac{z_j^{(T)}}{T}\right)}, 
\quad 
p_i^{(S)} = \frac{\exp\left(\frac{z_i^{(S)}}{T}\right)}{\sum_j \exp\left(\frac{z_j^{(S)}}{T}\right)}.
$$

当 $T$ 越大，分布越平滑，能更好体现类别间的相对关系。

---

### 2. 蒸馏损失 (Distillation Loss)

学生模型需要拟合教师模型的 soft target 分布，常用 **KL 散度**：

$$
\mathcal{L}_{\text{KD}} = T^2 \cdot \mathrm{KL}\big(p^{(T)} \,\|\, p^{(S)}\big) 
= T^2 \sum_i p_i^{(T)} \log \frac{p_i^{(T)}}{p_i^{(S)}}.
$$

其中 $T^2$ 是一个缩放因子，保证梯度大小与温度无关。

---

### 3. 总损失函数

实际训练时通常结合 **真实标签的交叉熵损失** 与 **蒸馏损失**：

$$
\mathcal{L} = \alpha \cdot \mathcal{L}_ {\text{CE}}(y, p^{(S)}_{T=1}) 
+ (1 - \alpha) \cdot \mathcal{L}_{\text{KD}}.
$

其中：

* $\mathcal{L}_{\text {CE}}(y, p^{(S)}_{T=1}) = - \sum_i y_i \log p_i^{(S)}$，使用真实标签 $y$；
* $\alpha \in [0,1]$ 控制真实标签和教师信号的权重。



---

要不要我帮你画一个公式结构图，展示 **Teacher → Student → Loss** 的流程图？




## Knowledge Distillation (知识蒸馏）
<div align="center">
<img width="600" height="240" alt="image" src="https://github.com/user-attachments/assets/2a79095f-2443-4a08-a7fa-a1a091bba957" />
</div>

编写一个基于PyTorch的最简单Knowledge Distillation（知识蒸馏）示例，使用真实数据集（MNIST手写数字数据集），实现从一个较大的教师模型（CNN）到较小的学生模型（MLP）的知识蒸馏。任务是数字分类，知识蒸馏通过教师模型的软标签指导学生模型学习。结果将通过评估分类准确率和可视化学生模型的预测混淆矩阵来展示。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

# 教师模型（较大的CNN）
class TeacherCNN(nn.Module):
    def __init__(self):
        super(TeacherCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.fc = nn.Linear(32 * 7 * 7, 10)
    
    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# 学生模型（较小的MLP）
class StudentMLP(nn.Module):
    def __init__(self):
        super(StudentMLP, self).__init__()
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.fc(x)

# 知识蒸馏损失
class DistillationLoss(nn.Module):
    def __init__(self, temperature=2.0, alpha=0.5):
        super(DistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_logits, teacher_logits, labels):
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        distillation_loss = self.kl_loss(soft_student, soft_teacher) * (self.temperature ** 2)
        ce_loss = self.ce_loss(student_logits, labels)
        return self.alpha * distillation_loss + (1 - self.alpha) * ce_loss

# 数据加载
def get_data_loaders():
    transform = transforms.ToTensor()
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)
    return train_loader, test_loader

# 训练教师模型
def train_teacher(model, train_loader, epochs=5):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Teacher Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}')

# 训练学生模型（知识蒸馏）
def train_student(teacher, student, train_loader, epochs=5):
    optimizer = optim.Adam(student.parameters(), lr=0.001)
    criterion = DistillationLoss(temperature=2.0, alpha=0.5)
    
    teacher.eval()
    student.train()
    for epoch in range(epochs):
        total_loss = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            with torch.no_grad():
                teacher_logits = teacher(images)
            student_logits = student(images)
            loss = criterion(student_logits, teacher_logits, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Student Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}')

# 评估和可视化
def evaluate_and_visualize(model, test_loader):
    model.eval()
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    # 计算准确率
    accuracy = accuracy_score(true_labels, predictions)
    print(f'Test Accuracy: {accuracy:.4f}')
    
    # 绘制混淆矩阵
    cm = confusion_matrix(true_labels, predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix of Student Model on MNIST')
    plt.savefig('mnist_confusion_matrix.png')
    plt.close()
    print("Confusion matrix saved as 'mnist_confusion_matrix.png'")
    
    # 打印前几个样本的预测结果
    print("\nSample Predictions (First 5):")
    for i in range(5):
        print(f"Sample {i+1}: True Label={true_labels[i]}, Predicted Label={predictions[i]}")

def main():
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 初始化模型
    teacher = TeacherCNN().to(device)
    student = StudentMLP().to(device)
    
    # 加载数据
    train_loader, test_loader = get_data_loaders()
    
    # 训练教师模型
    print("Training Teacher Model...")
    train_teacher(teacher, train_loader, epochs=5)
    
    # 训练学生模型（知识蒸馏）
    print("\nTraining Student Model with Knowledge Distillation...")
    train_student(teacher, student, train_loader, epochs=5)
    
    # 评估学生模型
    print("\nEvaluating Student Model...")
    evaluate_and_visualize(student, test_loader)

if __name__ == "__main__":
    main()
```

### 代码说明：
1. **数据集**：
   - 使用MNIST手写数字数据集（60,000训练样本，10,000测试样本，28x28灰度图像，10类）。
   - 数据通过`torchvision`加载，应用标准归一化（ToTensor）。

2. **模型结构**：
   - **教师模型**（CNN）：两层卷积（16和32通道，带ReLU和MaxPool），接全连接层，输出10维分类分数。
   - **学生模型**（MLP）：两层全连接（784→128→10，带ReLU），参数量远小于教师模型。
   - 教师模型提供软标签，学生模型学习硬标签（真实标签）和软标签（教师输出）。

3. **知识蒸馏损失**：
   - 使用`DistillationLoss`，结合交叉熵损失（硬标签）和KL散度损失（软标签）。
   - 超参数：温度`T=2.0`（软化概率分布），`alpha=0.5`（平衡硬标签和软标签损失）。

4. **训练**：
   - 教师模型：用交叉熵损失训练5个epoch，Adam优化器，学习率0.001。
   - 学生模型：用知识蒸馏损失训练5个epoch，教师模型固定，学生模型学习。
   - 每轮打印平均损失。

5. **评估与可视化**：
   - **评估**：在测试集上计算学生模型的分类准确率。
   - **可视化**：绘制学生模型的混淆矩阵（10x10，展示预测和真实标签的分布），保存为`mnist_confusion_matrix.png`。
   - **预测**：打印前5个测试样本的真实标签和预测标签。
   - 混淆矩阵对角线值越大，表示分类越准确。

6. **依赖**：
   - 需安装`torch`、`torchvision`、`sklearn`、`matplotlib`、`seaborn`（`pip install torch torchvision scikit-learn matplotlib seaborn`）。
   - MNIST数据集会自动下载到`./data`目录。

### 运行结果：
- 输出教师和学生模型的训练损失（每轮）。
- 输出学生模型的测试集分类准确率。
- 生成`mnist_confusion_matrix.png`，展示学生模型的分类性能（混淆矩阵）。
- 打印前5个测试样本的真实和预测标签。

### 注意：
- 混淆矩阵保存在运行目录下，可用图像查看器检查，蓝色深浅表示预测数量。
- 模型简单（教师为小型CNN，学生为小型MLP），适合展示知识蒸馏概念；实际应用可增加模型复杂性或调整超参数（如温度、alpha）。
