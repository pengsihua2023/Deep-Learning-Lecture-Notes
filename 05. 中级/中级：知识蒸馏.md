## Knowledge Distillation (知识蒸馏）

在深度学习中，**知识蒸馏（Knowledge Distillation, KD）**是一种**模型压缩与迁移学习技术**，其核心思想是：
通过训练一个较小、较轻量的模型（通常称为 **学生模型 Student**），去模仿一个较大、性能更强的模型（称为 **教师模型 Teacher**）的输出或中间表示，从而让学生模型继承教师模型的知识。
这样，学生模型在保持较高精度的同时，能显著减少计算开销和存储需求，非常适合在资源受限的场景（如移动端、嵌入式设备）中部署。
<div align="center">
<img width="600" height="240" alt="image" src="https://github.com/user-attachments/assets/2a79095f-2443-4a08-a7fa-a1a091bba957" />
</div>

<div align="center">
(此图引自Internet。)
</div> 


## 知识蒸馏的基本原理

1. **教师模型（Teacher）**
   通常是一个在大规模数据集上训练过的高性能模型，可能非常庞大（如 BERT、ResNet-152）。

2. **学生模型（Student）**
   通常是一个参数更少、更高效的模型（如小型 CNN、MobileNet）。

3. **蒸馏过程（Distillation）**

   * 学生模型不仅学习训练数据的**真实标签（hard labels）**，
   * 还学习教师模型输出的**软标签（soft labels）**。

     - “软标签”是指教师模型在分类时输出的概率分布，而不是单一的正确类别标签。
     - 例如：真实标签是“猫”，教师模型可能输出 `[猫:0.85, 狗:0.10, 狐狸:0.05]`，这包含了更多类别间相似度信息。

## 知识蒸馏的类型

1. **离线蒸馏（Offline Distillation）**
   先训练好教师模型，再用它来指导学生模型。

2. **在线蒸馏（Online Distillation）**
   教师和学生同时训练，教师可能是一个动态更新的集成模型。

3. **自蒸馏（Self-Distillation）**
   学生与教师结构相同，甚至参数共享；模型通过不同层或不同阶段自我指导。

4. **特征蒸馏（Feature Distillation）**
   不只学习输出概率，还让学生模仿教师的中间特征表示或注意力图。

## 应用场景

* **模型压缩**：让轻量模型接近大模型性能。
* **部署优化**：在移动设备、嵌入式系统上运行高效模型。
* **半监督学习**：利用教师模型在未标注数据上的预测，生成伪标签帮助学生训练。
* **集成学习简化**：用一个大集成模型蒸馏到单一学生模型，降低推理成本。

---

## 知识蒸馏的数学描述

### 基本思想

知识蒸馏的目标是通过让 **学生模型（Student, S）** 模仿 **教师模型（Teacher, T）** 的输出分布，从而在保证精度的前提下降低模型复杂度。

### 数学公式

#### 1. Softmax 与温度缩放

教师模型输出的 logits 记为 $z^{(T)}$，学生模型输出的 logits 记为 $z^{(S)}$。
通过 softmax 函数并引入温度参数 $T > 1$ 来平滑概率分布：

$$
p_i^{(T)} = \frac{\exp\left(\frac{z_i^{(T)}}{T}\right)}{\sum_j \exp\left(\frac{z_j^{(T)}}{T}\right)}, 
\quad 
p_i^{(S)} = \frac{\exp\left(\frac{z_i^{(S)}}{T}\right)}{\sum_j \exp\left(\frac{z_j^{(S)}}{T}\right)}.
$$

当 $T$ 越大，分布越平滑，能更好体现类别间的相对关系。


#### 2. 蒸馏损失 (Distillation Loss)

学生模型需要拟合教师模型的 soft target 分布，常用 **KL 散度**：

$$
\mathcal{L}_{\text{KD}} = T^2 \cdot \mathrm{KL}\big(p^{(T)} \,\|\, p^{(S)}\big) 
= T^2 \sum_i p_i^{(T)} \log \frac{p_i^{(T)}}{p_i^{(S)}}.
$$

其中 $T^2$ 是一个缩放因子，保证梯度大小与温度无关。


#### 3. 总损失函数

实际训练时通常结合 **真实标签的交叉熵损失** 与 **蒸馏损失**：

<img width="300" height="45" alt="image" src="https://github.com/user-attachments/assets/6efedb7f-b924-4cb4-a0df-ba94b8792732" />
 

其中：

* $\mathcal{L}_ {\text {CE}}(y, p^{(S)}_{T=1}) = - \sum_i y_i \log p_i^{(S)}$，使用真实标签 $y$；
* $\alpha \in [0,1]$ 控制真实标签和教师信号的权重。

---

## 代码

这是一个基于PyTorch的最简单Knowledge Distillation（知识蒸馏）示例，使用真实数据集（MNIST手写数字数据集），实现从一个较大的教师模型（CNN）到较小的学生模型（MLP）的知识蒸馏。任务是数字分类，知识蒸馏通过教师模型的软标签指导学生模型学习。结果将通过评估分类准确率和可视化学生模型的预测混淆矩阵来展示。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

# 教师模型（较大的CNN）
class TeacherCNN(nn.Module):
    def __init__(self):
        super(TeacherCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.fc = nn.Linear(32 * 7 * 7, 10)
    
    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# 学生模型（较小的MLP）
class StudentMLP(nn.Module):
    def __init__(self):
        super(StudentMLP, self).__init__()
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.fc(x)

# 知识蒸馏损失
class DistillationLoss(nn.Module):
    def __init__(self, temperature=2.0, alpha=0.5):
        super(DistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_logits, teacher_logits, labels):
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        distillation_loss = self.kl_loss(soft_student, soft_teacher) * (self.temperature ** 2)
        ce_loss = self.ce_loss(student_logits, labels)
        return self.alpha * distillation_loss + (1 - self.alpha) * ce_loss

# 数据加载
def get_data_loaders():
    transform = transforms.ToTensor()
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)
    return train_loader, test_loader

# 训练教师模型
def train_teacher(model, train_loader, epochs=5):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Teacher Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}')

# 训练学生模型（知识蒸馏）
def train_student(teacher, student, train_loader, epochs=5):
    optimizer = optim.Adam(student.parameters(), lr=0.001)
    criterion = DistillationLoss(temperature=2.0, alpha=0.5)
    
    teacher.eval()
    student.train()
    for epoch in range(epochs):
        total_loss = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            with torch.no_grad():
                teacher_logits = teacher(images)
            student_logits = student(images)
            loss = criterion(student_logits, teacher_logits, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Student Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}')

# 评估和可视化
def evaluate_and_visualize(model, test_loader):
    model.eval()
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    # 计算准确率
    accuracy = accuracy_score(true_labels, predictions)
    print(f'Test Accuracy: {accuracy:.4f}')
    
    # 绘制混淆矩阵
    cm = confusion_matrix(true_labels, predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix of Student Model on MNIST')
    plt.savefig('mnist_confusion_matrix.png')
    plt.close()
    print("Confusion matrix saved as 'mnist_confusion_matrix.png'")
    
    # 打印前几个样本的预测结果
    print("\nSample Predictions (First 5):")
    for i in range(5):
        print(f"Sample {i+1}: True Label={true_labels[i]}, Predicted Label={predictions[i]}")

def main():
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 初始化模型
    teacher = TeacherCNN().to(device)
    student = StudentMLP().to(device)
    
    # 加载数据
    train_loader, test_loader = get_data_loaders()
    
    # 训练教师模型
    print("Training Teacher Model...")
    train_teacher(teacher, train_loader, epochs=5)
    
    # 训练学生模型（知识蒸馏）
    print("\nTraining Student Model with Knowledge Distillation...")
    train_student(teacher, student, train_loader, epochs=5)
    
    # 评估学生模型
    print("\nEvaluating Student Model...")
    evaluate_and_visualize(student, test_loader)

if __name__ == "__main__":
    main()
```

### 代码说明：
1. **数据集**：
   - 使用MNIST手写数字数据集（60,000训练样本，10,000测试样本，28x28灰度图像，10类）。
   - 数据通过`torchvision`加载，应用标准归一化（ToTensor）。

2. **模型结构**：
   - **教师模型**（CNN）：两层卷积（16和32通道，带ReLU和MaxPool），接全连接层，输出10维分类分数。
   - **学生模型**（MLP）：两层全连接（784→128→10，带ReLU），参数量远小于教师模型。
   - 教师模型提供软标签，学生模型学习硬标签（真实标签）和软标签（教师输出）。

3. **知识蒸馏损失**：
   - 使用`DistillationLoss`，结合交叉熵损失（硬标签）和KL散度损失（软标签）。
   - 超参数：温度`T=2.0`（软化概率分布），`alpha=0.5`（平衡硬标签和软标签损失）。

4. **训练**：
   - 教师模型：用交叉熵损失训练5个epoch，Adam优化器，学习率0.001。
   - 学生模型：用知识蒸馏损失训练5个epoch，教师模型固定，学生模型学习。
   - 每轮打印平均损失。

5. **评估与可视化**：
   - **评估**：在测试集上计算学生模型的分类准确率。
   - **可视化**：绘制学生模型的混淆矩阵（10x10，展示预测和真实标签的分布），保存为`mnist_confusion_matrix.png`。
   - **预测**：打印前5个测试样本的真实标签和预测标签。
   - 混淆矩阵对角线值越大，表示分类越准确。

6. **依赖**：
   - 需安装`torch`、`torchvision`、`sklearn`、`matplotlib`、`seaborn`（`pip install torch torchvision scikit-learn matplotlib seaborn`）。
   - MNIST数据集会自动下载到`./data`目录。

### 运行结果：
- 输出教师和学生模型的训练损失（每轮）。
- 输出学生模型的测试集分类准确率。
- 生成`mnist_confusion_matrix.png`，展示学生模型的分类性能（混淆矩阵）。
- 打印前5个测试样本的真实和预测标签。

### 注意：
- 混淆矩阵保存在运行目录下，可用图像查看器检查，蓝色深浅表示预测数量。
- 模型简单（教师为小型CNN，学生为小型MLP），适合展示知识蒸馏概念；实际应用可增加模型复杂性或调整超参数（如温度、alpha）。
