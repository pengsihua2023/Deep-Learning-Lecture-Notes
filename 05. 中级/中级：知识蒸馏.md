## Knowledge Distillation (çŸ¥è¯†è’¸é¦ï¼‰

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œ**çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillation, KDï¼‰**æ˜¯ä¸€ç§**æ¨¡å‹å‹ç¼©ä¸è¿ç§»å­¦ä¹ æŠ€æœ¯**ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
é€šè¿‡è®­ç»ƒä¸€ä¸ªè¾ƒå°ã€è¾ƒè½»é‡çš„æ¨¡å‹ï¼ˆé€šå¸¸ç§°ä¸º **å­¦ç”Ÿæ¨¡å‹ Student**ï¼‰ï¼Œå»æ¨¡ä»¿ä¸€ä¸ªè¾ƒå¤§ã€æ€§èƒ½æ›´å¼ºçš„æ¨¡å‹ï¼ˆç§°ä¸º **æ•™å¸ˆæ¨¡å‹ Teacher**ï¼‰çš„è¾“å‡ºæˆ–ä¸­é—´è¡¨ç¤ºï¼Œä»è€Œè®©å­¦ç”Ÿæ¨¡å‹ç»§æ‰¿æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†ã€‚
è¿™æ ·ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨ä¿æŒè¾ƒé«˜ç²¾åº¦çš„åŒæ—¶ï¼Œèƒ½æ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€å’Œå­˜å‚¨éœ€æ±‚ï¼Œéå¸¸é€‚åˆåœ¨èµ„æºå—é™çš„åœºæ™¯ï¼ˆå¦‚ç§»åŠ¨ç«¯ã€åµŒå…¥å¼è®¾å¤‡ï¼‰ä¸­éƒ¨ç½²ã€‚
<div align="center">
<img width="600" height="240" alt="image" src="https://github.com/user-attachments/assets/2a79095f-2443-4a08-a7fa-a1a091bba957" />
</div>

<div align="center">
(æ­¤å›¾å¼•è‡ªInternetã€‚)
</div> 


## ğŸ“– çŸ¥è¯†è’¸é¦çš„åŸºæœ¬åŸç†

1. **æ•™å¸ˆæ¨¡å‹ï¼ˆTeacherï¼‰**
   é€šå¸¸æ˜¯ä¸€ä¸ªåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒè¿‡çš„é«˜æ€§èƒ½æ¨¡å‹ï¼Œå¯èƒ½éå¸¸åºå¤§ï¼ˆå¦‚ BERTã€ResNet-152ï¼‰ã€‚

2. **å­¦ç”Ÿæ¨¡å‹ï¼ˆStudentï¼‰**
   é€šå¸¸æ˜¯ä¸€ä¸ªå‚æ•°æ›´å°‘ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ï¼ˆå¦‚å°å‹ CNNã€MobileNetï¼‰ã€‚

3. **è’¸é¦è¿‡ç¨‹ï¼ˆDistillationï¼‰**

   * å­¦ç”Ÿæ¨¡å‹ä¸ä»…å­¦ä¹ è®­ç»ƒæ•°æ®çš„**çœŸå®æ ‡ç­¾ï¼ˆhard labelsï¼‰**ï¼Œ
   * è¿˜å­¦ä¹ æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„**è½¯æ ‡ç­¾ï¼ˆsoft labelsï¼‰**ã€‚

     - â€œè½¯æ ‡ç­¾â€æ˜¯æŒ‡æ•™å¸ˆæ¨¡å‹åœ¨åˆ†ç±»æ—¶è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å•ä¸€çš„æ­£ç¡®ç±»åˆ«æ ‡ç­¾ã€‚
     - ä¾‹å¦‚ï¼šçœŸå®æ ‡ç­¾æ˜¯â€œçŒ«â€ï¼Œæ•™å¸ˆæ¨¡å‹å¯èƒ½è¾“å‡º `[çŒ«:0.85, ç‹—:0.10, ç‹ç‹¸:0.05]`ï¼Œè¿™åŒ…å«äº†æ›´å¤šç±»åˆ«é—´ç›¸ä¼¼åº¦ä¿¡æ¯ã€‚

## ğŸ“– çŸ¥è¯†è’¸é¦çš„ç±»å‹

1. **ç¦»çº¿è’¸é¦ï¼ˆOffline Distillationï¼‰**
   å…ˆè®­ç»ƒå¥½æ•™å¸ˆæ¨¡å‹ï¼Œå†ç”¨å®ƒæ¥æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ã€‚

2. **åœ¨çº¿è’¸é¦ï¼ˆOnline Distillationï¼‰**
   æ•™å¸ˆå’Œå­¦ç”ŸåŒæ—¶è®­ç»ƒï¼Œæ•™å¸ˆå¯èƒ½æ˜¯ä¸€ä¸ªåŠ¨æ€æ›´æ–°çš„é›†æˆæ¨¡å‹ã€‚

3. **è‡ªè’¸é¦ï¼ˆSelf-Distillationï¼‰**
   å­¦ç”Ÿä¸æ•™å¸ˆç»“æ„ç›¸åŒï¼Œç”šè‡³å‚æ•°å…±äº«ï¼›æ¨¡å‹é€šè¿‡ä¸åŒå±‚æˆ–ä¸åŒé˜¶æ®µè‡ªæˆ‘æŒ‡å¯¼ã€‚

4. **ç‰¹å¾è’¸é¦ï¼ˆFeature Distillationï¼‰**
   ä¸åªå­¦ä¹ è¾“å‡ºæ¦‚ç‡ï¼Œè¿˜è®©å­¦ç”Ÿæ¨¡ä»¿æ•™å¸ˆçš„ä¸­é—´ç‰¹å¾è¡¨ç¤ºæˆ–æ³¨æ„åŠ›å›¾ã€‚

## ğŸ“– åº”ç”¨åœºæ™¯

* **æ¨¡å‹å‹ç¼©**ï¼šè®©è½»é‡æ¨¡å‹æ¥è¿‘å¤§æ¨¡å‹æ€§èƒ½ã€‚
* **éƒ¨ç½²ä¼˜åŒ–**ï¼šåœ¨ç§»åŠ¨è®¾å¤‡ã€åµŒå…¥å¼ç³»ç»Ÿä¸Šè¿è¡Œé«˜æ•ˆæ¨¡å‹ã€‚
* **åŠç›‘ç£å­¦ä¹ **ï¼šåˆ©ç”¨æ•™å¸ˆæ¨¡å‹åœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šçš„é¢„æµ‹ï¼Œç”Ÿæˆä¼ªæ ‡ç­¾å¸®åŠ©å­¦ç”Ÿè®­ç»ƒã€‚
* **é›†æˆå­¦ä¹ ç®€åŒ–**ï¼šç”¨ä¸€ä¸ªå¤§é›†æˆæ¨¡å‹è’¸é¦åˆ°å•ä¸€å­¦ç”Ÿæ¨¡å‹ï¼Œé™ä½æ¨ç†æˆæœ¬ã€‚



## ğŸ“– çŸ¥è¯†è’¸é¦çš„æ•°å­¦æè¿°

### åŸºæœ¬æ€æƒ³

çŸ¥è¯†è’¸é¦çš„ç›®æ ‡æ˜¯é€šè¿‡è®© **å­¦ç”Ÿæ¨¡å‹ï¼ˆStudent, Sï¼‰** æ¨¡ä»¿ **æ•™å¸ˆæ¨¡å‹ï¼ˆTeacher, Tï¼‰** çš„è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œåœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹é™ä½æ¨¡å‹å¤æ‚åº¦ã€‚

### æ•°å­¦å…¬å¼

#### 1. Softmax ä¸æ¸©åº¦ç¼©æ”¾

æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„ logits è®°ä¸º $z^{(T)}$ï¼Œå­¦ç”Ÿæ¨¡å‹è¾“å‡ºçš„ logits è®°ä¸º $z^{(S)}$ã€‚
é€šè¿‡ softmax å‡½æ•°å¹¶å¼•å…¥æ¸©åº¦å‚æ•° $T > 1$ æ¥å¹³æ»‘æ¦‚ç‡åˆ†å¸ƒï¼š

$$
p_i^{(T)} = \frac{\exp\left(\frac{z_i^{(T)}}{T}\right)}{\sum_j \exp\left(\frac{z_j^{(T)}}{T}\right)}, 
\quad 
p_i^{(S)} = \frac{\exp\left(\frac{z_i^{(S)}}{T}\right)}{\sum_j \exp\left(\frac{z_j^{(S)}}{T}\right)}.
$$

å½“ $T$ è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šå¹³æ»‘ï¼Œèƒ½æ›´å¥½ä½“ç°ç±»åˆ«é—´çš„ç›¸å¯¹å…³ç³»ã€‚


#### 2. è’¸é¦æŸå¤± (Distillation Loss)

å­¦ç”Ÿæ¨¡å‹éœ€è¦æ‹Ÿåˆæ•™å¸ˆæ¨¡å‹çš„ soft target åˆ†å¸ƒï¼Œå¸¸ç”¨ **KL æ•£åº¦**ï¼š

$$
\mathcal{L}_{\text{KD}} = T^2 \cdot \mathrm{KL}\big(p^{(T)} \,\|\, p^{(S)}\big) 
= T^2 \sum_i p_i^{(T)} \log \frac{p_i^{(T)}}{p_i^{(S)}}.
$$

å…¶ä¸­ $T^2$ æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œä¿è¯æ¢¯åº¦å¤§å°ä¸æ¸©åº¦æ— å…³ã€‚


#### 3. æ€»æŸå¤±å‡½æ•°

å®é™…è®­ç»ƒæ—¶é€šå¸¸ç»“åˆ **çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±** ä¸ **è’¸é¦æŸå¤±**ï¼š

<img width="300" height="45" alt="image" src="https://github.com/user-attachments/assets/6efedb7f-b924-4cb4-a0df-ba94b8792732" />
 

å…¶ä¸­ï¼š

* $\mathcal{L}_ {\text {CE}}(y, p^{(S)}_{T=1}) = - \sum_i y_i \log p_i^{(S)}$ï¼Œä½¿ç”¨çœŸå®æ ‡ç­¾ $y$ï¼›
* $\alpha \in [0,1]$ æ§åˆ¶çœŸå®æ ‡ç­¾å’Œæ•™å¸ˆä¿¡å·çš„æƒé‡ã€‚

---

## ğŸ“– ä»£ç 

è¿™æ˜¯ä¸€ä¸ªåŸºäºPyTorchçš„æœ€ç®€å•Knowledge Distillationï¼ˆçŸ¥è¯†è’¸é¦ï¼‰ç¤ºä¾‹ï¼Œä½¿ç”¨çœŸå®æ•°æ®é›†ï¼ˆMNISTæ‰‹å†™æ•°å­—æ•°æ®é›†ï¼‰ï¼Œå®ç°ä»ä¸€ä¸ªè¾ƒå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼ˆCNNï¼‰åˆ°è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ï¼ˆMLPï¼‰çš„çŸ¥è¯†è’¸é¦ã€‚ä»»åŠ¡æ˜¯æ•°å­—åˆ†ç±»ï¼ŒçŸ¥è¯†è’¸é¦é€šè¿‡æ•™å¸ˆæ¨¡å‹çš„è½¯æ ‡ç­¾æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹å­¦ä¹ ã€‚ç»“æœå°†é€šè¿‡è¯„ä¼°åˆ†ç±»å‡†ç¡®ç‡å’Œå¯è§†åŒ–å­¦ç”Ÿæ¨¡å‹çš„é¢„æµ‹æ··æ·†çŸ©é˜µæ¥å±•ç¤ºã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

# æ•™å¸ˆæ¨¡å‹ï¼ˆè¾ƒå¤§çš„CNNï¼‰
class TeacherCNN(nn.Module):
    def __init__(self):
        super(TeacherCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.fc = nn.Linear(32 * 7 * 7, 10)
    
    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# å­¦ç”Ÿæ¨¡å‹ï¼ˆè¾ƒå°çš„MLPï¼‰
class StudentMLP(nn.Module):
    def __init__(self):
        super(StudentMLP, self).__init__()
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.fc(x)

# çŸ¥è¯†è’¸é¦æŸå¤±
class DistillationLoss(nn.Module):
    def __init__(self, temperature=2.0, alpha=0.5):
        super(DistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.ce_loss = nn.CrossEntropyLoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_logits, teacher_logits, labels):
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        distillation_loss = self.kl_loss(soft_student, soft_teacher) * (self.temperature ** 2)
        ce_loss = self.ce_loss(student_logits, labels)
        return self.alpha * distillation_loss + (1 - self.alpha) * ce_loss

# æ•°æ®åŠ è½½
def get_data_loaders():
    transform = transforms.ToTensor()
    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)
    return train_loader, test_loader

# è®­ç»ƒæ•™å¸ˆæ¨¡å‹
def train_teacher(model, train_loader, epochs=5):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Teacher Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}')

# è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼ˆçŸ¥è¯†è’¸é¦ï¼‰
def train_student(teacher, student, train_loader, epochs=5):
    optimizer = optim.Adam(student.parameters(), lr=0.001)
    criterion = DistillationLoss(temperature=2.0, alpha=0.5)
    
    teacher.eval()
    student.train()
    for epoch in range(epochs):
        total_loss = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            with torch.no_grad():
                teacher_logits = teacher(images)
            student_logits = student(images)
            loss = criterion(student_logits, teacher_logits, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Student Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}')

# è¯„ä¼°å’Œå¯è§†åŒ–
def evaluate_and_visualize(model, test_loader):
    model.eval()
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(true_labels, predictions)
    print(f'Test Accuracy: {accuracy:.4f}')
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_labels, predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix of Student Model on MNIST')
    plt.savefig('mnist_confusion_matrix.png')
    plt.close()
    print("Confusion matrix saved as 'mnist_confusion_matrix.png'")
    
    # æ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ
    print("\nSample Predictions (First 5):")
    for i in range(5):
        print(f"Sample {i+1}: True Label={true_labels[i]}, Predicted Label={predictions[i]}")

def main():
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆå§‹åŒ–æ¨¡å‹
    teacher = TeacherCNN().to(device)
    student = StudentMLP().to(device)
    
    # åŠ è½½æ•°æ®
    train_loader, test_loader = get_data_loaders()
    
    # è®­ç»ƒæ•™å¸ˆæ¨¡å‹
    print("Training Teacher Model...")
    train_teacher(teacher, train_loader, epochs=5)
    
    # è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼ˆçŸ¥è¯†è’¸é¦ï¼‰
    print("\nTraining Student Model with Knowledge Distillation...")
    train_student(teacher, student, train_loader, epochs=5)
    
    # è¯„ä¼°å­¦ç”Ÿæ¨¡å‹
    print("\nEvaluating Student Model...")
    evaluate_and_visualize(student, test_loader)

if __name__ == "__main__":
    main()
```

## ğŸ“– ä»£ç è¯´æ˜ï¼š
1. **æ•°æ®é›†**ï¼š
   - ä½¿ç”¨MNISTæ‰‹å†™æ•°å­—æ•°æ®é›†ï¼ˆ60,000è®­ç»ƒæ ·æœ¬ï¼Œ10,000æµ‹è¯•æ ·æœ¬ï¼Œ28x28ç°åº¦å›¾åƒï¼Œ10ç±»ï¼‰ã€‚
   - æ•°æ®é€šè¿‡`torchvision`åŠ è½½ï¼Œåº”ç”¨æ ‡å‡†å½’ä¸€åŒ–ï¼ˆToTensorï¼‰ã€‚

2. **æ¨¡å‹ç»“æ„**ï¼š
   - **æ•™å¸ˆæ¨¡å‹**ï¼ˆCNNï¼‰ï¼šä¸¤å±‚å·ç§¯ï¼ˆ16å’Œ32é€šé“ï¼Œå¸¦ReLUå’ŒMaxPoolï¼‰ï¼Œæ¥å…¨è¿æ¥å±‚ï¼Œè¾“å‡º10ç»´åˆ†ç±»åˆ†æ•°ã€‚
   - **å­¦ç”Ÿæ¨¡å‹**ï¼ˆMLPï¼‰ï¼šä¸¤å±‚å…¨è¿æ¥ï¼ˆ784â†’128â†’10ï¼Œå¸¦ReLUï¼‰ï¼Œå‚æ•°é‡è¿œå°äºæ•™å¸ˆæ¨¡å‹ã€‚
   - æ•™å¸ˆæ¨¡å‹æä¾›è½¯æ ‡ç­¾ï¼Œå­¦ç”Ÿæ¨¡å‹å­¦ä¹ ç¡¬æ ‡ç­¾ï¼ˆçœŸå®æ ‡ç­¾ï¼‰å’Œè½¯æ ‡ç­¾ï¼ˆæ•™å¸ˆè¾“å‡ºï¼‰ã€‚

3. **çŸ¥è¯†è’¸é¦æŸå¤±**ï¼š
   - ä½¿ç”¨`DistillationLoss`ï¼Œç»“åˆäº¤å‰ç†µæŸå¤±ï¼ˆç¡¬æ ‡ç­¾ï¼‰å’ŒKLæ•£åº¦æŸå¤±ï¼ˆè½¯æ ‡ç­¾ï¼‰ã€‚
   - è¶…å‚æ•°ï¼šæ¸©åº¦`T=2.0`ï¼ˆè½¯åŒ–æ¦‚ç‡åˆ†å¸ƒï¼‰ï¼Œ`alpha=0.5`ï¼ˆå¹³è¡¡ç¡¬æ ‡ç­¾å’Œè½¯æ ‡ç­¾æŸå¤±ï¼‰ã€‚

4. **è®­ç»ƒ**ï¼š
   - æ•™å¸ˆæ¨¡å‹ï¼šç”¨äº¤å‰ç†µæŸå¤±è®­ç»ƒ5ä¸ªepochï¼ŒAdamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡0.001ã€‚
   - å­¦ç”Ÿæ¨¡å‹ï¼šç”¨çŸ¥è¯†è’¸é¦æŸå¤±è®­ç»ƒ5ä¸ªepochï¼Œæ•™å¸ˆæ¨¡å‹å›ºå®šï¼Œå­¦ç”Ÿæ¨¡å‹å­¦ä¹ ã€‚
   - æ¯è½®æ‰“å°å¹³å‡æŸå¤±ã€‚

5. **è¯„ä¼°ä¸å¯è§†åŒ–**ï¼š
   - **è¯„ä¼°**ï¼šåœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—å­¦ç”Ÿæ¨¡å‹çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚
   - **å¯è§†åŒ–**ï¼šç»˜åˆ¶å­¦ç”Ÿæ¨¡å‹çš„æ··æ·†çŸ©é˜µï¼ˆ10x10ï¼Œå±•ç¤ºé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾çš„åˆ†å¸ƒï¼‰ï¼Œä¿å­˜ä¸º`mnist_confusion_matrix.png`ã€‚
   - **é¢„æµ‹**ï¼šæ‰“å°å‰5ä¸ªæµ‹è¯•æ ·æœ¬çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ã€‚
   - æ··æ·†çŸ©é˜µå¯¹è§’çº¿å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºåˆ†ç±»è¶Šå‡†ç¡®ã€‚

6. **ä¾èµ–**ï¼š
   - éœ€å®‰è£…`torch`ã€`torchvision`ã€`sklearn`ã€`matplotlib`ã€`seaborn`ï¼ˆ`pip install torch torchvision scikit-learn matplotlib seaborn`ï¼‰ã€‚
   - MNISTæ•°æ®é›†ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°`./data`ç›®å½•ã€‚

## ğŸ“– è¿è¡Œç»“æœï¼š
- è¾“å‡ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒæŸå¤±ï¼ˆæ¯è½®ï¼‰ã€‚
- è¾“å‡ºå­¦ç”Ÿæ¨¡å‹çš„æµ‹è¯•é›†åˆ†ç±»å‡†ç¡®ç‡ã€‚
- ç”Ÿæˆ`mnist_confusion_matrix.png`ï¼Œå±•ç¤ºå­¦ç”Ÿæ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ï¼ˆæ··æ·†çŸ©é˜µï¼‰ã€‚
- æ‰“å°å‰5ä¸ªæµ‹è¯•æ ·æœ¬çš„çœŸå®å’Œé¢„æµ‹æ ‡ç­¾ã€‚

## ğŸ“– æ³¨æ„ï¼š
- æ··æ·†çŸ©é˜µä¿å­˜åœ¨è¿è¡Œç›®å½•ä¸‹ï¼Œå¯ç”¨å›¾åƒæŸ¥çœ‹å™¨æ£€æŸ¥ï¼Œè“è‰²æ·±æµ…è¡¨ç¤ºé¢„æµ‹æ•°é‡ã€‚
- æ¨¡å‹ç®€å•ï¼ˆæ•™å¸ˆä¸ºå°å‹CNNï¼Œå­¦ç”Ÿä¸ºå°å‹MLPï¼‰ï¼Œé€‚åˆå±•ç¤ºçŸ¥è¯†è’¸é¦æ¦‚å¿µï¼›å®é™…åº”ç”¨å¯å¢åŠ æ¨¡å‹å¤æ‚æ€§æˆ–è°ƒæ•´è¶…å‚æ•°ï¼ˆå¦‚æ¸©åº¦ã€alphaï¼‰ã€‚
