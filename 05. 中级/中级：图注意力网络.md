# 图注意力网络（Graph Attention Network, GAT）

## 一、定义

图注意力网络（Graph Attention Network, GAT）是一类图神经网络 (GNN)。它在图结构上通过**注意力机制**来计算邻居节点对目标节点表示的贡献，而不是像 GCN 那样用固定的归一化权重。这样可以让模型自动学习哪些邻居更重要。

---

## 二、数学描述

设图 $G=(V,E)$，节点集合 $V$，边集合 $E$。节点 $i$ 的输入特征为 $h_i \in \mathbb{R}^F$。

1. **线性映射**

   $$
   z_i = W h_i, \quad W \in \mathbb{R}^{F' \times F}
   $$

2. **注意力打分**（对 $i$ 的邻居 $j$，含自环）：

   $$
   e_{ij} = \text{LeakyReLU}\big(a^\top [z_i \, \| \, z_j]\big), \quad a \in \mathbb{R}^{2F'}
   $$

3. **归一化（邻居 softmax）**

   $$
   \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)\cup\{i\}} \exp(e_{ik})}
   $$

4. **聚合更新**

   $$
   h'_i = \sigma\!\left(\sum_{j \in \mathcal{N}(i)\cup\{i\}} \alpha_{ij}\, z_j \right)
   $$

多头注意力：重复上面的计算 $H$ 次，再拼接或平均。

---

## 三、最简代码（PyTorch，密集版）

> 适合小图、教学演示；大图请用稀疏实现。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GATLayer(nn.Module):
    def __init__(self, in_dim, out_dim, heads=1, concat=True, dropout=0.6, negative_slope=0.2):
        super().__init__()
        self.heads = heads
        self.out_dim = out_dim
        self.concat = concat

        # 线性变换
        self.W = nn.Linear(in_dim, heads * out_dim, bias=False)
        # 注意力参数 a: 每个头一个向量
        self.a = nn.Parameter(torch.empty(heads, 2 * out_dim))
        nn.init.xavier_uniform_(self.W.weight, gain=1.414)
        nn.init.xavier_uniform_(self.a, gain=1.414)

        self.leakyrelu = nn.LeakyReLU(negative_slope)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, adj):
        N = x.size(0)
        z = self.W(x).view(N, self.heads, self.out_dim)   # (N, H, F')

        # 构造 pair (i,j)
        z_i = z.unsqueeze(1).expand(N, N, self.heads, self.out_dim)
        z_j = z.unsqueeze(0).expand(N, N, self.heads, self.out_dim)
        e = self.leakyrelu(((torch.cat([z_i, z_j], dim=-1)) * self.a).sum(-1))  # (N,N,H)

        # 邻接矩阵加自环
        adj_hat = adj + torch.eye(N, device=adj.device)
        mask = adj_hat.unsqueeze(-1).expand_as(e)
        e = e.masked_fill(mask == 0, float('-inf'))

        # softmax
        alpha = torch.softmax(e, dim=1)  # (N,N,H)
        alpha = self.dropout(alpha)

        # 聚合
        out = (alpha.unsqueeze(-1) * z_j).sum(1)  # (N,H,F')

        if self.concat:
            return out.reshape(N, self.heads * self.out_dim)
        else:
            return out.mean(1)

# 小测试：两层 GAT
class TinyGAT(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, heads=2):
        super().__init__()
        self.gat1 = GATLayer(in_dim, hidden_dim, heads=heads, concat=True)
        self.gat2 = GATLayer(hidden_dim*heads, out_dim, heads=1, concat=False)

    def forward(self, x, adj):
        x = F.elu(self.gat1(x, adj))
        return self.gat2(x, adj)

if __name__ == "__main__":
    # 例子：4 节点小图
    adj = torch.tensor([
        [0,1,1,0],
        [1,0,1,1],
        [1,1,0,0],
        [0,1,0,0]
    ], dtype=torch.float32)

    x = torch.randn(4, 3)   # 节点特征
    model = TinyGAT(3, 8, 2, heads=2)
    out = model(x, adj)
    print(out)
```

输出结果是每个节点的 2 维表示，可直接接 softmax 做分类。

---



