
# GAT（Graph Attention Network，图注意力网络）

## 定义（直观版）

GAT 在每个节点上，用邻居特征做**注意力加权平均**来更新该节点的表示。权重由一个可学习的打分函数决定，而不是像 GCN 那样用固定的度数归一化。

## 数学描述（单头，含自环）

给定无向/有向图 $G=(V,E)$，节点特征 $h_i\in\mathbb{R}^{F}$ 。

1. 线性变换

$$
\mathbf{z}_i = \mathbf{W}\, \mathbf{h}_i,\qquad \mathbf{W}\in\mathbb{R}^{F'\times F}
$$

2. 注意力“生分”

$$
e_{ij} = \text{LeakyReLU}\Big(\mathbf{a}^\top [\mathbf{z}_i \,\|\, \mathbf{z}_j]\Big)
\quad \text{仅当 } j\in\mathcal{N}(i)\cup\{i\}
$$

其中 $\mathbf{a}\in\mathbb{R}^{2F'}$ ， $[\cdot\|\cdot]$ 是拼接。

3. 邻域 softmax 归一化

$$
\alpha_{ij}=\frac{\exp(e_{ij})}{\sum_{k\in\mathcal{N}(i)\cup\{i\}}\exp(e_{ik})}
$$

4. 聚合并激活

$$
\mathbf{h}_i'=\sigma\Big(\sum_{j\in\mathcal{N}(i)\cup\{i\}} \alpha_{ij} \mathbf{z}_j\Big)
$$

**多头**：做 $H$ 个独立头 $\{\alpha^{(h)}, \mathbf{W}^{(h)}, \mathbf{a}^{(h)}\}$ 。

* 中间层常用**拼接**：
  $\mathbf{h}_i'=\mathbin\Vert_{h=1}^H \sigma(\sum_j \alpha^{(h)}_{ij}\mathbf{z}^{(h)}_j)$
* 输出层常用**平均**：
  $\mathbf{h}_i'=\frac{1}{H}\sum_h \sum_j \alpha^{(h)}_{ij}\mathbf{z}^{(h)}_j$

## 最简可跑代码（密集版，适合看原理）

说明：为了把公式写清楚，这里用邻接矩阵 $A\in\{0,1\}^{N\times N}$ 做**掩码**，并自动加自环（ $I$ ）。复杂度 $O(N^2)$ ，只适合小图/教学；真实项目请改用稀疏边（edge\_index + segment softmax）或 PyTorch Geometric 的 `GATConv`。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DenseGATLayer(nn.Module):
    """
    教学用：密集版单层单/多头 GAT。
    x: [N, F] 节点特征
    A: [N, N] 0/1 邻接(无需自环, 本层会加)
    返回:
      - concat=True : [N, H*F_out]
      - concat=False: [N, F_out]
    """
    def __init__(self, in_dim, out_dim, heads=1, concat=True, negative_slope=0.2, dropout=0.0):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.heads = heads
        self.concat = concat
        self.leakyrelu = nn.LeakyReLU(negative_slope)
        self.dropout = nn.Dropout(dropout)

        # 线性映射 W: 统一做成 [F -> H*F']
        self.W = nn.Linear(in_dim, heads * out_dim, bias=False)
        # 注意力向量 a: 每个头各自一个 a ∈ R^{2F'}
        self.a = nn.Parameter(torch.empty(heads, 2 * out_dim))
        nn.init.xavier_uniform_(self.W.weight, gain=1.414)
        nn.init.xavier_uniform_(self.a, gain=1.414)

    def forward(self, x, A):
        N = x.size(0)
        device = x.device

        # 1) 线性变换并 reshape 为 [N, H, F']
        z = self.W(x).view(N, self.heads, self.out_dim)  # (N, H, F')

        # 2) 准备成 (i,j) 两两组合，便于算 e_ij
        # z_i: (N, N, H, F'), z_j: (N, N, H, F')
        z_i = z[:, None, :, :].expand(N, N, self.heads, self.out_dim)
        z_j = z[None, :, :, :].expand(N, N, self.heads, self.out_dim)

        # 拼接 [z_i || z_j] -> (N,N,H,2F')
        z_cat = torch.cat([z_i, z_j], dim=-1)

        # a: (H, 2F'), 先扩到 (1,1,H,2F')
        a = self.a.view(1, 1, self.heads, 2 * self.out_dim)

        # 3) 原始注意力分数 e_ij: (N,N,H)
        e = self.leakyrelu((z_cat * a).sum(dim=-1))

        # 仅保留邻居：加自环，A_hat = A ∨ I
        A_hat = (A > 0).float()
        A_hat = A_hat + torch.eye(N, device=device)
        A_hat = (A_hat > 0).float()  # (N,N)

        # 对非邻居置 -inf，做行 softmax（对每个 i 的邻域 j）
        # 扩到 (N,N,H) 以便逐头 mask
        mask = A_hat[:, :, None].expand_as(e)
        e_masked = e.masked_fill(mask == 0, float('-inf'))

        # 4) 邻域 softmax
        alpha = torch.softmax(e_masked, dim=1)  # (N,N,H)
        alpha = self.dropout(alpha)

        # 5) 加权聚合：sum_j α_ij * z_j
        # 先把 z_j 变成 (N,N,H,F')
        z_j = z[None, :, :, :].expand(N, N, self.heads, self.out_dim)
        out = (alpha[..., None] * z_j).sum(dim=1)  # (N,H,F')

        if self.concat:
            out = out.reshape(N, self.heads * self.out_dim)  # 拼接
        else:
            out = out.mean(dim=1)  # 平均
        return out

# ====== 演示：一个两层的小 GAT（第一层拼接，多头；第二层平均，单头） ======
class TinyGAT(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, heads=4):
        super().__init__()
        self.gat1 = DenseGATLayer(in_dim, hidden_dim, heads=heads, concat=True, dropout=0.6)
        self.gat2 = DenseGATLayer(hidden_dim*heads, out_dim, heads=1, concat=False, dropout=0.6)

    def forward(self, x, A):
        x = F.elu(self.gat1(x, A))
        x = self.gat2(x, A)
        return x

# ====== 小例子 ======
if __name__ == "__main__":
    torch.manual_seed(0)
    # 4 个节点的小图（无向）
    A = torch.tensor([
        [0,1,1,0],
        [1,0,1,1],
        [1,1,0,0],
        [0,1,0,0],
    ], dtype=torch.float32)

    # 每个节点 3 维输入
    x = torch.randn(4, 3)

    model = TinyGAT(in_dim=3, hidden_dim=8, out_dim=2, heads=2)
    out = model(x, A)          # [4, 2]
    probs = F.log_softmax(out, dim=-1)
    print("node logits:\n", out)
    print("log-probs:\n", probs)
```

### 小贴士

* 复杂度：上面“密集版”是 $O(N^2)$。真实任务里应使用**稀疏实现**（基于边列表 `edge_index` 的消息传递 + 分组 softmax），复杂度 $O(|E|F)$。
* 训练：分类任务时可把 `out` 接 cross-entropy；节点回归可用 MSE。
* 变体：GATv2 把打分改成 $ \mathbf{a}^\top \text{LeakyReLU}(\mathbf{W}_1\mathbf{z}_i + \mathbf{W}_2\mathbf{z}_j)$ 以避免原版的“静态排序”问题；实现方式与此相近。


