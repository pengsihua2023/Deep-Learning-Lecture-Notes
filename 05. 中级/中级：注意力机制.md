
# æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)
## ğŸ“– å¼•è¨€
æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰æ˜¯ä¸€ç§æ¨¡ä»¿äººç±»è§†è§‰å’Œè®¤çŸ¥ç³»ç»Ÿçš„æ–¹æ³•ï¼Œå®ƒå…è®¸ç¥ç»ç½‘ç»œåœ¨å¤„ç†è¾“å…¥æ•°æ®æ—¶é›†ä¸­æ³¨æ„åŠ›äºç›¸å…³çš„éƒ¨åˆ†ã€‚é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¥ç»ç½‘ç»œèƒ½å¤Ÿè‡ªåŠ¨åœ°å­¦ä¹ å¹¶é€‰æ‹©æ€§åœ°å…³æ³¨è¾“å…¥ä¸­çš„é‡è¦ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚  
<div align="center">
<img width="469" height="220" alt="image" src="https://github.com/user-attachments/assets/d5516323-dabc-4ba8-8c32-d1db9bc8e396" />
 </div> 
 
 <div align="center">
(æ­¤å›¾å¼•è‡ªInternetã€‚)
</div> 

## ğŸ“– é‡è¦æ€§
Transformer çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ çš„åŸºçŸ³ï¼Œè¡ç”Ÿå‡ºå¦‚ BERTã€GPT ç­‰æ¨¡å‹ï¼Œé©±åŠ¨äº† NLP å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚  
é«˜çº§ç­å¯ä»¥æ·±å…¥æ¢è®¨æ³¨æ„åŠ›æœºåˆ¶çš„å˜ç§ï¼ˆå¦‚å¤šå¤´æ³¨æ„åŠ›ã€è‡ªæˆ‘æ³¨æ„åŠ›ï¼‰ã€‚  
## ğŸ“– æ ¸å¿ƒæ¦‚å¿µ
æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹èšç„¦è¾“å…¥ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼ˆå¦‚å¥å­ä¸­çš„å…³é”®è¯ï¼‰ï¼Œé€šè¿‡â€œæŸ¥è¯¢-é”®-å€¼â€æœºåˆ¶è®¡ç®—æƒé‡ã€‚  
## ğŸ“– åº”ç”¨
èŠå¤©æœºå™¨äººï¼ˆå¦‚ Grokï¼‰ã€æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€‚  


## ğŸ“– æ³¨æ„åŠ›æœºåˆ¶çš„æ•°å­¦æè¿°

æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**åœ¨ä¿¡æ¯åºåˆ—ä¸­ä¸ºä¸åŒå…ƒç´ åˆ†é…ä¸åŒçš„æƒé‡ï¼Œä»è€Œçªå‡ºâ€œé‡è¦â€ä¿¡æ¯å¹¶æŠ‘åˆ¶â€œæ— å…³â€ä¿¡æ¯**ã€‚

## 1. è¾“å…¥è¡¨ç¤º

ç»™å®šä¸€ä¸ªè¾“å…¥å‘é‡åºåˆ—ï¼š

$$
X = [x_1, x_2, \dots, x_n], \quad x_i \in \mathbb{R}^d
$$

é€šè¿‡çº¿æ€§å˜æ¢å°†å…¶æ˜ å°„ä¸º **æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰ã€å€¼ï¼ˆValueï¼‰** å‘é‡ï¼š

$$
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
$$

å…¶ä¸­ï¼š

* $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ æ˜¯å¯å­¦ä¹ å‚æ•°ï¼›
* $Q, K, V \in \mathbb{R}^{n \times d_k}$ï¼›
* è¿™é‡Œ **$d_k$** è¡¨ç¤º **Key å‘é‡çš„ç»´åº¦**ï¼ˆé€šå¸¸ä¹Ÿç­‰äº Query çš„ç»´åº¦ï¼‰ï¼›

* $d_k = \frac{d_{\text{model}}}{h}, \quad \text{ç¼©æ”¾å› å­} = \sqrt{d_k}$ ã€‚

## 2. æ³¨æ„åŠ›æ‰“åˆ†å‡½æ•°

è®¡ç®— **ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆscoreï¼‰** æ¥è¡¡é‡ Query ä¸ Key çš„ç›¸å…³æ€§ï¼š

$$
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j^\top}{\sqrt{d_k}}
$$

å…¶ä¸­ $\sqrt{d_k}$ æ˜¯ç¼©æ”¾å› å­ï¼Œç”¨äºé˜²æ­¢æ•°å€¼è¿‡å¤§ã€‚

## 3. æƒé‡åˆ†å¸ƒï¼ˆSoftmaxï¼‰

å°†æ‰€æœ‰å¾—åˆ†è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š

$$
\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_{l=1}^n \exp(\text{score}(q_i, k_l))}
$$

å…¶ä¸­ $\alpha_{ij}$ è¡¨ç¤ºç¬¬ $i$ ä¸ª Query å¯¹ç¬¬ $j$ ä¸ª Key çš„æ³¨æ„åŠ›æƒé‡ã€‚

## 4. ä¸Šä¸‹æ–‡å‘é‡ï¼ˆåŠ æƒæ±‚å’Œï¼‰

æ ¹æ®æ³¨æ„åŠ›æƒé‡å¯¹ Value å‘é‡åŠ æƒï¼š

$$
z_i = \sum_{j=1}^n \alpha_{ij} v_j
$$

å¾—åˆ°æœ€ç»ˆçš„ä¸Šä¸‹æ–‡è¡¨ç¤º $z_i$ã€‚

## 5. çŸ©é˜µå½¢å¼ï¼ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰

ä¸Šè¿°æ­¥éª¤å¯ä»¥å†™æˆç´§å‡‘çš„çŸ©é˜µå½¢å¼ï¼š

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

## 6. è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰
<div align="center">
<img width="469" height="220" alt="image" src="https://github.com/user-attachments/assets/a1dae221-067b-4f1c-a57f-caf7af22fbb5" />
</div>

è‡ªæ³¨æ„åŠ›æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ç§ç‰¹ä¾‹ï¼Œå…¶ä¸­ **Query (Q)ã€Key (K)ã€Value (V)** éƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ— $X$ã€‚

å½¢å¼åŒ–è¡¨ç¤ºä¸ºï¼š

$$
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
$$

æ³¨æ„åŠ›è¾“å‡ºä¸ºï¼š

$$
\text{SelfAttention}(X) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

### ç›´è§‚ç†è§£

- **ç›®çš„**ï¼š  
  è‡ªæ³¨æ„åŠ›ä½¿åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®èƒ½å¤Ÿå…³æ³¨åˆ°æ‰€æœ‰å…¶ä»–ä½ç½®ï¼Œä»è€Œæ•æ‰ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚  

- **ç¤ºä¾‹**ï¼š  
  åœ¨å¥å­ *â€œThe cat sat on the matâ€* ä¸­ï¼Œè¯ *â€œcatâ€* å¯ä»¥å…³æ³¨åˆ° *â€œsatâ€* å’Œ *â€œmatâ€*ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ã€‚  

- **ä¼˜åŠ¿**ï¼š  
  ä¸åŒäºå¾ªç¯ç½‘ç»œï¼Œè‡ªæ³¨æ„åŠ›å¯ä»¥å¹¶è¡Œå¤„ç†æ‰€æœ‰ tokenï¼Œåœ¨æ•æ‰é•¿ç¨‹ä¾èµ–æ—¶æ›´é«˜æ•ˆã€‚  

## 7. å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰
<div align="center">
<img width="560" height="380" alt="image" src="https://github.com/user-attachments/assets/f2c01db3-28ea-4724-9a03-538cda1ebeb3" />
</div>

ä¸å…¶åªè®¡ç®—å•ä¸€çš„æ³¨æ„åŠ›å‡½æ•°ï¼Œå¤šå¤´æ³¨æ„åŠ›å…è®¸æ¨¡å‹åœ¨ä¸åŒä½ç½®çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´ä¸­å…±åŒå…³æ³¨ä¿¡æ¯ã€‚

å½¢å¼åŒ–è¡¨ç¤ºä¸ºï¼š

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

å…¶ä¸­æ¯ä¸ªæ³¨æ„åŠ›å¤´å®šä¹‰ä¸ºï¼š

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

å…¶ä¸­ $W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$ï¼Œè€Œ $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ æ˜¯å¯å­¦ä¹ çš„æŠ•å½±çŸ©é˜µã€‚

### ç›´è§‚ç†è§£

- **ä¸ºä»€ä¹ˆè¦æœ‰å¤šä¸ªå¤´ï¼Ÿ**  
  å•ä¸€æ³¨æ„åŠ›å‡½æ•°å¯èƒ½åœ¨æ•æ‰å…³ç³»ç±»å‹ä¸Šå—åˆ°é™åˆ¶ã€‚  
  å¤šä¸ªå¤´å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼ˆæˆ–ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»ï¼‰ã€‚  

- **å·¥ä½œåŸç†**ï¼š  
  æ¯ä¸ªå¤´å°†è¾“å…¥æŠ•å½±åˆ°ä¸€ä¸ªä½ç»´å­ç©ºé—´ï¼Œåº”ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œå¹¶è¾“å‡ºä¸Šä¸‹æ–‡å‘é‡ã€‚  
  æ‰€æœ‰å¤´çš„ç»“æœè¢«æ‹¼æ¥åï¼Œå†é€šè¿‡çº¿æ€§æŠ•å½±å½¢æˆæœ€ç»ˆè¡¨ç¤ºã€‚  

- **ä¼˜åŠ¿**ï¼š  
  å¤šå¤´æ³¨æ„åŠ›å¢å¼ºäº†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œæœ‰åŠ©äºæ•æ‰æ•°æ®ä¸­çš„å¤šæ ·åŒ–å…³ç³»ã€‚  

## 8. æ€»ç»“

* **Queryâ€“Key**ï¼šå†³å®šå…³æ³¨ä»€ä¹ˆï¼›
* **Softmax æƒé‡**ï¼šåˆ†é…æ³¨æ„åŠ›ï¼›
* **Value**ï¼šæ‰¿è½½ä¿¡æ¯ï¼›
* **Self-Attention**ï¼šå½“ Qã€Kã€V æ¥è‡ªåŒä¸€åºåˆ—æ—¶ï¼Œç”¨äºæ•æ‰åºåˆ—å†…éƒ¨ä¾èµ–ï¼›
* **Multi-Head Attention**ï¼šå¹¶è¡Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œä»¥æ•æ‰å¤šæ ·åŒ–æ¨¡å¼ï¼›
* **æœ€ç»ˆè¾“å‡º**ï¼šè¾“å…¥çš„åŠ æƒè¡¨ç¤ºã€‚

æ ¸å¿ƒå…¬å¼ä¸ºï¼š

$$
\boxed{  \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V  }
$$

---
## ğŸ“– ä»£ç 
æ·»åŠ æ³¨æ„åŠ›æƒé‡çš„å¯è§†åŒ–ï¼Œä½¿ç”¨ä¸€ä¸ªçƒ­å›¾æ¥å±•ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼Œå¸®åŠ©ç›´è§‚ç†è§£Attentionæœºåˆ¶å¦‚ä½•å…³æ³¨ä¸åŒè¯ä¹‹é—´çš„å…³ç³»ã€‚ä»£ç ä»åŸºäºIMDbæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨PyTorchå®ç°ç®€å•çš„ Scaled Dot-Product Attentionã€‚ç”±äºæ‚¨è¦æ±‚ç»“æœå¯è§†åŒ–ï¼Œå°†ç”Ÿæˆä¸€ä¸ªçƒ­å›¾ï¼Œæ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡ã€‚   

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import load_dataset
from torchtext.vocab import build_vocab_from_iterator
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class SimpleAttention(nn.Module):
    def __init__(self, dim):
        super(SimpleAttention, self).__init__()
        self.dim = dim
    
    def forward(self, query, key, value):
        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.dim ** 0.5)
        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, value)
        return output, attention_weights

def yield_tokens(dataset):
    for example in dataset:
        yield example['text'].lower().split()

def plot_attention_weights(attention_weights, tokens, title="Attention Weights Heatmap"):
    plt.figure(figsize=(10, 8))
    sns.heatmap(attention_weights, xticklabels=tokens, yticklabels=tokens, cmap='viridis')
    plt.title(title)
    plt.xlabel("Key Tokens")
    plt.ylabel("Query Tokens")
    plt.tight_layout()
    plt.savefig("attention_heatmap.png")
    plt.close()
    print("Attention heatmap saved as 'attention_heatmap.png'")

def main():
    # åŠ è½½IMDbæ•°æ®é›†
    dataset = load_dataset("imdb", split="train[:1000]")  # ä½¿ç”¨å‰1000æ¡è¯„è®º
    batch_size = 32
    max_length = 20  # ç¼©çŸ­åºåˆ—é•¿åº¦ä»¥ä¾¿å¯è§†åŒ–
    embed_dim = 64

    # æ„å»ºè¯æ±‡è¡¨
    vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=['<pad>', '<unk>'])
    vocab.set_default_index(vocab['<unk>'])

    # åˆ›å»ºè¯åµŒå…¥å±‚
    embedding = nn.Embedding(len(vocab), embed_dim)

    # å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•
    def text_pipeline(text):
        tokens = text.lower().split()[:max_length]
        tokens += ['<pad>'] * (max_length - len(tokens))
        return [vocab[token] for token in tokens]

    input_ids = torch.tensor([text_pipeline(example['text']) for example in dataset], dtype=torch.long)
    
    # è·å–è¯åµŒå…¥
    embedded = embedding(input_ids)  # [num_samples, max_length, embed_dim]
    
    # åˆå§‹åŒ–Attentionæ¨¡å‹
    model = SimpleAttention(embed_dim)
    
    # åˆ†æ‰¹å¤„ç†
    outputs = []
    attention_weights_list = []
    
    for i in range(0, len(dataset), batch_size):
        batch = embedded[i:i+batch_size]
        output, attention_weights = model(batch, batch, batch)
        outputs.append(output)
        attention_weights_list.append(attention_weights)
    
    outputs = torch.cat(outputs, dim=0)
    attention_weights = torch.cat(attention_weights_list, dim=0)
    
    # æ‰“å°åŸºæœ¬ä¿¡æ¯
    print("Dataset size:", len(dataset))
    print("Sample text:", dataset[0]['text'][:100] + "...")
    print("Output shape:", outputs.shape)
    print("Attention weights shape:", attention_weights.shape)
    
    # å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡
    first_attention = attention_weights[0].detach().numpy()  # [max_length, max_length]
    first_tokens = dataset[0]['text'].lower().split()[:max_length]
    first_tokens += ['<pad>'] * (max_length - len(first_tokens))
    plot_attention_weights(first_attention, first_tokens)

if __name__ == "__main__":
    main()
```

### ğŸ“– ç®€è¦è¯´æ˜ï¼š
1. **æ•°æ®é›†**ï¼šç»§ç»­ä½¿ç”¨IMDbæ•°æ®é›†çš„å‰1000æ¡è¯„è®ºï¼Œåºåˆ—é•¿åº¦ç¼©çŸ­è‡³20ï¼Œä»¥ä¾¿çƒ­å›¾æ›´æ˜“è¯»ã€‚
2. **å¯è§†åŒ–**ï¼šæ·»åŠ `plot_attention_weights`å‡½æ•°ï¼Œä½¿ç”¨`seaborn`ç»˜åˆ¶ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æƒé‡çƒ­å›¾ï¼Œä¿å­˜ä¸º`attention_heatmap.png`ã€‚
3. **çƒ­å›¾å†…å®¹**ï¼š
   - Xè½´å’ŒYè½´æ˜¾ç¤ºè¾“å…¥å¥å­çš„è¯ï¼ˆæˆ–`<pad>`ï¼‰ã€‚
   - é¢œè‰²æ·±æµ…è¡¨ç¤ºæ³¨æ„åŠ›æƒé‡å¤§å°ï¼ˆé€šè¿‡`viridis`é¢œè‰²æ˜ å°„ï¼‰ã€‚
   - çƒ­å›¾ç›´è§‚å±•ç¤ºå“ªäº›è¯åœ¨Attentionæœºåˆ¶ä¸­å¯¹å…¶ä»–è¯çš„å…³æ³¨ç¨‹åº¦æ›´é«˜ã€‚
4. **ä¾èµ–**ï¼šéœ€å®‰è£…`datasets`ã€`torchtext`ã€`matplotlib`å’Œ`seaborn`ï¼ˆ`pip install datasets torchtext matplotlib seaborn`ï¼‰ã€‚

### ğŸ“– è¿è¡Œç»“æœï¼š
- ç¨‹åºå°†å¤„ç†1000æ¡IMDbè¯„è®ºï¼Œè¾“å‡ºæ•°æ®é›†ä¿¡æ¯ã€è¾“å‡ºå¼ é‡å½¢çŠ¶å’Œæ³¨æ„åŠ›æƒé‡å½¢çŠ¶ã€‚
- ç”Ÿæˆä¸€ä¸ªçƒ­å›¾æ–‡ä»¶`attention_heatmap.png`ï¼Œå±•ç¤ºç¬¬ä¸€ä¸ªè¯„è®ºçš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚
- çƒ­å›¾ä¸­çš„æ¯ä¸ªå•å…ƒæ ¼è¡¨ç¤ºqueryè¯å¯¹keyè¯çš„æ³¨æ„åŠ›æƒé‡ï¼Œé¢œè‰²è¶Šäº®è¡¨ç¤ºæƒé‡è¶Šå¤§ã€‚

### ğŸ“– æ³¨æ„ï¼š
- çƒ­å›¾æ–‡ä»¶ä¿å­˜åœ¨è¿è¡Œç›®å½•ä¸‹ï¼Œå¯ç”¨å›¾åƒæŸ¥çœ‹å™¨æ‰“å¼€ã€‚
- ç”±äºåºåˆ—é•¿åº¦é™åˆ¶ä¸º20ï¼Œçƒ­å›¾æ˜¾ç¤ºå‰20ä¸ªè¯çš„æ³¨æ„åŠ›å…³ç³»ï¼Œé€‚åˆç›´è§‚åˆ†æã€‚

