

## 📘 多任务学习的数学形式化描述

假设有 $T$ 个任务，每个任务 $t \in \{1,\dots,T\}$ 对应一个数据集

$$
\mathcal{D}_t = \left\lbrace (x_i^t, y_i^t) \right\rbrace_{i=1}^{N_t},
$$

其中 $N_t$ 表示任务 $t$ 的样本数量，$x_i^t$ 是输入，$y_i^t$ 是对应的标签。

---

### 1. 单任务损失函数

对于每个任务 $t$，我们定义其经验风险（经验损失）为

$$
\mathcal{L}_t(\theta) \;=\; \frac{1}{N_t}\sum_{i=1}^{N_t} \, \ell\!\big(f_\theta(x_i^t), y_i^t\big),
$$

其中：

* $f_\theta$ 表示共享参数 $\theta$ 的模型（或一组模型），
* $\ell(\cdot,\cdot)$ 是单样本损失函数（例如平方误差、交叉熵）。

---

### 2. 多任务优化目标

在多任务学习中，我们希望在所有任务上同时优化，通常通过加权求和得到整体目标函数：

$$
\min_{\theta} \; \sum_{t=1}^{T} \lambda_t \, \mathcal{L}_t(\theta),
$$

其中 $\lambda_t \ge 0$ 是任务 $t$ 的权重，用来平衡不同任务的重要性。

---

### 3. 总结

* **数据集层面**：每个任务 $t$ 有自己的数据集 $\mathcal{D}_t$。
* **损失层面**：基于数据集 $\mathcal{D}_t$，定义任务损失 $\mathcal{L}_t(\theta)$。
* **优化层面**：通过加权组合各任务损失，得到多任务学习的最终目标函数。

---


