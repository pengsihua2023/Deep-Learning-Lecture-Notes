# å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMulti-Query Attention, MQAï¼‰

å®ƒæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰çš„ä¸€ä¸ªå˜ä½“ï¼Œç”¨äºæå‡æ•ˆç‡ã€‚  
<div align="center">
<img width="660" height="260" alt="image" src="https://github.com/user-attachments/assets/27daf7ab-99a9-4cf7-853c-682ac6baf531" />
</div>


## 1. å®šä¹‰

åœ¨æ ‡å‡† **å¤šå¤´æ³¨æ„åŠ› (MHA)** ä¸­ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„ $Q, K, V$ æŠ•å½±çŸ©é˜µï¼Œè®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨éƒ½æ¯”è¾ƒå¤§ã€‚

**å¤šæŸ¥è¯¢æ³¨æ„åŠ› (MQA)** çš„æ ¸å¿ƒæ”¹è¿›æ˜¯ï¼š

* æ¯ä¸ªæ³¨æ„åŠ›å¤´ **æœ‰ç‹¬ç«‹çš„æŸ¥è¯¢ (Query) æŠ•å½±**ï¼›
* ä½† **å…±äº«åŒä¸€ä¸ªé”® (Key) å’Œå€¼ (Value) æŠ•å½±**ã€‚

è¿™æ ·ï¼š

* è®¡ç®— KV çš„å¼€é”€å’Œå­˜å‚¨ä» $O(h \cdot d)$ å‡å°‘åˆ° $O(d)$ï¼›
* æ³¨æ„åŠ›å¤´ä¹‹é—´ä»èƒ½é€šè¿‡ä¸åŒçš„æŸ¥è¯¢ $Q_i$ æ•æ‰ä¸åŒçš„ç‰¹å¾ï¼›
* ç‰¹åˆ«é€‚åˆå¤§æ¨¡å‹æ¨ç†æ—¶èŠ‚çœæ˜¾å­˜å’Œå»¶è¿Ÿï¼ˆå¦‚ PaLMã€LLaMA ç­‰éƒ½ç”¨åˆ°ï¼‰ã€‚



## 2. æ•°å­¦æè¿°

è®¾ï¼š

* è¾“å…¥åºåˆ— $X \in \mathbb{R}^{n \times d_{\text{model}}}$
* å¤´æ•° $h$ï¼Œæ¯ä¸ªå¤´ç»´åº¦ $d_k = d_{\text{model}} / h$

1. **æŠ•å½±**

$$
Q_i = X W_i^Q \quad (i=1,\dots,h), \quad K = X W^K, \quad V = X W^V
$$

å…¶ä¸­ï¼š

* $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$ï¼ˆæ¯ä¸ªå¤´ç‹¬ç«‹ï¼‰
* $W^K, W^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$ï¼ˆæ‰€æœ‰å¤´å…±äº«ï¼‰

2. **æ³¨æ„åŠ›è®¡ç®—**
   å¯¹æ¯ä¸ªå¤´ï¼š

$$
\text{head}_i = \text{softmax}\left(\frac{Q_i K^\top}{\sqrt{d_k}}\right)V
$$

3. **æ‹¼æ¥è¾“å‡º**

$$
\text{MQA}(X) = \text{Concat}(\text{head}_1,\dots,\text{head}_h) W^O
$$

å…¶ä¸­ $W^O \in \mathbb{R}^{hd_k \times d_{\text{model}}}$ã€‚



## 3. æœ€ç®€ PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiQueryAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # æ¯ä¸ªå¤´ç‹¬ç«‹çš„ Q
        self.W_q = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])
        # å…±äº«çš„ K å’Œ V
        self.W_k = nn.Linear(d_model, self.d_k)
        self.W_v = nn.Linear(d_model, self.d_k)
        # è¾“å‡ºæŠ•å½±
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        batch_size, seq_len, _ = x.size()

        # å…±äº« K, V
        K = self.W_k(x)  # (batch, seq_len, d_k)
        V = self.W_v(x)  # (batch, seq_len, d_k)

        heads = []
        for Wq in self.W_q:
            Q = Wq(x)  # (batch, seq_len, d_k)
            scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
            attn = F.softmax(scores, dim=-1)
            heads.append(torch.matmul(attn, V))  # (batch, seq_len, d_k)

        # æ‹¼æ¥æ‰€æœ‰å¤´
        out = torch.cat(heads, dim=-1)  # (batch, seq_len, d_model)
        return self.W_o(out)

# æµ‹è¯•
x = torch.rand(2, 5, 16)  # batch=2, seq_len=5, d_model=16
mqa = MultiQueryAttention(d_model=16, num_heads=4)
y = mqa(x)
print(y.shape)  # (2, 5, 16)
```



ğŸ‘‰ æ€»ç»“ä¸€ä¸‹ï¼š

* **MHA**ï¼šæ¯ä¸ªå¤´æœ‰ç‹¬ç«‹çš„ $Q,K,V$
* **MQA**ï¼šæ¯ä¸ªå¤´ç‹¬ç«‹ $Q$ï¼Œä½†å…±äº« $K,V$ï¼Œæ›´çœå†…å­˜å’Œè®¡ç®—



è¦ä¸è¦æˆ‘å†å¸®ä½ å†™ä¸€ä¸ª **å¯¹æ¯” MHA vs MQA çš„å¤æ‚åº¦å’Œæ˜¾å­˜å¼€é”€åˆ†æ**ï¼Œè¿™æ ·æ›´æ¸…æ¥šä¸ºä»€ä¹ˆ MQA åœ¨æ¨ç†æ—¶å¿«ï¼Ÿ

