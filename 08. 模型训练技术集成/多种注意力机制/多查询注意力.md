# 多查询注意力（Multi-Query Attention, MQA）

它是多头注意力（MHA）的一个变体，用于提升效率。  
<div align="center">
<img width="660" height="260" alt="image" src="https://github.com/user-attachments/assets/27daf7ab-99a9-4cf7-853c-682ac6baf531" />
</div>


## 1. 定义

在标准 **多头注意力 (MHA)** 中，每个注意力头都有自己独立的 $Q, K, V$ 投影矩阵，计算复杂度和存储都比较大。

**多查询注意力 (MQA)** 的核心改进是：

* 每个注意力头 **有独立的查询 (Query) 投影**；
* 但 **共享同一个键 (Key) 和值 (Value) 投影**。

这样：

* 计算 KV 的开销和存储从 $O(h \cdot d)$ 减少到 $O(d)$；
* 注意力头之间仍能通过不同的查询 $Q_i$ 捕捉不同的特征；
* 特别适合大模型推理时节省显存和延迟（如 PaLM、LLaMA 等都用到）。



## 2. 数学描述

设：

* 输入序列 $X \in \mathbb{R}^{n \times d_{\text{model}}}$
* 头数 $h$，每个头维度 $d_k = d_{\text{model}} / h$

1. **投影**

$$
Q_i = X W_i^Q \quad (i=1,\dots,h), \quad K = X W^K, \quad V = X W^V
$$

其中：

* $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$（每个头独立）
* $W^K, W^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$（所有头共享）

2. **注意力计算**
   对每个头：

$$
\text{head}_i = \text{softmax}\left(\frac{Q_i K^\top}{\sqrt{d_k}}\right)V
$$

3. **拼接输出**

$$
\text{MQA}(X) = \text{Concat}(\text{head}_1,\dots,\text{head}_h) W^O
$$

其中 $W^O \in \mathbb{R}^{hd_k \times d_{\text{model}}}$。



## 3. 最简 PyTorch 实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiQueryAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # 每个头独立的 Q
        self.W_q = nn.ModuleList([nn.Linear(d_model, self.d_k) for _ in range(num_heads)])
        # 共享的 K 和 V
        self.W_k = nn.Linear(d_model, self.d_k)
        self.W_v = nn.Linear(d_model, self.d_k)
        # 输出投影
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        batch_size, seq_len, _ = x.size()

        # 共享 K, V
        K = self.W_k(x)  # (batch, seq_len, d_k)
        V = self.W_v(x)  # (batch, seq_len, d_k)

        heads = []
        for Wq in self.W_q:
            Q = Wq(x)  # (batch, seq_len, d_k)
            scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
            attn = F.softmax(scores, dim=-1)
            heads.append(torch.matmul(attn, V))  # (batch, seq_len, d_k)

        # 拼接所有头
        out = torch.cat(heads, dim=-1)  # (batch, seq_len, d_model)
        return self.W_o(out)

# 测试
x = torch.rand(2, 5, 16)  # batch=2, seq_len=5, d_model=16
mqa = MultiQueryAttention(d_model=16, num_heads=4)
y = mqa(x)
print(y.shape)  # (2, 5, 16)
```



👉 总结一下：

* **MHA**：每个头有独立的 $Q,K,V$
* **MQA**：每个头独立 $Q$，但共享 $K,V$，更省内存和计算



要不要我再帮你写一个 **对比 MHA vs MQA 的复杂度和显存开销分析**，这样更清楚为什么 MQA 在推理时快？

