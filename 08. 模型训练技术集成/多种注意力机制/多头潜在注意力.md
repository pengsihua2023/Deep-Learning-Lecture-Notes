# Multi-Head Latent Attention (MHLA，多头潜在注意力)

## 1. 定义

**Multi-Head Latent Attention** 是在一些高效 Transformer 变体（如 **Perceiver、Perceiver IO、Transformer-XL 改进方案** 等）中提出的注意力机制。它的核心思想是：

* 不直接在输入序列的所有 token 上做全连接的 self-attention（复杂度 $O(n^2)$），而是通过一组 **潜在变量（latent array）** 作为中间“瓶颈”。
* 输入序列通过注意力投影到固定长度的 latent 表示；再由 latent 表示进行后续 cross/self attention 处理。
* 多头结构则让 latent 空间也能从不同子空间并行学习特征。

这就好像引入了一组“记忆槽”或“查询点”，用来压缩输入序列的信息，同时保持表达能力。



## 2. 数学描述

设：

* 输入序列： $X \in \mathbb{R}^{n \times d_{\text{in}}}$
* 潜在向量（可训练参数，固定长度）： $L \in \mathbb{R}^{m \times d_{\text{model}}}$，其中 $m \ll n$

### Step 1: Cross-Attention (Latent Queries, Input Keys/Values)

对每个头 $i$：

$$
Q_i = L W_i^Q, \quad K_i = X W_i^K, \quad V_i = X W_i^V
$$

其中 $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$，其余同理。

注意力：

$$
\text{head}_i = \text{softmax}\left(\frac{Q_i K_i^\top}{\sqrt{d_k}}\right)V_i
$$

### Step 2: 多头拼接

$$
H = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

最终得到更新后的 latent 表示：

$$
L' = H
$$

在 Perceiver 架构中，还会在 latent 内部再做一次 **Latent Self-Attention**，即 $Q,K,V$ 都来自 $L$，从而反复更新 latent。

---

## 3. 最简代码实现（PyTorch）

这是一个极简的 **Multi-Head Latent Attention**：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadLatentAttention(nn.Module):
    def __init__(self, d_model, num_heads, n_latents, d_input):
        super().__init__()
        assert d_model % num_heads == 0
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.latents = nn.Parameter(torch.randn(n_latents, d_model))  # 可训练 latent

        # 投影
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_input, d_model)
        self.W_v = nn.Linear(d_input, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        """
        x: (batch, seq_len, d_input)
        """
        batch_size = x.size(0)
        L = self.latents.unsqueeze(0).expand(batch_size, -1, -1)  # (batch, n_latents, d_model)

        Q = self.W_q(L).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)

        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)
        return self.W_o(out)  # (batch, n_latents, d_model)

# 测试
x = torch.rand(2, 100, 32)  # batch=2, 序列长度=100, 输入维度=32
mhla = MultiHeadLatentAttention(d_model=64, num_heads=8, n_latents=16, d_input=32)
y = mhla(x)
print(y.shape)  # (2, 16, 64)
```

这里：

* 输入序列长度是 100，维度 32；
* 16 个 latent 作为中间表示；
* 输出就是 batch × latent 数 × d\_model。

