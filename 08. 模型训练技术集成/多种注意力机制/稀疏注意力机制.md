# 稀疏注意力（Sparse Attention）
它是对标准全连接注意力的一种优化，主要目的是 **降低计算复杂度**。  


## 1. 定义

在标准 **自注意力 (self-attention)** 中，每个 token 都和所有 $n$ 个 token 交互，因此计算量和显存都是 $O(n^2)$。

**稀疏注意力 (Sparse Attention)** 的核心思想是：

* 限制每个 token 只和一部分 token 交互，而不是全部；
* 通过设计特定的稀疏模式（例如局部窗口、stride、block-sparse、因果mask 等），在保持表达能力的同时，降低复杂度。

典型应用：

* Transformer-XL, Longformer, BigBird, Sparse Transformer 都用了稀疏注意力来处理长序列。



## 2. 数学描述

标准注意力：

$$
\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M \right)V
$$

其中 $M$ 是掩码矩阵（mask），如果是全连接注意力，$M=0$。

在稀疏注意力中，定义一个 **稀疏掩码矩阵** $M \in \{-\infty, 0\}^{n \times n}$：

* $M_{ij} = 0$，表示位置 $i$ 可以关注位置 $j$；
* $M_{ij} = -\infty$，表示禁止关注。

因此，稀疏注意力就是在 softmax 前加掩码：

$$
\alpha_{ij} = \frac{\exp\left(\frac{Q_i K_j^\top}{\sqrt{d_k}} + M_{ij}\right)}{\sum_{j'} \exp\left(\frac{Q_i K_{j'}^\top}{\sqrt{d_k}} + M_{ij'}\right)}
$$

输出：

$$
O_i = \sum_{j} \alpha_{ij} V_j
$$



## 3. 最简代码实现（PyTorch）

下面给出一个 **局部窗口稀疏注意力 (local window sparse attention)** 的最小实现：

```python
import torch
import torch.nn.functional as F

def sparse_attention(Q, K, V, window_size=4):
    """
    简化版稀疏注意力 (local window)
    Q, K, V: (batch, n, d)
    """
    batch, n, d = Q.shape
    scale = 1.0 / (d ** 0.5)

    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale  # (batch, n, n)

    # 构造稀疏mask（仅允许每个token看前后window_size个）
    mask = torch.full((n, n), float("-inf"))
    for i in range(n):
        start, end = max(0, i - window_size), min(n, i + window_size + 1)
        mask[i, start:end] = 0.0
    scores = scores + mask  # (batch, n, n)

    attn = F.softmax(scores, dim=-1)
    out = torch.matmul(attn, V)  # (batch, n, d)
    return out

# 测试
batch, n, d = 2, 10, 16
Q = torch.randn(batch, n, d)
K = torch.randn(batch, n, d)
V = torch.randn(batch, n, d)

out = sparse_attention(Q, K, V, window_size=2)
print(out.shape)  # (2, 10, 16)
```



## 4. 总结

* **全连接注意力**：复杂度 $O(n^2)$
* **稀疏注意力**：通过稀疏 mask 限制交互，复杂度可降到 $O(n \log n)$ 或 $O(n)$，常见模式：

  * **局部窗口**（local window, 如 Longformer）
  * **stride**（隔点关注）
  * **block-sparse**（分块稀疏，如 BigBird）
  * **随机稀疏**（random sparse）


