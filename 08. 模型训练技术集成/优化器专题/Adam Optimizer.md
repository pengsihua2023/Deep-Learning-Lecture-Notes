## è‡ªé€‚åº”å­¦ä¹ ç‡ ï¼ˆAdam Optimizerï¼‰
### ğŸ“– ä»€ä¹ˆæ˜¯è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆAdamä¼˜åŒ–å™¨ï¼‰ï¼Ÿ

Adamä¼˜åŒ–å™¨ï¼ˆAdaptive Moment Estimationï¼Œé€‚åº”æ€§çŸ©ä¼°è®¡ï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­å¹¿æ³›ä½¿ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼Œç»“åˆäº†åŠ¨é‡æ³•å’ŒRMSPropçš„ä¼˜ç‚¹ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°è°ƒæ•´å­¦ä¹ ç‡æ¥åŠ é€Ÿæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›ã€‚å®ƒç‰¹åˆ«é€‚åˆå¤„ç†ç¨€ç–æ¢¯åº¦æˆ–å™ªå£°è¾ƒå¤§çš„ä¼˜åŒ–é—®é¢˜ã€‚

### ğŸ“– æ ¸å¿ƒåŸç†
Adamé€šè¿‡è·Ÿè¸ªæ¢¯åº¦çš„ä¸€é˜¶çŸ©ï¼ˆå‡å€¼ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆæ–¹å·®ï¼‰çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼æ¥åŠ¨æ€è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚ä¸»è¦æ­¥éª¤ï¼š

1. **è®¡ç®—æ¢¯åº¦**ï¼šå¯¹æŸå¤±å‡½æ•°æ±‚å‚æ•°çš„æ¢¯åº¦ $g_t$ã€‚

2. **æ›´æ–°ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰**ï¼š

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$

ç±»ä¼¼åŠ¨é‡æ³•ã€‚

3. **æ›´æ–°äºŒé˜¶çŸ©ï¼ˆæ–¹å·®ï¼‰**ï¼š

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

ç±»ä¼¼ RMSPropã€‚

4. **åå·®æ ¡æ­£**ï¼šå¯¹ $m_t$ å’Œ $v_t$ è¿›è¡Œåå·®æ ¡æ­£ï¼Œç¡®ä¿åˆæœŸä¼°è®¡æ— åã€‚

5. **å‚æ•°æ›´æ–°**ï¼šä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡æ›´æ–°å‚æ•°ï¼š

$$
\theta_{t+1} = \theta_t - \eta \frac{m_t}{\sqrt{v_t} + \epsilon}
$$



* **å…¶ä¸­**:

  * $\eta$ï¼šåˆå§‹å­¦ä¹ ç‡ï¼ˆé€šå¸¸ 0.001ï¼‰ã€‚
  * $\beta_1, \beta_2$ï¼šåŠ¨é‡å‚æ•°ï¼ˆé€šå¸¸ 0.9 å’Œ 0.999ï¼‰ã€‚
  * $\epsilon$ï¼šå°å€¼é˜²æ­¢é™¤é›¶ï¼ˆé€šå¸¸ $1e^{-8}$ï¼‰ã€‚


### ğŸ“– ä¼˜åŠ¿
- **è‡ªé€‚åº”æ€§**ï¼šæ ¹æ®æ¢¯åº¦å†å²è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒå‚ã€‚
- **é«˜æ•ˆæ€§**ï¼šé€‚åˆå¤§è§„æ¨¡æ•°æ®é›†å’Œå¤æ‚æ¨¡å‹ï¼ˆå¦‚æ·±åº¦ç¥ç»ç½‘ç»œï¼‰ã€‚
- **ç¨³å®šæ€§**ï¼šå¯¹ç¨€ç–æ¢¯åº¦æˆ–å™ªå£°ä¼˜åŒ–é—®é¢˜è¡¨ç°è‰¯å¥½ã€‚

### ğŸ“– å±€é™æ€§

* åœ¨æŸäº›ä»»åŠ¡ä¸Šå¯èƒ½ä¸å¦‚ SGD + åŠ¨é‡ æ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚
* è¶…å‚æ•°ï¼ˆå¦‚ $\beta_1, \beta_2$ï¼‰ä»éœ€é€‚å½“é€‰æ‹©ã€‚


---

### ğŸ“– Pythonä»£ç ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„PyTorchç¤ºä¾‹ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨è®­ç»ƒä¸€ä¸ªå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œè§£å†³MNISTæ‰‹å†™æ•°å­—åˆ†ç±»é—®é¢˜ã€‚ä»£ç èšç„¦äºAdamçš„å®ç°ï¼Œä¿æŒç®€æ´ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# æ­¥éª¤1: å®šä¹‰ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥ï¼š28x28åƒç´ 
        self.fc2 = nn.Linear(128, 10)       # è¾“å‡ºï¼š10ç±»
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³è¾“å…¥
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# æ­¥éª¤2: åŠ è½½MNISTæ•°æ®é›†
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

# æ­¥éª¤3: åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’ŒAdamä¼˜åŒ–å™¨
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)

# æ­¥éª¤4: è®­ç»ƒå‡½æ•°
def train(epoch):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        output = model(data)
        loss = criterion(output, target)
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # ä½¿ç”¨Adamæ›´æ–°å‚æ•°
        total_loss += loss.item()
    print(f'Epoch {epoch}, Average Loss: {total_loss / len(train_loader):.4f}')

# æ­¥éª¤5: æµ‹è¯•å‡½æ•°
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

# æ­¥éª¤6: è¿è¡Œè®­ç»ƒå’Œæµ‹è¯•
epochs = 5
for epoch in range(1, epochs + 1):
    train(epoch)
    test()
```

---

### ğŸ“– ä»£ç è¯´æ˜

1. **æ¨¡å‹å®šä¹‰**ï¼š
   - `SimpleNet` æ˜¯ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œè¾“å…¥ä¸ºMNISTçš„28x28åƒç´ å›¾åƒï¼Œè¾“å‡ºä¸º10ç±»åˆ†ç±»ã€‚

2. **æ•°æ®é›†**ï¼š
   - ä½¿ç”¨`torchvision`åŠ è½½MNISTæ•°æ®é›†ï¼Œæ‰¹é‡å¤§å°ä¸º64ï¼Œæ•°æ®é¢„å¤„ç†ä»…åŒ…æ‹¬è½¬æ¢ä¸ºå¼ é‡ã€‚

3. **Adamä¼˜åŒ–å™¨**ï¼š
   - åˆå§‹åŒ–ä¸º`optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)`ã€‚
   - `lr=0.001`ï¼šåˆå§‹å­¦ä¹ ç‡ï¼ŒAdamçš„é»˜è®¤å€¼é€šå¸¸æ•ˆæœè‰¯å¥½ã€‚
   - `betas=(0.9, 0.999)`ï¼šä¸€é˜¶å’ŒäºŒé˜¶çŸ©çš„è¡°å‡ç‡ï¼Œæ ‡å‡†å€¼ã€‚
   - `eps=1e-8`ï¼šé˜²æ­¢é™¤é›¶çš„å°å€¼ã€‚

4. **è®­ç»ƒä¸æµ‹è¯•**ï¼š
   - è®­ç»ƒæ—¶ï¼ŒAdamä¼˜åŒ–å™¨æ ¹æ®æ¢¯åº¦è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°ã€‚
   - æ¯ä¸ªepochæ‰“å°å¹³å‡æŸå¤±ï¼Œæµ‹è¯•æ—¶è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ã€‚

5. **è¾“å‡ºç¤ºä¾‹**ï¼š
   ```
   Epoch 1, Average Loss: 0.3256
   Test Accuracy: 94.50%
   Epoch 2, Average Loss: 0.1423
   Test Accuracy: 96.20%
   ...
   Epoch 5, Average Loss: 0.0854
   Test Accuracy: 97.80%
   ```
   å®é™…å€¼å› éšæœºåˆå§‹åŒ–è€Œå¼‚ã€‚

---

### ğŸ“– Adamä¸SGDçš„å¯¹æ¯”
ä¸ºäº†å±•ç¤ºAdamçš„è‡ªé€‚åº”æ€§ï¼Œå¯ä»¥å¯¹æ¯”SGDçš„å®ç°ï¼ˆä¸å«åŠ¨é‡ï¼‰ï¼š

```python
# ä½¿ç”¨SGDä¼˜åŒ–å™¨ï¼ˆæ— åŠ¨é‡ï¼‰
optimizer = optim.SGD(model.parameters(), lr=0.01)  # æ›¿æ¢Adam
```

- **Adam**ï¼šè‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼Œé€šå¸¸æ”¶æ•›æ›´å¿«ï¼Œå¯¹åˆå§‹å­¦ä¹ ç‡ä¸æ•æ„Ÿã€‚
- **SGD**ï¼šå›ºå®šå­¦ä¹ ç‡ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´æˆ–ç»“åˆå­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¦‚StepLRï¼‰ã€‚

---

### ğŸ“– å®é™…åº”ç”¨åœºæ™¯
- **æ·±åº¦å­¦ä¹ **ï¼šAdamæ˜¯CNNã€RNNã€Transformerç­‰æ¨¡å‹çš„é»˜è®¤ä¼˜åŒ–å™¨ï¼Œå› å…¶æ”¶æ•›å¿«ã€ç¨³å®šæ€§å¥½ã€‚
- **ç¨€ç–æ•°æ®**ï¼šAdamå¯¹ç¨€ç–æ¢¯åº¦ï¼ˆå¦‚NLPä»»åŠ¡ï¼‰è¡¨ç°ä¼˜å¼‚ã€‚
- **è¶…å‚æ•°è°ƒä¼˜**ï¼šè™½ç„¶Adamå¯¹å­¦ä¹ ç‡ä¸æ•æ„Ÿï¼Œä½†ä»å¯é€šè¿‡è´å¶æ–¯ä¼˜åŒ–ï¼ˆå¦‚å‰è¿°é—®é¢˜ï¼‰è°ƒæ•´`lr`ã€`betas`ç­‰ã€‚

### ğŸ“– æ³¨æ„äº‹é¡¹
- **å­¦ä¹ ç‡**ï¼šAdamé»˜è®¤`lr=0.001`é€šå¸¸æœ‰æ•ˆï¼Œä½†å¯¹äºç‰¹å®šä»»åŠ¡å¯èƒ½éœ€è¦å¾®è°ƒï¼ˆå¦‚1e-4åˆ°1e-2ï¼‰ã€‚
- **æ”¶æ•›æ€§**ï¼šåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼ŒAdamå¯èƒ½æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ï¼Œå¯å°è¯•SGD+åŠ¨é‡æˆ–AdamWï¼ˆæ”¹è¿›ç‰ˆAdamï¼ŒåŠ å…¥L2æ­£åˆ™åŒ–ï¼‰ã€‚
- **å†…å­˜å¼€é”€**ï¼šAdaméœ€å­˜å‚¨ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ï¼Œå†…å­˜éœ€æ±‚ç•¥é«˜äºSGDã€‚

---
