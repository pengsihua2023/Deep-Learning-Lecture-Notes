## 优化器概述
在深度学习中，除了 **Adam 优化器**，还有许多其他优化器被广泛使用。每种优化器都有其独特的特性和适用场景，适用于不同类型的问题和模型。以下是深度学习中常见优化器的列表及其简要说明：

### 1. **SGD（随机梯度下降，Stochastic Gradient Descent）**
   - **描述**：最基础的优化器，根据梯度的反方向更新参数。可以选择是否加入动量（momentum）以加速收敛。
   - **公式**：
<img width="770" height="210" alt="image" src="https://github.com/user-attachments/assets/a0c321d9-c0e5-42ab-9dcf-a26601e3ec9d" />


* **公式**:

$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta)
$$

若加入动量:

$$
v_t = \beta v_{t-1} + (1 - \beta)\nabla_\theta J(\theta), \quad \theta_{t+1} = \theta_t - \eta v_t
$$

   - **特点**：
     - 简单直接，适合简单的凸优化问题。
     - 收敛较慢，容易陷入局部最优或鞍点。
     - 需要手动调整学习率，可能需要学习率调度器。
   - **适用场景**：数据规模较大时，SGD+动量通常表现不错，尤其在图像分类任务中。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
     ```

### 2. **RMSProp（Root Mean Square Propagation）**
   - **描述**：通过梯度平方的指数移动平均调整学习率，适合非平稳目标函数。
   - **公式**：
<img width="828" height="358" alt="image" src="https://github.com/user-attachments/assets/2023853f-a26a-495d-b1c1-44ae388e6104" />

   - **PyTorch 示例**：
    
     optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)
  

### 3. **Adagrad（Adaptive Gradient Algorithm）**
   - **描述**：根据历史梯度平方和自适应调整学习率，适合稀疏数据。
   - **公式**：
<img width="373" height="112" alt="image" src="https://github.com/user-attachments/assets/97d9c695-56de-4461-a09a-ee62d9ab310f" />

   - **特点**：
     - 对频繁更新的参数学习率逐渐减小，适合稀疏特征。
     - 学习率单调递减，可能过早停止学习。
   - **适用场景**：稀疏数据（如自然语言处理中的词嵌入）。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)
     ```

### 4. **Adadelta**
   - **描述**：Adagrad 的改进版，使用滑动窗口计算梯度平方均值，避免学习率无限减小。
   - **公式**：
<img width="676" height="103" alt="image" src="https://github.com/user-attachments/assets/f4e0a1d5-2a90-4a6f-811d-d37b1c7b6cc9" />

   - **特点**：
     - 无需手动设置学习率。
     - 更适合长时间训练，缓解 Adagrad 学习率过快衰减的问题。
   - **适用场景**：需要鲁棒性和稳定性时。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9)
     ```

### 5. **AdamW（Adam with Weight Decay）**
   - **描述**：Adam 的变种，加入了权重衰减（L2 正则化的改进形式），更适合正则化模型。
   - **特点**：
     - 在 Adam 的基础上，权重衰减直接作用于参数，而不是梯度。
     - 更适合需要正则化的复杂模型。
   - **适用场景**：广泛用于深度学习任务，尤其是需要正则化的场景。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
     ```

### 6. **AMSGrad**
   - **描述**：Adam 的变种，修复了 Adam 在某些情况下收敛性不佳的问题，通过保留最大二阶动量。
   - **特点**：
     - 改进 Adam 的二阶动量计算，避免学习率过快衰减。
     - 在某些非凸问题中表现更稳定。
   - **适用场景**：当 Adam 收敛不佳时可尝试。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.Adam(model.parameters(), lr=0.001, amsgrad=True)
     ```

### 7. **Nadam**
   - **描述**：结合 Adam 和 Nesterov 动量法，提前“预见”梯度更新方向。
   - **特点**：
     - 比 Adam 更精确地利用动量，适合复杂优化问题。
     - 计算复杂度略高。
   - **适用场景**：需要更快收敛的场景。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.NAdam(model.parameters(), lr=0.002)
     ```

### 8. **L-BFGS（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）**
   - **描述**：二阶优化算法，基于拟牛顿法，适合小批量数据。
   - **特点**：
     - 使用有限内存近似 Hessian 矩阵，适合小数据集。
     - 计算开销较高，批量处理复杂。
   - **适用场景**：小规模数据集或需要高精度优化的任务。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.LBFGS(model.parameters(), lr=0.01)
     ```

### 9. **Rprop（Resilient Backpropagation）**
   - **描述**：仅使用梯度符号（而非幅度）更新参数，适合噪声较大的数据。
   - **特点**：
     - 对梯度大小不敏感，鲁棒性强。
     - 不适合深度网络，计算效率较低。
   - **适用场景**：简单神经网络或噪声较大的优化问题。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.Rprop(model.parameters(), lr=0.01)
     ```

### 10. **SparseAdam**
   - **描述**：Adam 的变种，专为稀疏张量优化设计。
   - **特点**：
     - 仅更新非零参数的动量，节省内存。
     - 适合稀疏数据（如嵌入层）。
   - **适用场景**：自然语言处理中的稀疏特征优化。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.001)
     ```

### 11. **ASGD（Averaged Stochastic Gradient Descent）**
   - **描述**：SGD 的变种，通过平均历史参数提高稳定性。
   - **特点**：
     - 适合大规模分布式训练。
     - 收敛较慢，但更稳定。
   - **适用场景**：分布式训练或需要稳定收敛的场景。
   - **PyTorch 示例**：
     ```python
     optimizer = torch.optim.ASGD(model.parameters(), lr=0.01)
     ```

### 选择优化器的建议
- **默认选择**：Adam 或 AdamW 是目前最常用的优化器，因其自适应性和鲁棒性适合大多数任务。
- **需要正则化**：优先选择 AdamW。
- **稀疏数据**：考虑 Adagrad 或 SparseAdam。
- **简单任务或大批量数据**：SGD+动量可能更有效。
- **复杂非凸问题**：尝试 Nadam 或 AMSGrad。
- **小数据集**：L-BFGS 可能更适合。

### 优化器的选择
Adam 是二分类任务的常见选择，因为它对初始学习率不敏感，收敛速度快。如果在二分类任务中遇到收敛问题，可以尝试：
- **AdamW**：加入权重衰减，增强正则化。
- **SGD+动量**：如果模型较简单或数据规模较大。
- **RMSProp**：如果数据噪声较大或模型是序列相关（如 RNN）。

### 总结
深度学习优化器种类繁多，主要包括 SGD、Adam、RMSProp、Adagrad、Adadelta、AdamW、Nadam 等。每种优化器在学习率调整、收敛速度和适用场景上有所不同。选择优化器时需根据任务类型、数据规模和模型复杂度权衡。
