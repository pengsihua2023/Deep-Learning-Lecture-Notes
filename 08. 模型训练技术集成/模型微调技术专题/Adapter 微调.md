# Adapter å¾®è°ƒ

## ğŸ“– 1. å®šä¹‰

**Adapter å¾®è°ƒ** æ˜¯ä¸€ç§é«˜æ•ˆå‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œå¸¸ç”¨äºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ (å¦‚ BERTã€GPT ç­‰)ã€‚
æ ¸å¿ƒæ€æƒ³ï¼š

* **å†»ç»“åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°**ï¼Œé¿å…å¤§è§„æ¨¡æ›´æ–°ï¼›
* åœ¨æ¯ä¸€å±‚ Transformer **æ’å…¥ä¸€ä¸ªå°å‹çš„ç“¶é¢ˆç»“æ„ï¼ˆAdapter æ¨¡å—ï¼‰**ï¼Œä»…è®­ç»ƒè¿™äº› Adapter å‚æ•°ï¼›
* Adapter ä¸€èˆ¬æ˜¯ï¼š**é™ç»´ â†’ æ¿€æ´» â†’ å‡ç»´**ï¼Œå³ï¼š

  * æŠŠè¾“å…¥ç»´åº¦ $d$ é™åˆ°ä¸€ä¸ªå°çš„ç“¶é¢ˆç»´åº¦ $r$ï¼›
  * ç»è¿‡éçº¿æ€§æ¿€æ´»ï¼ˆå¦‚ ReLU / GELUï¼‰ï¼›
  * å†å‡å›åŸç»´åº¦ $d$ï¼Œå¹¶ä¸æ®‹å·®è¿æ¥ã€‚

è¿™æ ·åšèƒ½å¤§å¹…å‡å°‘éœ€è¦è®­ç»ƒçš„å‚æ•°é‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚


## ğŸ“– 2. æ•°å­¦å…¬å¼

è®¾ï¼š

* Transformer éšçŠ¶æ€å‘é‡ï¼š $h \in \mathbb{R}^d$
* Adapter é™ç»´çŸ©é˜µï¼š $W_{down} \in \mathbb{R}^{r \times d}$
* Adapter å‡ç»´çŸ©é˜µï¼š $W_{up} \in \mathbb{R}^{d \times r}$
* æ¿€æ´»å‡½æ•°ï¼š $\sigma(\cdot)$

**Adapter å‰å‘è®¡ç®—**ï¼š

$$
h' = h + W_{up} \, \sigma(W_{down} h)
$$

å…¶ä¸­ï¼š

* $W_{down}$ï¼šå°† $d$-ç»´å‘é‡å‹ç¼©åˆ°ä½ç»´ $r$ï¼›
* $\sigma$ï¼šéçº¿æ€§æ˜ å°„ï¼ˆå¦‚ ReLUï¼‰ï¼›
* $W_{up}$ï¼šå†æ˜ å°„å› $d$-ç»´ï¼›
* æœ€ç»ˆç”¨æ®‹å·®è¿æ¥åŠ å› $h$ã€‚

è®­ç»ƒæ—¶ **åªæ›´æ–° $W_{down}, W_{up}$**ï¼Œå…¶ä½™æ¨¡å‹å‚æ•°ä¿æŒå†»ç»“ã€‚


## ğŸ“– 3. æœ€ç®€ä»£ç ä¾‹å­

ç”¨ **PyTorch** å†™ä¸€ä¸ªæœ€å°åŒ–çš„ Adapter å±‚å¹¶æ’å…¥åˆ°æ¨¡å‹é‡Œï¼š

```python
import torch
import torch.nn as nn

# ===== Adapter æ¨¡å— =====
class Adapter(nn.Module):
    def __init__(self, d_model, r=16):
        super().__init__()
        self.down = nn.Linear(d_model, r, bias=False)
        self.act = nn.ReLU()
        self.up = nn.Linear(r, d_model, bias=False)

    def forward(self, x):
        return x + self.up(self.act(self.down(x)))  # æ®‹å·®è¿æ¥

# ===== åœ¨ Transformer å±‚é‡Œç”¨ Adapter =====
class ToyTransformerLayer(nn.Module):
    def __init__(self, d_model=128, adapter_r=16):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, num_heads=4)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU()
        )
        self.adapter = Adapter(d_model, r=adapter_r)

    def forward(self, x):
        attn_out, _ = self.self_attn(x, x, x)
        x = x + attn_out
        x = self.ffn(x)
        x = self.adapter(x)  # æ’å…¥ Adapter
        return x

# ===== ç®€å•æµ‹è¯• =====
x = torch.randn(10, 32, 128)  # [seq_len, batch_size, hidden_dim]
layer = ToyTransformerLayer()
out = layer(x)
print("è¾“å‡ºç»´åº¦:", out.shape)
```

## ğŸ“– è§£é‡Š

1. **Adapter**ï¼šä¸¤å±‚å…¨è¿æ¥ï¼Œå½¢æˆé™ç»´â€“å‡ç»´ç“¶é¢ˆã€‚
2. **æ®‹å·®è¿æ¥**ï¼šä¿è¯ä¸ç ´ååŸæœ‰æ¨¡å‹ç»“æ„ã€‚
3. **è®­ç»ƒ**ï¼šå®é™…åº”ç”¨ä¸­å†»ç»“æ‰€æœ‰é¢„è®­ç»ƒå‚æ•°ï¼Œåªè®­ç»ƒ Adapter å±‚çš„å‚æ•°ã€‚
