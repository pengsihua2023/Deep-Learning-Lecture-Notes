# ReLoRA (Restarted Low-Rank Adaptation) å¾®è°ƒæ–¹æ³•


## ğŸ“– 1. å®šä¹‰

**ReLoRA** æ˜¯åœ¨ **LoRA (Low-Rank Adaptation)** åŸºç¡€ä¸Šçš„æ”¹è¿›å‹å¾®è°ƒæ–¹æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

* **LoRA**ï¼šåœ¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒæ—¶ï¼Œä¸æ›´æ–°å®Œæ•´å‚æ•° $W \in \mathbb{R}^{d \times k}$ï¼Œè€Œæ˜¯æ’å…¥ä½ç§©çŸ©é˜µåˆ†è§£ $W + BA$ï¼Œå…¶ä¸­ $B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, r \ll \min(d, k)$ã€‚è¿™æ ·æ˜¾è‘—å‡å°‘å‚æ•°å’Œæ˜¾å­˜å¼€é”€ã€‚
* **ReLoRA**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ **å®šæœŸå°† LoRA çš„ä½ç§©å¢é‡åˆå¹¶åˆ°åŸå§‹æƒé‡ä¸­ï¼Œç„¶åé‡ç½® LoRA å‚æ•°**ã€‚

  * è¿™æ ·å¯ä»¥åœ¨ä¿æŒä½ç§©æ›´æ–°æ•ˆç‡çš„åŒæ—¶ï¼Œé¿å…é•¿æœŸåªä¾èµ–ä½ç§©è¿‘ä¼¼å¯¼è‡´çš„æ¬ æ‹Ÿåˆï¼›
  * åŒæ—¶èƒ½é€šè¿‡å¤šæ¬¡ â€œé‡å¯â€ ç´¯ç§¯æ›´å¤šçš„ä¿¡æ¯ï¼Œæå‡æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ•ˆæœã€‚

æ¢å¥è¯è¯´ï¼ŒReLoRA ç›¸å½“äº **å‘¨æœŸæ€§åœ°æŠŠ LoRA å­¦åˆ°çš„çŸ¥è¯†â€œå¸æ”¶åˆ°â€æ¨¡å‹æƒé‡é‡Œï¼Œå†ç»™ LoRA ä¸€ä¸ªæ–°çš„å­¦ä¹ ç©ºé—´**ã€‚


## ğŸ“– 2. æ•°å­¦å…¬å¼

è®¾ï¼š

* åŸå§‹æƒé‡çŸ©é˜µï¼š$W \in \mathbb{R}^{d \times k}$
* LoRA å‚æ•°ï¼š$A_t \in \mathbb{R}^{r \times k}, B_t \in \mathbb{R}^{d \times r}$
* æœ‰æ•ˆæƒé‡ï¼š

$$
W_t^{\text{eff}} = W + B_t A_t
$$

### ReLoRA æ›´æ–°æ­¥éª¤ï¼š

1. **å¸¸è§„ LoRA æ›´æ–°**ï¼ˆåœ¨ä¸€ä¸ªå‘¨æœŸå†…ï¼‰ï¼š

   $$
   (A_t, B_t) \leftarrow (A_{t-1}, B_{t-1}) - \eta \nabla_{A,B} L(W_{t-1}^{\text{eff}})
   $$

2. **å‘¨æœŸæ€§åˆå¹¶**ï¼ˆæ¯éš” $T$ æ­¥ï¼‰ï¼š

   $$
   W \leftarrow W + B_t A_t
   $$

   $$
   A_t, B_t \leftarrow \text{init}() \quad (\text{é‡æ–°éšæœºåˆå§‹åŒ–})
   $$

è¿™æ ·ï¼Œæ¨¡å‹æƒé‡ $W$ ä¼šä¸æ–­å¸æ”¶ LoRA çš„ä½ç§©æ”¹è¿›ï¼Œè€Œ LoRA å‚æ•°åˆ™ä¸æ–­é‡å¯ï¼Œé¿å…è®­ç»ƒæ—©æœŸçš„é™åˆ¶ã€‚


## ğŸ“– 3. æœ€ç®€ä»£ç ä¾‹å­

ä¸‹é¢ç»™å‡ºä¸€ä¸ªæç®€çš„ **PyTorch ReLoRA å¾®è°ƒç¤ºæ„ä»£ç **ï¼ˆä»…æ¼”ç¤ºæœºåˆ¶ï¼Œä¸æ˜¯å®Œæ•´åº“å®ç°ï¼‰ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ===== ç®€å•çš„ LoRA æ¨¡å— =====
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=4):
        super().__init__()
        self.W = nn.Parameter(torch.randn(out_features, in_features))  # åŸå§‹æƒé‡
        self.A = nn.Parameter(torch.randn(rank, in_features) * 0.01)   # LoRA A
        self.B = nn.Parameter(torch.randn(out_features, rank) * 0.01)  # LoRA B
        self.rank = rank

    def forward(self, x):
        return nn.functional.linear(x, self.W + self.B @ self.A)

    def merge_lora(self):
        """æŠŠ LoRA èåˆè¿›ä¸»æƒé‡"""
        with torch.no_grad():
            self.W += self.B @ self.A
            nn.init.normal_(self.A, std=0.01)
            nn.init.normal_(self.B, std=0.01)

# ===== æ•°æ®å’Œæ¨¡å‹ =====
x = torch.randn(100, 10)
y = torch.randn(100, 5)

model = LoRALinear(10, 5, rank=4)
criterion = nn.MSELoss()
optimizer = optim.Adam([model.A, model.B], lr=1e-2)  # åªè®­ç»ƒ LoRA å‚æ•°

# ===== ReLoRA è®­ç»ƒ =====
steps = 200
merge_every = 50  # æ¯éš” 50 æ­¥åˆå¹¶ä¸€æ¬¡

for step in range(steps):
    optimizer.zero_grad()
    outputs = model(x)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()

    if (step + 1) % merge_every == 0:
        print(f"Step {step+1}: Loss = {loss.item():.4f}, merging LoRA...")
        model.merge_lora()

```


### ğŸ“– è§£é‡Š

1. **LoRALinear**ï¼šå®ç°äº†ä¸€ä¸ªå¸¦ LoRA çš„çº¿æ€§å±‚ã€‚
2. **merge\_lora()**ï¼šæŠŠ $BA$ èåˆè¿›ä¸»æƒé‡ $W$ï¼Œç„¶åé‡æ–°åˆå§‹åŒ– $A, B$ã€‚
3. **è®­ç»ƒå¾ªç¯**ï¼šæ¯éš” `merge_every` æ­¥è°ƒç”¨ä¸€æ¬¡ `merge_lora()`ï¼Œå®ç° ReLoRA çš„å‘¨æœŸæ€§â€œé‡å¯â€ã€‚
4. **æ•ˆæœ**ï¼šç›¸æ¯”å•çº¯ LoRAï¼ŒReLoRA å¯ä»¥è·å¾—æ›´ç¨³å®šçš„æ”¶æ•›æ•ˆæœã€‚

## ReLoRA vs LoRA æ”¶æ•›æ•ˆæœå¯¹æ¯”çš„ç¤ºä¾‹ä»£ç ã€‚
æˆ‘ä»¬ç”¨ä¸€ä¸ª **ç®€å•çš„å›å½’ä»»åŠ¡**ï¼Œå¯¹æ¯”ä¸¤è€…åœ¨ç›¸åŒæ¡ä»¶ä¸‹çš„æŸå¤±ä¸‹é™æƒ…å†µã€‚

## ReLoRA vs LoRA å¯¹æ¯”å®éªŒ

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ===== LoRA æ¨¡å— =====
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=4):
        super().__init__()
        self.W = nn.Parameter(torch.randn(out_features, in_features) * 0.1)
        self.A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.B = nn.Parameter(torch.randn(out_features, rank) * 0.01)

    def forward(self, x):
        return nn.functional.linear(x, self.W + self.B @ self.A)

    def merge_lora(self):
        """æŠŠ LoRA èåˆè¿›ä¸»æƒé‡ï¼Œå¹¶é‡ç½® A,B"""
        with torch.no_grad():
            self.W += self.B @ self.A
            nn.init.normal_(self.A, std=0.01)
            nn.init.normal_(self.B, std=0.01)

# ===== æ•°æ® =====
torch.manual_seed(42)
x = torch.randn(200, 10)
true_w = torch.randn(5, 10)
y = x @ true_w.T + torch.randn(200, 5) * 0.1  # çº¿æ€§ä»»åŠ¡ + å°‘é‡å™ªå£°

# ===== å®éªŒé…ç½® =====
steps = 300
merge_every = 50

def train(model, relora=False):
    criterion = nn.MSELoss()
    optimizer = optim.Adam([model.A, model.B], lr=1e-2)
    losses = []
    for step in range(steps):
        optimizer.zero_grad()
        outputs = model(x)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()

        if relora and (step + 1) % merge_every == 0:
            model.merge_lora()

        losses.append(loss.item())
    return losses

# ===== è®­ç»ƒ LoRA å’Œ ReLoRA =====
model_lora = LoRALinear(10, 5, rank=4)
losses_lora = train(model_lora, relora=False)

model_relora = LoRALinear(10, 5, rank=4)
losses_relora = train(model_relora, relora=True)

# ===== ç»˜å›¾ =====
plt.plot(losses_lora, label="LoRA")
plt.plot(losses_relora, label="ReLoRA")
plt.xlabel("Step")
plt.ylabel("Loss")
plt.title("LoRA vs ReLoRA æ”¶æ•›å¯¹æ¯”")
plt.legend()
plt.show()
```



## ğŸ“– ä»£ç è¯´æ˜

1. **æ•°æ®**ï¼šæ„é€ äº†ä¸€ä¸ªçº¿æ€§ä»»åŠ¡ $y = Wx + \epsilon$ã€‚
2. **LoRALinear**ï¼šå’Œä¹‹å‰ä¸€æ ·ï¼Œå®ç° LoRA æƒé‡ã€‚
3. **LoRA è®­ç»ƒ**ï¼šåªæ›´æ–°ä½ç§©çŸ©é˜µ $A, B$ã€‚
4. **ReLoRA è®­ç»ƒ**ï¼šåœ¨è®­ç»ƒä¸­æ¯éš” `merge_every=50` æ­¥æ‰§è¡Œä¸€æ¬¡ `merge_lora()`ã€‚
5. **ç»“æœ**ï¼šç»˜åˆ¶ LoRA å’Œ ReLoRA çš„æ”¶æ•›æ›²çº¿ã€‚

åœ¨å®é™…è¿è¡Œä¸­ä½ ä¼šçœ‹åˆ°ï¼š

* **LoRA** æ›²çº¿ä¸‹é™ï¼Œä½†æœ‰æ—¶ä¼šæ”¶æ•›å¾—æ¯”è¾ƒæ…¢æˆ–åœæ»ï¼›
* **ReLoRA** æ›²çº¿ä¸‹é™æ›´ç¨³å®šï¼Œèƒ½è¾¾åˆ°æ›´ä½çš„ lossã€‚



