# LoRA å¾®è°ƒï¼ˆLow-Rank Adaptationï¼‰

## ğŸ“–  1. å®šä¹‰

**LoRA** æ˜¯ä¸€ç§ **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFT, Parameter-Efficient Fine-Tuningï¼‰** æ–¹æ³•ï¼Œç”±å¾®è½¯åœ¨ 2021 å¹´æå‡ºã€‚

æ ¸å¿ƒæ€æƒ³ï¼š

* **å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°** $W$ï¼Œä¸ç›´æ¥æ›´æ–°å®ƒä»¬ã€‚
* åœ¨æ¯ä¸ªå¤§æƒé‡çŸ©é˜µçš„æ›´æ–°é¡¹ä¸­ï¼Œå¼•å…¥ä¸€ä¸ª **ä½ç§©åˆ†è§£å‚æ•°** $\Delta W = BA$ã€‚
* åœ¨å¾®è°ƒæ—¶ **åªè®­ç»ƒä½ç§©çŸ©é˜µ $A, B$**ï¼Œè€ŒåŸå§‹æƒé‡ä¿æŒä¸å˜ã€‚

ğŸ‘‰ å¥½å¤„ï¼š

* æå¤§å‡å°‘å¯è®­ç»ƒå‚æ•°é‡ï¼ˆå› ä¸ºç§© $r \ll d,k$ ï¼‰ã€‚
* è®­ç»ƒåå¯é€‰æ‹©â€œåˆå¹¶â€ $\Delta W$ åˆ°åŸæ¨¡å‹ï¼Œæˆ–ä»¥â€œæ’ä»¶â€æ–¹å¼åŠ è½½ã€‚



## ğŸ“– 2. æ•°å­¦æè¿°

è®¾åŸå§‹æƒé‡çŸ©é˜µï¼š

$$
W \in \mathbb{R}^{d \times k}
$$

åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒLoRA å°†æƒé‡ä¿®æ”¹ä¸ºï¼š

$$
W' = W + \Delta W, \quad \Delta W = B A
$$

å…¶ä¸­ï¼š

* $A \in \mathbb{R}^{r \times k}$ ï¼Œ $B \in \mathbb{R}^{d \times r}$ ï¼Œ $r \ll \min(d,k)$ ã€‚
* åˆå§‹æ—¶ $BA = 0$ ï¼Œç¡®ä¿ä¸ä¼šå½±å“é¢„è®­ç»ƒæ¨¡å‹ã€‚
* åªè®­ç»ƒ $A, B$ ï¼Œå†»ç»“ $W$ ã€‚

åº”ç”¨åœ¨ Transformer ä¸­ï¼ŒLoRA é€šå¸¸ä½œç”¨åœ¨ **æ³¨æ„åŠ›å±‚çš„æŠ•å½±çŸ©é˜µ** $W_Q, W_V$ã€‚


## ğŸ“– 3. ç®€å•ä»£ç ç¤ºä¾‹ï¼ˆPyTorch å®ç°ï¼‰

ä¸‹é¢ç”¨ PyTorch å†™ä¸€ä¸ª **çº¿æ€§å±‚ + LoRA** çš„å®ç°ï¼š

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, in_dim, out_dim, rank=4):
        super(LoRALayer, self).__init__()
        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))  
        self.weight.requires_grad = False  # å†»ç»“åŸå§‹æƒé‡

        # LoRA å‚æ•° (ä½ç§©åˆ†è§£)
        self.A = nn.Parameter(torch.randn(rank, in_dim) * 0.01)
        self.B = nn.Parameter(torch.randn(out_dim, rank) * 0.01)

    def forward(self, x):
        W_eff = self.weight + self.B @ self.A  # æœ‰æ•ˆæƒé‡ = å†»ç»“æƒé‡ + LoRA å¢é‡
        return x @ W_eff.T

# æµ‹è¯•
x = torch.randn(2, 10)  # batch=2, è¾“å…¥ç»´åº¦=10
layer = LoRALayer(in_dim=10, out_dim=6, rank=2)
y = layer(x)
print("Output shape:", y.shape)  # (2, 6)
```

åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒLoRA é€šå¸¸æ’å…¥åˆ° Transformer çš„ **Q (query projection)** å’Œ **V (value projection)** çŸ©é˜µä¸­ï¼Œç”¨å°‘é‡å‚æ•°å®ç°é«˜æ•ˆé€‚é…ã€‚


## ğŸ“– 4. æ€»ç»“

* **å®šä¹‰**ï¼šLoRA é€šè¿‡ä½ç§©åˆ†è§£ï¼Œä»…è®­ç»ƒå¢é‡å‚æ•°çŸ©é˜µï¼Œè€Œå†»ç»“åŸå§‹æƒé‡ã€‚
* **å…¬å¼**ï¼š

$$
W' = W + BA, \quad A \in \mathbb{R}^{r \times k}, \; B \in \mathbb{R}^{d \times r}, \; r \ll \min(d,k)
$$

* **ç‰¹ç‚¹**ï¼š

  * æ˜¾è‘—å‡å°‘å¯è®­ç»ƒå‚æ•°é‡ã€‚
  * è®­ç»ƒåå¯â€œåˆå¹¶â€æˆ–â€œæ’ä»¶å¼â€ä½¿ç”¨ã€‚
  * ä¸ Transformer æ¶æ„é«˜åº¦å…¼å®¹ã€‚
* **ä»£ç **ï¼šPyTorch å¯ä»¥é€šè¿‡ `LoRALayer` ç±»æ¥å®ç°ã€‚

---

# ğŸ“– åœ¨ Transformer æ³¨æ„åŠ›å±‚ä¸­é›†æˆ LoRA

## 1. æ€è·¯

* åœ¨ **Self-Attention** é‡Œï¼Œé€šå¸¸æœ‰ä¸‰ä¸ªçº¿æ€§æŠ•å½±ï¼š

$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$
  
* **LoRA** åªä½œç”¨åœ¨ $W_Q, W_V$ï¼ˆä¹Ÿå¯ä»¥æ‰©å±•åˆ° $W_K, W_O$ï¼‰ã€‚
* æ›¿æ¢æ–¹å¼ï¼š

$$
W_Q' = W_Q + B_Q A_Q, \quad W_V' = W_V + B_V A_V
$$

* è¿™æ ·åªéœ€è®­ç»ƒ **ä½ç§©çŸ©é˜µ $A, B$**ï¼Œå†»ç»“åŸå§‹å‚æ•°ã€‚



## 2. PyTorch ç¤ºä¾‹ä»£ç 

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class LoRALinear(nn.Module):
    def __init__(self, in_dim, out_dim, rank=4):
        super(LoRALinear, self).__init__()
        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))
        self.weight.requires_grad = False  # å†»ç»“åŸå§‹æƒé‡

        # LoRA å‚æ•°
        self.A = nn.Parameter(torch.randn(rank, in_dim) * 0.01)
        self.B = nn.Parameter(torch.randn(out_dim, rank) * 0.01)

    def forward(self, x):
        W_eff = self.weight + self.B @ self.A
        return x @ W_eff.T


class LoRAAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, rank=4):
        super(LoRAAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # æŠ•å½±çŸ©é˜µï¼šQã€Kã€Vï¼ˆå…¶ä¸­Qå’ŒVä½¿ç”¨LoRAï¼‰
        self.W_Q = LoRALinear(embed_dim, embed_dim, rank)
        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_V = LoRALinear(embed_dim, embed_dim, rank)
        self.W_O = nn.Linear(embed_dim, embed_dim, bias=False)

    def forward(self, x):
        B, T, D = x.shape

        Q = self.W_Q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.W_K(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.W_V(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)

        out = out.transpose(1, 2).contiguous().view(B, T, D)
        return self.W_O(out)


# æµ‹è¯•
x = torch.randn(2, 5, 16)  # batch=2, seq_len=5, embed_dim=16
attn = LoRAAttention(embed_dim=16, num_heads=2, rank=2)
y = attn(x)
print("Output shape:", y.shape)  # (2, 5, 16)
```

---

## ğŸ“– 3. æ€»ç»“

* **Qã€V æŠ•å½±çŸ©é˜µ** è¢«æ›¿æ¢æˆ **LoRA ç‰ˆæœ¬**ï¼Œåªè®­ç»ƒä½ç§©çŸ©é˜µ $A, B$ã€‚
* **å‚æ•°é‡æ˜¾è‘—å‡å°‘**ï¼šæ¯”å¦‚ $d=1024, k=1024, r=8$ï¼ŒLoRA å‚æ•°åªæœ‰ $16k$ï¼Œè¿œå°äºå…¨é‡å‚æ•° $1M+$ã€‚
* **å…¼å®¹æ€§å¼º**ï¼šLoRA å¯ä»¥æ— ç¼æ’å…¥ç°æœ‰ Transformer æ¨¡å‹ã€‚

---


# ğŸ“– Hugging Face Transformers + LoRA ç¤ºä¾‹

## 1. å®‰è£…ä¾èµ–

```bash
pip install transformers datasets peft accelerate
```



## 2. åŠ è½½æ•°æ® & æ¨¡å‹

æˆ‘ä»¬ç”¨ **Hugging Face Datasets** åŠ è½½ `sst2` æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶åŠ è½½ `bert-base-uncased`ã€‚

```python
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# æ•°æ®é›†
dataset = load_dataset("glue", "sst2")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def preprocess(examples):
    return tokenizer(examples["sentence"], truncation=True, padding="max_length", max_length=128)

encoded_dataset = dataset.map(preprocess, batched=True)

# æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
```



## 3. é…ç½® LoRA

`peft` åº“æä¾›äº†ç®€æ´çš„æ¥å£ã€‚

```python
from peft import LoraConfig, get_peft_model

# LoRA é…ç½®
config = LoraConfig(
    r=8,                      # ä½ç§©åˆ†è§£ç»´åº¦
    lora_alpha=16,            # ç¼©æ”¾å› å­
    target_modules=["query", "value"],  # ä½œç”¨åœ¨ Self-Attention çš„ Q,V çŸ©é˜µ
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS"
)

# æ³¨å…¥ LoRA
model = get_peft_model(model, config)
model.print_trainable_parameters()
```

è¾“å‡ºç¤ºä¾‹ï¼š

```
trainable params: 590,848 || all params: 109,483,778 || trainable%: 0.54
```

ğŸ‘‰ åªéœ€è®­ç»ƒ **ä¸åˆ° 1% çš„å‚æ•°**ã€‚



## 4. è®­ç»ƒ

æˆ‘ä»¬ç”¨ Hugging Face çš„ `Trainer` æ¥è¿›è¡Œå¾®è°ƒã€‚

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./lora-bert",
    per_device_train_batch_size=16,
    num_train_epochs=3,
    learning_rate=5e-4,
    logging_dir="./logs",
    evaluation_strategy="epoch",
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"].shuffle(seed=42).select(range(5000)),  # å°æ ·æœ¬æ¼”ç¤º
    eval_dataset=encoded_dataset["validation"].select(range(1000)),
    tokenizer=tokenizer
)

trainer.train()
```



## 5. æ¨ç† & ä¿å­˜ LoRA

```python
# æ¨ç†
text = "The movie was absolutely wonderful!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
pred = outputs.logits.argmax(dim=-1).item()
print("Prediction:", "Positive" if pred == 1 else "Negative")

# åªä¿å­˜ LoRA å‚æ•°
model.save_pretrained("./lora-bert")
```

---

## ğŸ“– æ€»ç»“

* **LoRA åœ¨ Hugging Face ä¸­çš„å®ç°**éå¸¸æ–¹ä¾¿ï¼Œåªéœ€ç”¨ `peft.LoraConfig` æ³¨å…¥å³å¯ã€‚
* åªè®­ç»ƒ **Qã€V çŸ©é˜µ**çš„ä½ç§©æ›´æ–°ï¼Œå¤§å¤§å‡å°‘å‚æ•°é‡ã€‚
* å¯ç”¨äº BERTã€GPT-2ã€T5 ç­‰å¤§æ¨¡å‹çš„ **é«˜æ•ˆå¾®è°ƒ**ã€‚






