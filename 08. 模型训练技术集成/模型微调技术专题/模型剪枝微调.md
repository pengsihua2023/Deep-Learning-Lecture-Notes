# 模型剪枝（Model Pruning）

## 📖 1. 定义（Definition）

**模型剪枝（Model Pruning）** 是一种 **参数高效微调方法**，通过 **移除神经网络中冗余或不重要的参数/连接**，以减少模型大小和计算开销，同时尽量保持性能。

* 剪枝通常在 **预训练模型** 上进行，然后再对保留的参数进行 **微调**。
* 可分为：

  * **非结构化剪枝（unstructured pruning）**：直接将某些权重置零，不改变张量形状。
  * **结构化剪枝（structured pruning）**：移除整个通道、卷积核或注意力头。
* 微调阶段：

  * 保留非零参数，继续在下游任务上训练，以恢复性能。

---

## 🔢 2. 数学描述（Mathematical Formulation）

设原始模型参数为：

$$
W = \{ w_1, w_2, \dots, w_n \}
$$

定义一个 **掩码向量** $M = \{ m_1, m_2, \dots, m_n \}$，其中：

$$
m_i \in \{0, 1\}, \quad W' = W \odot M
$$

其中：

* $m_i = 0$ 表示该参数被剪枝；
* $m_i = 1$ 表示该参数保留；
* $\odot$ 表示逐元素乘法。

训练目标函数变为：

$$
\mathcal{L}(W', M) = - \sum_{(x, y)} \log p(y \mid x; W \odot M)
$$

在微调时，固定 $M$，优化保留下来的参数 $W'$。


## 💻 3. 简单代码演示（PyTorch）

以下代码展示一个 **非结构化剪枝 + 微调** 的最小示例，基于 PyTorch：

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import torch.nn.functional as F

# 一个简单的全连接网络
class SimpleNet(nn.Module):
    def __init__(self, input_size=100, hidden_size=50, num_classes=2):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

# 初始化模型
model = SimpleNet()
print("参数数量 (剪枝前):", sum(p.numel() for p in model.parameters()))

# 对第一层权重进行非结构化剪枝 (50% 最小权重剪枝)
prune.l1_unstructured(model.fc1, name="weight", amount=0.5)

# 查看剪枝掩码
print("剪枝掩码：")
print(model.fc1.weight_mask)

# 使用剪枝后的模型进行一次前向传播
x = torch.randn(4, 100)
logits = model(x)
print("输出 logits:", logits)

# 剪枝后继续微调
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
labels = torch.tensor([0, 1, 0, 1])
loss = F.cross_entropy(logits, labels)
loss.backward()
optimizer.step()
```


## ✅ 总结

* **定义**：Model Pruning 是通过剪掉不重要的权重/通道来压缩模型，然后对剩余参数进行微调。
* **数学形式**：使用掩码 $M$ 对参数 $W$ 进行稀疏化，优化 $\mathcal{L}(W \odot M)$。
* **代码**：PyTorch 的 `torch.nn.utils.prune` 可方便地实现权重剪枝，并在剪枝后进行微调。

