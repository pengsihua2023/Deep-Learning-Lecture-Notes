# P-Tuning v2å¾®è°ƒ

## ğŸ“– 1. P-Tuning v2 çš„å®šä¹‰

**P-Tuning v2** æ˜¯æ¸…åå¤§å­¦æå‡ºçš„ **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•**ï¼Œæ˜¯å¯¹ P-Tuning çš„æ”¹è¿›ç‰ˆæœ¬ã€‚

* **æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨æ¨¡å‹ **æ¯ä¸€å±‚ Transformer çš„è¾“å…¥** éƒ½æ’å…¥ä¸€ç»„ **å¯å­¦ä¹ çš„è¿ç»­æç¤ºå‘é‡ï¼ˆprefix vectorsï¼‰**ã€‚
* **åŒºåˆ«äº Prompt Tuning**ï¼šPrompt Tuning åªåœ¨è¾“å…¥å±‚åŠ  soft promptï¼Œè€Œ P-Tuning v2 åœ¨ **å¤šå±‚å‰ç¼€æ³¨å…¥ï¼ˆprefix-tuning-likeï¼‰**ï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ›ã€‚
* **ç‰¹ç‚¹**ï¼š

  * ä¸å…¨å‚æ•°å¾®è°ƒæ•ˆæœæ¥è¿‘ã€‚
  * ä¸ä¾èµ–å¤æ‚çš„ LSTM/MHA ç»“æ„ï¼ˆæ¯” P-Tuning v1 ç®€æ´ï¼‰ã€‚
  * å¯¹å„ç§ PLMï¼ˆGPTã€BERTã€T5ï¼‰å‡é€‚ç”¨ã€‚



## ğŸ“– 2. æ•°å­¦æè¿°

è®¾ï¼š

* é¢„è®­ç»ƒ Transformer æ¨¡å‹ä¸º $f_\theta$ï¼Œå‚æ•° $\theta$ å†»ç»“ã€‚
* è¾“å…¥åºåˆ— embeddingï¼š

$$
X = (x_1, x_2, \dots, x_n), \quad x_i \in \mathbb{R}^d
$$

åœ¨æ¯ä¸€å±‚ $l \in \{1, \dots, L\}$ï¼Œå¼•å…¥ **prefix å‘é‡**ï¼š

$$
P^l = (p^l_1, p^l_2, \dots, p^l_m), \quad p^l_j \in \mathbb{R}^d
$$

åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œå°† prefix æ‹¼æ¥åˆ° query/key/value çš„è¾“å…¥ï¼š

$$
\text{SelfAttn}(X^l) = \text{Softmax}\left( \frac{[X^l W_Q; P^l_Q][X^l W_K; P^l_K]^T}{\sqrt{d_k}} \right) [X^l W_V; P^l_V]
$$

å…¶ä¸­ï¼š

* $W_Q, W_K, W_V$ æ˜¯å†»ç»“çš„æ¨¡å‹æƒé‡ã€‚
* $P^l_Q, P^l_K, P^l_V$ æ˜¯ prefix å‘é‡æŠ•å½±åçš„ç»“æœã€‚

è®­ç»ƒç›®æ ‡å‡½æ•°ï¼š

$$
\mathcal{L}(\{P^l\}) = - \sum_{(X, y)} \log p(y \mid X, \theta, \{P^l\})
$$

å³ï¼šå†»ç»“æ¨¡å‹å‚æ•° $\theta$ï¼Œåªè®­ç»ƒ prefix å‚æ•° $\{P^l\}$ã€‚



## ğŸ“– 3. ç®€å•ä»£ç æ¼”ç¤º

ä¸‹é¢æ˜¯ä¸€ä¸ªåŸºäº Hugging Face çš„ **P-Tuning v2 ç®€åŒ–å®ç°**ï¼ˆåªå±•ç¤ºæ ¸å¿ƒé€»è¾‘ï¼‰ï¼š

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

class PrefixEncoder(nn.Module):
    """ç”¨ MLP ç”Ÿæˆ prefix embeddings"""
    def __init__(self, prefix_length, hidden_size):
        super().__init__()
        self.embedding = nn.Embedding(prefix_length, hidden_size)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, hidden_size)
        )
    
    def forward(self, batch_size):
        prefix_tokens = torch.arange(self.embedding.num_embeddings).to(self.embedding.weight.device)
        prefix_embeds = self.embedding(prefix_tokens)  # [m, d]
        prefix_embeds = self.mlp(prefix_embeds)
        return prefix_embeds.unsqueeze(0).expand(batch_size, -1, -1)  # [B, m, d]

class PTuningV2Model(nn.Module):
    def __init__(self, model_name="bert-base-uncased", prefix_length=10, num_labels=2):
        super().__init__()
        self.bert = BertModel.from_pretrained(model_name)
        for param in self.bert.parameters():
            param.requires_grad = False  # å†»ç»“é¢„è®­ç»ƒå‚æ•°

        self.prefix_encoder = PrefixEncoder(prefix_length, self.bert.config.hidden_size)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.prefix_length = prefix_length

    def forward(self, input_ids, attention_mask):
        batch_size = input_ids.size(0)

        # åŸå§‹è¯åµŒå…¥
        inputs_embeds = self.bert.embeddings.word_embeddings(input_ids)

        # prefix embeddings
        prefix_embeds = self.prefix_encoder(batch_size)

        # æ‹¼æ¥ prefix + åŸå§‹è¾“å…¥
        inputs_embeds = torch.cat([prefix_embeds, inputs_embeds], dim=1)

        # æ‰©å±• attention mask
        prefix_mask = torch.ones(batch_size, self.prefix_length).to(attention_mask.device)
        attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)

        # è¾“å…¥ BERT
        outputs = self.bert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0]

        # åˆ†ç±»
        logits = self.classifier(cls_output)
        return logits

# ç¤ºä¾‹
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
inputs = tokenizer("P-Tuning v2 is powerful!", return_tensors="pt")

model = PTuningV2Model()
logits = model(**inputs)
print(logits)
```



## ğŸ“– æ€»ç»“

* **å®šä¹‰**ï¼šP-Tuning v2 åœ¨ Transformer **å„å±‚** æ³¨å…¥ prefix å‘é‡ï¼Œåªè®­ç»ƒ prefix å‚æ•°ï¼Œå†»ç»“æ¨¡å‹ã€‚
* **æ•°å­¦å½¢å¼**ï¼šåœ¨æ¯å±‚è‡ªæ³¨æ„åŠ›åŠ å…¥ prefix Q/K/Vï¼Œä¼˜åŒ– $\{P^l\}$ã€‚
* **ä»£ç å®ç°**ï¼šç”¨ä¸€ä¸ª `PrefixEncoder` ç”Ÿæˆ prefix embeddingï¼Œæ‹¼æ¥åˆ°è¾“å…¥ embedding å’Œæ³¨æ„åŠ›è®¡ç®—ä¸­ã€‚


