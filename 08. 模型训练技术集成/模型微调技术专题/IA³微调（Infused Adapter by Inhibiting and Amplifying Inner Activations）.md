
# IAÂ³ å¾®è°ƒï¼ˆInfused Adapter by Inhibiting and Amplifying Inner Activationsï¼‰

## 1. å®šä¹‰

**IAÂ³ (Infused Adapter by Inhibiting and Amplifying Inner Activations)** æ˜¯ä¸€ç§ **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFT, Parameter-Efficient Fine-Tuningï¼‰** æ–¹æ³•ï¼Œç”± Liu et al. (2022) æå‡ºã€‚

å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

* ä¸æ›´æ–°åŸå§‹é¢„è®­ç»ƒæ¨¡å‹å‚æ•° $\theta$ã€‚
* åœ¨ **Transformer çš„æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚** ä¸­å¼•å…¥ **å¯è®­ç»ƒçš„æ ‡é‡å‘é‡**ï¼Œå¯¹æ¿€æ´»å€¼è¿›è¡Œ **ç¼©æ”¾ï¼ˆinhibit/amplifyï¼‰**ã€‚
* è¿™æ ·ä»…éœ€è®­ç»ƒå°‘é‡å‚æ•°ï¼Œå´èƒ½é«˜æ•ˆé€‚é…ä¸‹æ¸¸ä»»åŠ¡ã€‚

ğŸ‘‰ ç®€å•ç†è§£ï¼šIAÂ³ ç»™æ¯ä¸€å±‚çš„æ³¨æ„åŠ›å’Œå€¼æŠ•å½±å¢åŠ  **é€é€šé“çš„ç¼©æ”¾å› å­**ï¼Œåƒæ—‹é’®ä¸€æ ·è°ƒèŠ‚ä¿¡å·å¼ºåº¦ã€‚



## 2. æ•°å­¦æè¿°

### 2.1 Transformer ä¸­çš„æ³¨æ„åŠ›è®¡ç®—

æ ‡å‡† **Scaled Dot-Product Attention**ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

å…¶ä¸­ï¼š

* $Q = XW_Q$
* $K = XW_K$
* $V = XW_V$

### 2.2 IAÂ³ ä¿®æ”¹åçš„æ³¨æ„åŠ›

IAÂ³ åœ¨æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚ä¸­æ’å…¥ **é€é€šé“ç¼©æ”¾å‘é‡** $l_k, l_v, l_{ff}$ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q (K \odot l_k)^T}{\sqrt{d_k}}\right)(V \odot l_v)
$$

$$
\text{FFN}(X) = (X W_1 \odot l_{ff}) W_2
$$

å…¶ä¸­ï¼š

* $\odot$ è¡¨ç¤ºé€å…ƒç´ ç›¸ä¹˜ï¼ˆå¹¿æ’­ï¼‰ã€‚
* $l_k, l_v, l_{ff}$ æ˜¯ **å¯è®­ç»ƒå‚æ•°å‘é‡**ï¼Œç»´åº¦åˆ†åˆ«ä¸ $K$ã€$V$ã€FFN ä¸­é—´å±‚ç›¸åŒã€‚

### 2.3 æŸå¤±å‡½æ•°

åªè®­ç»ƒè¿™äº›ç¼©æ”¾å‚æ•°ï¼š

$$
\mathcal{L} = \mathcal{L}_{task}(f(x; \theta, l_k, l_v, l_{ff}))
$$

$\theta$ å›ºå®šï¼ˆå†»ç»“ï¼‰ï¼Œåªæ›´æ–° $l_k, l_v, l_{ff}$ã€‚



## 3. ç®€å•ä»£ç ç¤ºä¾‹ï¼ˆPyTorchï¼‰

è¿™é‡Œç”¨ PyTorch å†™ä¸€ä¸ª **ç®€åŒ–ç‰ˆ IAÂ³ æ³¨æ„åŠ›å±‚**ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class IA3Attention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(IA3Attention, self).__init__()
        self.num_heads = num_heads
        self.embed_dim = embed_dim
        self.head_dim = embed_dim // num_heads

        # åŸå§‹æŠ•å½±å±‚ï¼ˆå†»ç»“ï¼‰
        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False)

        for p in self.parameters():
            p.requires_grad = False

        # IAÂ³ å¯è®­ç»ƒç¼©æ”¾å‚æ•°
        self.l_k = nn.Parameter(torch.ones(1, 1, embed_dim))
        self.l_v = nn.Parameter(torch.ones(1, 1, embed_dim))

    def forward(self, x):
        Q = self.W_Q(x)
        K = self.W_K(x) * self.l_k  # ç¼©æ”¾ Keys
        V = self.W_V(x) * self.l_v  # ç¼©æ”¾ Values

        # åˆ†å¤šå¤´
        Q = Q.view(x.size(0), -1, self.num_heads, self.head_dim).transpose(1,2)
        K = K.view(x.size(0), -1, self.num_heads, self.head_dim).transpose(1,2)
        V = V.view(x.size(0), -1, self.num_heads, self.head_dim).transpose(1,2)

        # æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)
        attn = F.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)

        out = out.transpose(1,2).reshape(x.size(0), -1, self.embed_dim)
        return out

# æµ‹è¯•
x = torch.rand(2, 5, 16)  # batch=2, seq_len=5, embed_dim=16
attn = IA3Attention(embed_dim=16, num_heads=2)
out = attn(x)
print("Output shape:", out.shape)  # (2, 5, 16)
```

åœ¨çœŸå® Transformer ä¸­ï¼Œè¿˜ä¼šåœ¨ FFNï¼ˆå‰é¦ˆå±‚ï¼‰ä¸­åŠ ä¸Šç±»ä¼¼çš„ç¼©æ”¾å‚æ•° $l_{ff}$ã€‚



## 4. æ€»ç»“

* **å®šä¹‰**ï¼šIAÂ³ æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚ä¸­å¼•å…¥ç¼©æ”¾å‘é‡è°ƒèŠ‚æ¿€æ´»å€¼ã€‚
* **å…¬å¼**ï¼š

  $$
  \text{Attn}(Q,K,V) = \text{softmax}\Big(\frac{Q (K \odot l_k)^T}{\sqrt{d_k}}\Big)(V \odot l_v)
  $$

  $$
  \text{FFN}(X) = (X W_1 \odot l_{ff}) W_2
  $$
* **ç‰¹ç‚¹**ï¼š

  * åªè®­ç»ƒå°‘é‡å‚æ•°ï¼ˆç¼©æ”¾å‘é‡ï¼‰ã€‚
  * ä¸ä¿®æ”¹é¢„è®­ç»ƒæƒé‡ï¼Œæ˜¾å­˜/å­˜å‚¨å¼€é”€å°ã€‚
  * æ•ˆæœæ¥è¿‘å…¨é‡å¾®è°ƒï¼Œå°¤å…¶é€‚åˆå¤§æ¨¡å‹é€‚é…ã€‚
* **ä»£ç **ï¼šåªéœ€åœ¨æ³¨æ„åŠ›å’Œ FFN ä¸­åŠ å…¥å¯è®­ç»ƒç¼©æ”¾å‘é‡ã€‚



