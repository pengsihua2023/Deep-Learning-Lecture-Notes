# QLoRA (Quantized Low-Rank Adaptation) å¾®è°ƒæ–¹æ³•


## ğŸ“– 1. å®šä¹‰

**QLoRA (Quantized Low-Rank Adaptation)** æ˜¯ 2023 å¹´æå‡ºçš„ä¸€ç§é«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº† **æƒé‡é‡åŒ– (Quantization)** å’Œ **LoRA (Low-Rank Adaptation)**ï¼š

* **æƒé‡é‡åŒ–**ï¼šæŠŠé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ä» 16/32-bit æµ®ç‚¹æ•°å‹ç¼©æˆ 4-bit è¡¨ç¤ºï¼ˆé€šå¸¸æ˜¯ NF4 æ ¼å¼ï¼‰ï¼Œæ˜¾è‘—å‡å°‘æ˜¾å­˜å ç”¨ã€‚
* **LoRA**ï¼šåœ¨é‡åŒ–æƒé‡çš„åŸºç¡€ä¸Šæ·»åŠ ä½ç§©çŸ©é˜µï¼ˆé€šå¸¸ rank=4\~64ï¼‰ï¼Œä»…è®­ç»ƒè¿™äº›å°çŸ©é˜µã€‚
* **ç»“æœ**ï¼š

  * å¯ä»¥åœ¨å•å¼  GPU ä¸Šå¾®è°ƒç™¾äº¿çº§åˆ«çš„æ¨¡å‹ï¼›
  * å‡ ä¹ä¸æŸå¤±æ€§èƒ½ï¼›
  * ä¿æŒé«˜æ•ˆå’Œä½æ˜¾å­˜å¼€é”€ã€‚


## ğŸ“– 2. æ•°å­¦å…¬å¼

è®¾ï¼š

* åŸå§‹æƒé‡çŸ©é˜µï¼š $W \in \mathbb{R}^{d \times k}$
* é‡åŒ–åçš„æƒé‡ï¼š $\hat{W} = Q(W)$ï¼Œå…¶ä¸­ $Q(\cdot)$ è¡¨ç¤ºé‡åŒ–å‡½æ•°ï¼ˆå¦‚ 4-bit NF4ï¼‰
* LoRA å‚æ•°ï¼š $A \in \mathbb{R}^{r \times k}, B \in \mathbb{R}^{d \times r}, r \ll \min(d,k)$

**QLoRA æœ‰æ•ˆæƒé‡è¡¨ç¤º**ï¼š

$$
W^{\text{eff}} = \hat{W} + BA
$$

å…¶ä¸­ï¼š

* $\hat{W}$ æ˜¯å†»ç»“çš„é‡åŒ–æƒé‡ï¼›
* $BA$ æ˜¯å¯è®­ç»ƒçš„ä½ç§©å¢é‡ã€‚

è®­ç»ƒæ—¶ä»…æ›´æ–° $A,B$ï¼Œä¿æŒ $\hat{W}$ ä¸å˜ï¼Œä»è€Œå‡å°‘å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚


## ğŸ“– 3. æœ€ç®€ä»£ç ä¾‹å­

ç”¨ Hugging Face **PEFT** åº“è¿›è¡Œ QLoRA å¾®è°ƒçš„æœ€å°ç¤ºä¾‹ï¼ˆå‡è®¾æ¨¡å‹æ˜¯ä¸€ä¸ªå°å‹ Transformers æ¨¡å‹ï¼‰ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

# 1. åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ 4-bit é‡åŒ–
model_name = "facebook/opt-125m"  # å°æ¨¡å‹ç¤ºä¾‹
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,                # å¯ç”¨4-bité‡åŒ–
    device_map="auto",                # è‡ªåŠ¨åˆ†é…åˆ°GPU
    torch_dtype=torch.float16
)

# 2. è®©æ¨¡å‹æ”¯æŒ k-bit è®­ç»ƒï¼ˆå†»ç»“é‡åŒ–æƒé‡ï¼‰
model = prepare_model_for_kbit_training(model)

# 3. å®šä¹‰ LoRA é…ç½®
lora_config = LoraConfig(
    r=8,                       # LoRA rank
    lora_alpha=16,             # ç¼©æ”¾å› å­
    target_modules=["q_proj","v_proj"],  # æŒ‡å®šåœ¨å“ªäº›å±‚æ’å…¥LoRA
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# 4. ç»™æ¨¡å‹åŠ ä¸Š LoRA å±‚ (å¾—åˆ° QLoRA)
model = get_peft_model(model, lora_config)

# 5. ç¤ºä¾‹è¾“å…¥
tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer("QLoRA is great because", return_tensors="pt").to("cuda")

# 6. å‰å‘ä¼ æ’­
outputs = model(**inputs)
print("Logits shape:", outputs.logits.shape)
```


## ğŸ“– è§£é‡Š

1. **`load_in_4bit=True`** â†’ æ¨¡å‹æƒé‡åŠ è½½ä¸º 4-bitï¼Œæ˜¾è‘—èŠ‚çœæ˜¾å­˜ã€‚
2. **`prepare_model_for_kbit_training`** â†’ å†»ç»“é‡åŒ–æƒé‡ï¼Œå¹¶å¯ç”¨å¿…è¦çš„æ¢¯åº¦æ”¯æŒã€‚
3. **`LoraConfig`** â†’ é…ç½® LoRA å‚æ•°ï¼Œæ¯”å¦‚ç§© `r=8`ï¼Œç›®æ ‡æ¨¡å—æ˜¯ Transformer é‡Œçš„ `q_proj, v_proj`ã€‚
4. **`get_peft_model`** â†’ æŠŠ LoRA æ’å…¥åˆ°æ¨¡å‹é‡Œï¼Œå½¢æˆ QLoRAã€‚
5. **è®­ç»ƒæ—¶** â†’ åªæ›´æ–° LoRA å‚æ•° (`A, B`)ï¼Œé‡åŒ–çš„æƒé‡ä¿æŒå†»ç»“ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ª **å®Œæ•´çš„ QLoRA å¾®è°ƒè®­ç»ƒå¾ªç¯** ç¤ºä¾‹ï¼Œæˆ‘ä»¬ç”¨ Hugging Face çš„ ğŸ¤— PEFT + Transformersï¼Œåœ¨ä¸€ä¸ªå°æ•°æ®é›†ï¼ˆæ¯”å¦‚ `tiny_shakespeare` æˆ–è€…ä¸€å°æ®µè‡ªå®šä¹‰æ–‡æœ¬ï¼‰ä¸Šè·‘å‡ æ­¥ï¼Œç›´è§‚æ¼”ç¤º **loss ä¸‹é™**ã€‚

---

## ğŸ“– QLoRA å¾®è°ƒå®Œæ•´ç¤ºä¾‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ (å°æ¨¡å‹ç¤ºä¾‹)
model_name = "facebook/opt-125m"  # ä¹Ÿå¯ä»¥æ¢æˆ LLaMA/OPT/GPT-NeoX ç­‰
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,           # å¼€å¯4-bité‡åŒ–
    device_map="auto",           # è‡ªåŠ¨åˆ†é…GPU
    torch_dtype=torch.float16
)

# 2. è®©æ¨¡å‹æ”¯æŒ k-bit è®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# 3. é…ç½® LoRA (QLoRA = 4-bité‡åŒ– + LoRA)
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # Transformeræ³¨æ„åŠ›å±‚å¸¸ç”¨çš„LoRAæ’å…¥ç‚¹
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# 4. æ„é€ ä¸€ä¸ªå°æ•°æ®é›† (tiny_shakespeare.txt æˆ–è€…è‡ªå®šä¹‰æ–‡æœ¬)
with open("tiny_shakespeare.txt", "w") as f:
    f.write("To be, or not to be, that is the question.\n"
            "Whether 'tis nobler in the mind to suffer\n"
            "The slings and arrows of outrageous fortune...")

train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="tiny_shakespeare.txt",
    block_size=64
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

# 5. è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./qlora_out",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=10,
    save_total_limit=1,
    logging_steps=5,
    learning_rate=2e-4,
    fp16=True,
    logging_dir="./logs"
)

# 6. Hugging Face Trainer è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset
)

trainer.train()

# 7. æµ‹è¯•ç”Ÿæˆ
inputs = tokenizer("To be, or not to", return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_length=30)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```



## ğŸ“– ä»£ç è¯´æ˜

1. **é‡åŒ–åŠ è½½**ï¼š`load_in_4bit=True` â†’ ä½¿ç”¨ 4-bit NF4 æƒé‡ã€‚
2. **LoRA é…ç½®**ï¼šä»…åœ¨æ³¨æ„åŠ›å±‚çš„ `q_proj, v_proj` ä¸Šæ’å…¥ä½ç§©é€‚é…å™¨ã€‚
3. **æ•°æ®é›†**ï¼šè¿™é‡Œç”¨ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ `tiny_shakespeare` æ–‡æœ¬ã€‚
4. **è®­ç»ƒå¾ªç¯**ï¼šä½¿ç”¨ `Trainer` å°è£…ï¼Œloss ä¼šé€æ¸ä¸‹é™ã€‚
5. **æ¨ç†**ï¼šè®­ç»ƒå®Œæˆåå¯ä»¥ç”Ÿæˆ Shakespeare é£æ ¼çš„æ–‡æœ¬ã€‚


