# Prompt Tuningå¾®è°ƒ
<img width="182" height="238" alt="image" src="https://github.com/user-attachments/assets/8454aee4-3658-4429-81d4-c07e0d728ea6" />

  Prompt Tuning æœ€æ—©æ˜¯ Google Research çš„ç ”ç©¶å›¢é˜Ÿ åœ¨ 2021 å¹´æå‡ºçš„ã€‚ç¬¬ä¸€ä½œè€…ï¼šBrian Lesterã€‚  

## ğŸ“– 1. Prompt Tuning çš„å®šä¹‰

**Prompt Tuning** æ˜¯ä¸€ç§ **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰** æ–¹æ³•ã€‚

* å®ƒé€šè¿‡åœ¨è¾“å…¥åºåˆ—å‰æ’å…¥ä¸€æ®µ **å¯å­¦ä¹ çš„è¿ç»­æç¤ºå‘é‡**ï¼ˆsoft promptï¼‰ï¼Œæ¥é€‚é…ä¸‹æ¸¸ä»»åŠ¡ã€‚
* ä¸ **P-Tuning** ä¸åŒï¼š

  * Prompt Tuning åªåœ¨ **embedding å±‚**åŠ æç¤ºï¼Œä¸åœ¨æ¨¡å‹ä¸­é—´å±‚æ’å…¥ã€‚
  * P-Tuning v1 æ›¾ä½¿ç”¨ LSTM/MHA æ¥å»ºæ¨¡ promptï¼Œæ›´å¤æ‚ã€‚
* è®­ç»ƒæ—¶ï¼Œ**ä»…æ›´æ–° soft prompt å‚æ•°**ï¼Œè€Œ **å†»ç»“æ•´ä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹**ã€‚



## ğŸ“– 2. æ•°å­¦æè¿°

è®¾ï¼š

* é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸º $f_\theta$ï¼Œå‚æ•° $\theta$ å†»ç»“ã€‚
* åŸå§‹è¾“å…¥ä¸º token åºåˆ—ï¼š

  $$
  X = (x_1, x_2, \dots, x_n), \quad x_i \in \mathbb{R}^d
  $$

  å…¶ä¸­ $d$ æ˜¯ embedding ç»´åº¦ã€‚

å®šä¹‰ soft prompt å‘é‡ï¼š

$$
P = (p_1, p_2, \dots, p_m), \quad p_j \in \mathbb{R}^d
$$

æ‹¼æ¥åå¾—åˆ°è¾“å…¥ï¼š

$$
X' = (p_1, p_2, \dots, p_m, x_1, x_2, \dots, x_n)
$$

ä»»åŠ¡ç›®æ ‡å‡½æ•°ï¼ˆä¾‹å¦‚åˆ†ç±»ä»»åŠ¡çš„äº¤å‰ç†µï¼‰ï¼š

$$
\mathcal{L}(P) = - \sum_{(X, y)} \log p(y \mid X'; \theta, P)
$$

å³ï¼šå†»ç»“ $\theta$ï¼Œä»…ä¼˜åŒ– $P$ã€‚



## ğŸ“– 3. ç®€å•ä»£ç æ¼”ç¤º

ä¸‹é¢ç”¨ Hugging Face + PyTorch ç»™å‡ºä¸€ä¸ªç®€åŒ–çš„ **Prompt Tuning** å®ç°ï¼ˆæ–‡æœ¬åˆ†ç±»ä¸ºä¾‹ï¼‰ï¼š

```python
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel

class PromptTuningModel(nn.Module):
    def __init__(self, model_name="bert-base-uncased", prompt_length=20, num_labels=2):
        super().__init__()
        # 1. é¢„è®­ç»ƒæ¨¡å‹
        self.bert = BertModel.from_pretrained(model_name)
        for param in self.bert.parameters():
            param.requires_grad = False  # å†»ç»“ BERT å‚æ•°
        
        # 2. å®šä¹‰ soft prompt (å¯å­¦ä¹ å‚æ•°)
        self.prompt_length = prompt_length
        self.hidden_size = self.bert.config.hidden_size
        self.soft_prompt = nn.Parameter(torch.randn(prompt_length, self.hidden_size))
        
        # 3. åˆ†ç±»å¤´
        self.classifier = nn.Linear(self.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        # è·å–åŸå§‹è¯åµŒå…¥
        inputs_embeds = self.bert.embeddings.word_embeddings(input_ids)

        # æ‰©å±• batch ç»´åº¦
        batch_size = input_ids.size(0)
        prompt_embeds = self.soft_prompt.unsqueeze(0).expand(batch_size, -1, -1)

        # æ‹¼æ¥ soft prompt å’ŒåŸå§‹è¾“å…¥
        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)

        # è°ƒæ•´ attention mask
        prompt_mask = torch.ones(batch_size, self.prompt_length).to(attention_mask.device)
        attention_mask = torch.cat([prompt_mask, attention_mask], dim=1)

        # è¾“å…¥åˆ° BERT
        outputs = self.bert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0]  # [CLS] token

        # åˆ†ç±»
        logits = self.classifier(cls_output)
        return logits

# ä½¿ç”¨ç¤ºä¾‹
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
inputs = tokenizer("Prompt tuning is efficient!", return_tensors="pt")

model = PromptTuningModel()
logits = model(**inputs)
print(logits)
```



## ğŸ“– æ€»ç»“

* **Prompt Tuning å®šä¹‰**ï¼šåœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ  **å¯å­¦ä¹ çš„ soft prompt**ï¼Œå†»ç»“æ¨¡å‹ï¼Œä»…è®­ç»ƒ prompt å‚æ•°ã€‚
* **æ•°å­¦å½¢å¼**ï¼š$\mathcal{L}(P) = - \sum \log p(y \mid [P; X]; \theta)$ã€‚
* **ä»£ç å®ç°**ï¼šç”¨ `nn.Parameter` å®šä¹‰ soft promptï¼Œå¹¶æ‹¼æ¥åˆ° embedding å‰ç«¯ã€‚



