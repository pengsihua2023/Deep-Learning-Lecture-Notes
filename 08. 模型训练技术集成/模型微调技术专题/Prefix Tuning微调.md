# Prefix Tuning å¾®è°ƒ


## ğŸ“– 1. å®šä¹‰

**Prefix Tuning** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT, Parameter-Efficient Fine-Tuning) æ–¹æ³•ã€‚
æ ¸å¿ƒæ€æƒ³ï¼š

* å†»ç»“é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼›
* åœ¨æ¯ä¸€å±‚ Transformer çš„ **æ³¨æ„åŠ›æœºåˆ¶è¾“å…¥å‰ï¼Œæ’å…¥ä¸€å°æ®µå¯è®­ç»ƒçš„â€œå‰ç¼€å‘é‡ (prefix tokens)â€**ï¼›
* è®­ç»ƒæ—¶åªæ›´æ–°è¿™éƒ¨åˆ† prefix å‚æ•°ï¼Œè€Œä¸æ”¹åŠ¨åŸå§‹æ¨¡å‹å‚æ•°ã€‚

è¿™æ ·ï¼ŒPrefix Tuning èƒ½æå¤§å‡å°‘éœ€è¦è®­ç»ƒçš„å‚æ•°é‡ï¼Œå¹¶ä¸”åœ¨ä¸åŒä»»åŠ¡é—´åªéœ€å­˜å‚¨ä¸€å°æ®µå‰ç¼€å³å¯å®Œæˆè¿ç§»ã€‚



## ğŸ“– 2. æ•°å­¦å…¬å¼

è®¾ Transformer çš„æ³¨æ„åŠ›å±‚è¾“å…¥ä¸º **query**ã€**key**ã€**value**ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
$$

åœ¨ Prefix Tuning ä¸­ï¼š

* å¯¹æ¯ä¸€å±‚ $l$ï¼Œå¼•å…¥ prefix key å’Œ prefix valueï¼š

$$
K' = [P_k^l; K], \quad V' = [P_v^l; V]
$$

å…¶ä¸­ï¼š

* $P_k^l, P_v^l$ æ˜¯å¯è®­ç»ƒçš„å‰ç¼€å‚æ•°ï¼Œé€šå¸¸é€šè¿‡ä¸€ä¸ªå°çš„ MLP ä» prefix embedding ç”Ÿæˆï¼›
* â€œ;â€ è¡¨ç¤ºæ‹¼æ¥æ“ä½œã€‚

å› æ­¤ï¼Œæ³¨æ„åŠ›æœºåˆ¶å˜ä¸ºï¼š

$$
\text{Attention}(Q, K', V') = \text{softmax}\left(\frac{Q {K'}^T}{\sqrt{d_k}}\right) V'
$$

è®­ç»ƒæ—¶åªæ›´æ–° $\{P_k^l, P_v^l\}$ï¼Œå†»ç»“åŸå§‹ $W_Q, W_K, W_V$ã€‚


## ğŸ“– 3. æœ€ç®€ä»£ç ä¾‹å­

ç”¨ **PyTorch** å†™ä¸€ä¸ªæœ€å°åŒ– Prefix Tuning çš„ç¤ºä¾‹ï¼ˆåœ¨ä¸€ä¸ª Transformer å±‚é‡ŒåŠ  prefixï¼‰ï¼š

```python
import torch
import torch.nn as nn

class PrefixTuningAttention(nn.Module):
    def __init__(self, d_model=128, n_heads=4, prefix_len=5):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads)
        self.prefix_len = prefix_len

        # å¯è®­ç»ƒçš„ prefix å‚æ•° (prefix_len ä¸ªå‰ç¼€ token)
        self.prefix_key = nn.Parameter(torch.randn(prefix_len, 1, d_model))
        self.prefix_value = nn.Parameter(torch.randn(prefix_len, 1, d_model))

    def forward(self, x):
        # x: [seq_len, batch, d_model]
        seq_len, batch, d_model = x.shape

        # å°† prefix æ‰©å±•åˆ° batch ç»´åº¦
        pk = self.prefix_key.expand(-1, batch, -1)  # [prefix_len, batch, d_model]
        pv = self.prefix_value.expand(-1, batch, -1)

        # ç”ŸæˆåŸå§‹ Q,K,V
        q = k = v = x

        # æ‹¼æ¥ prefix åˆ° K,V ä¸Š
        k = torch.cat([pk, k], dim=0)
        v = torch.cat([pv, v], dim=0)

        # æ³¨æ„åŠ›è®¡ç®—
        out, _ = self.attn(q, k, v)
        return out

# ===== æµ‹è¯• =====
x = torch.randn(10, 2, 128)  # [seq_len=10, batch=2, hidden=128]
layer = PrefixTuningAttention()
out = layer(x)
print("è¾“å‡ºå½¢çŠ¶:", out.shape)
```

è¾“å‡ºç»“æœï¼š

```
è¾“å‡ºå½¢çŠ¶: torch.Size([10, 2, 128])
```

è¯´æ˜ Prefix Tuning å±‚æ­£å¸¸è¿è¡Œã€‚



## ğŸ“– æ€»ç»“

* **Prefix Tuning**ï¼šåœ¨æ³¨æ„åŠ›å±‚å‰å¼•å…¥ prefix key/valueï¼Œä¸ä¿®æ”¹åŸå§‹æƒé‡ã€‚
* **ä¼˜ç‚¹**ï¼šæå¤§é™ä½è®­ç»ƒå‚æ•°é‡ï¼Œæ–¹ä¾¿å¤šä»»åŠ¡å…±äº«é¢„è®­ç»ƒæ¨¡å‹ã€‚
* **æ ¸å¿ƒå…¬å¼**ï¼š

$$
K' = [P_k; K], \quad V' = [P_v; V]
$$

---


## ğŸ“– Prefix Tuning with Hugging Face PEFT
ä¸‹é¢ä½¿ç”¨ Hugging Face PEFT åœ¨ BERT ä¸Šè¿›è¡Œ Prefix Tuning å¾®è°ƒ çš„å®Œæ•´ç¤ºä¾‹ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªå°å‹æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆSST-2 æƒ…æ„Ÿåˆ†ç±»ï¼‰æ¥æ¼”ç¤ºã€‚  
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
from peft import get_peft_model, PrefixTuningConfig, TaskType

# 1. åŠ è½½é¢„è®­ç»ƒ BERT æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# 2. å®šä¹‰ Prefix Tuning é…ç½®
peft_config = PrefixTuningConfig(
    task_type=TaskType.SEQ_CLS,   # ä»»åŠ¡ç±»å‹ï¼šåºåˆ—åˆ†ç±»
    num_virtual_tokens=20,        # prefix çš„é•¿åº¦ï¼ˆå¯è°ƒï¼‰
    encoder_hidden_size=768       # BERT çš„ hidden dim
)

# 3. æŠŠæ¨¡å‹åŒ…è£…æˆ Prefix Tuning æ¨¡å‹
model = get_peft_model(model, peft_config)

# 4. åŠ è½½æ•°æ®é›† (SST-2 æƒ…æ„Ÿåˆ†ç±»)
dataset = load_dataset("glue", "sst2")

def preprocess(example):
    return tokenizer(example["sentence"], truncation=True, padding="max_length", max_length=128)

encoded_dataset = dataset.map(preprocess, batched=True)
encoded_dataset = encoded_dataset.rename_column("label", "labels")
encoded_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

train_dataset = encoded_dataset["train"].shuffle(seed=42).select(range(2000))  # å°æ ·æœ¬æ¼”ç¤º
eval_dataset = encoded_dataset["validation"].shuffle(seed=42).select(range(500))

# 5. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./prefix_tuning_out",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    learning_rate=5e-4,
    evaluation_strategy="steps",
    eval_steps=50,
    logging_steps=50,
    save_strategy="no",
    fp16=True
)

# 6. Trainer API è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()

# 7. æµ‹è¯•æ¨ç†
text = "The movie was fantastic!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
pred = outputs.logits.argmax(dim=-1).item()
print(f"è¾“å…¥: {text} â†’ é¢„æµ‹ç±»åˆ«: {pred}")
```


## ğŸ“– è¯´æ˜

1. **Prefix Tuning é…ç½®**

   * `num_virtual_tokens=20` è¡¨ç¤ºåœ¨æ¯å±‚æ³¨æ„åŠ›å‰æ³¨å…¥ 20 ä¸ª prefix tokensã€‚
   * è®­ç»ƒæ—¶åªæ›´æ–° prefix å‚æ•°ï¼Œå…¶ä½™ BERT æƒé‡ä¿æŒå†»ç»“ã€‚

2. **æ•°æ®é›†**

   * ä½¿ç”¨ `GLUE/SST-2` æ•°æ®é›†ï¼ˆäºŒåˆ†ç±»ï¼špositive / negativeï¼‰ã€‚
   * è¿™é‡Œåªå–ä¸€å°éƒ¨åˆ†æ•°æ®è®­ç»ƒï¼Œæ–¹ä¾¿å¿«é€Ÿæµ‹è¯•ã€‚

3. **è®­ç»ƒ**

   * åªæ›´æ–° prefix å‚æ•°ï¼Œæ˜¾å­˜å’Œå‚æ•°é‡å¤§å¹…å‡å°‘ã€‚
   * è®­ç»ƒå‡ è½®å³å¯çœ‹åˆ°éªŒè¯é›†å‡†ç¡®ç‡ä¸Šå‡ã€‚

