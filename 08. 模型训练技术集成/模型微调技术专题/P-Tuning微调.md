# P-Tuningå¾®è°ƒ

<img width="105" height="140" alt="image" src="https://github.com/user-attachments/assets/7baa59a8-490f-4c58-b11b-757bbecfcb69" />  
  
P-Tuning æœ€æ—©æ˜¯ç”± å¤æ—¦å¤§å­¦é‚±é”¡é¹å›¢é˜Ÿï¼ˆFudanNLP, Qiu Xipeng ç­‰ï¼‰ åœ¨ 2021 å¹´æå‡ºçš„ã€‚  

## ğŸ“– 1. P-Tuning çš„å®šä¹‰

**P-Tuning**ï¼ˆPrompt Tuning çš„ä¸€ç§å˜ä½“ï¼‰æ˜¯ä¸€ç§ **å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•**ï¼Œå®ƒé€šè¿‡åœ¨è¾“å…¥åºåˆ—å‰æ’å…¥ **å¯å­¦ä¹ çš„è¿ç»­æç¤ºå‘é‡**ï¼ˆcontinuous promptsï¼‰ï¼Œæ¥å¼•å¯¼é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å®Œæˆä¸‹æ¸¸ä»»åŠ¡ã€‚

* åŸå§‹æ¨¡å‹å‚æ•°ä¿æŒå†»ç»“ã€‚
* åªè®­ç»ƒæç¤ºå‘é‡ï¼ˆé€šå¸¸æ˜¯å‡ ç™¾åˆ°å‡ åƒä¸ªå‚æ•°ï¼‰ï¼Œä»è€Œå¤§å¹…å‡å°‘å¾®è°ƒå¼€é”€ã€‚

ä¸ä¼ ç»Ÿ â€œç¦»æ•£ promptâ€ ä¸åŒï¼ŒP-Tuning çš„æç¤ºæ˜¯ **è¿ç»­å¯å¾®çš„å‘é‡**ï¼Œå¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™ç›´æ¥ä¼˜åŒ–ã€‚



## ğŸ“– 2. æ•°å­¦æè¿°

å‡è®¾ï¼š

* é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸º $f_\theta$ï¼Œå‚æ•° $\theta$ å†»ç»“ã€‚
* åŸå§‹è¾“å…¥åºåˆ—ä¸ºï¼š

  $$
  X = (x_1, x_2, \dots, x_n), \quad x_i \in \mathbb{R}^d
  $$

  å…¶ä¸­ $d$ æ˜¯è¯å‘é‡ç»´åº¦ã€‚

æˆ‘ä»¬å®šä¹‰ **å¯å­¦ä¹ çš„æç¤ºå‘é‡**ï¼š

$$
P = (p_1, p_2, \dots, p_m), \quad p_j \in \mathbb{R}^d
$$

æ–°çš„è¾“å…¥åºåˆ—ä¸ºï¼š

$$
X' = (p_1, p_2, \dots, p_m, x_1, x_2, \dots, x_n)
$$

è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬åªä¼˜åŒ– $P$ï¼Œç›®æ ‡å‡½æ•°ï¼ˆä¾‹å¦‚åˆ†ç±»ä»»åŠ¡çš„äº¤å‰ç†µï¼‰ä¸ºï¼š

$$
\mathcal{L}(P) = - \sum_{(X, y)} \log p(y \mid X'; \theta, P)
$$

å…¶ä¸­ï¼š

* $\theta$ å›ºå®šï¼ˆæ¨¡å‹å‚æ•°å†»ç»“ï¼‰ã€‚
* ä»…é€šè¿‡åå‘ä¼ æ’­æ›´æ–° $P$ã€‚



## ğŸ“– 3. ç®€å•ä»£ç ç¤ºä¾‹

ä¸‹é¢ç»™å‡ºä¸€ä¸ª Hugging Face `transformers` ä¸Šçš„ **P-Tuning ç®€åŒ–å®ç°**ï¼ˆæ–‡æœ¬åˆ†ç±»ä¸ºä¾‹ï¼‰ï¼š

```python
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel

class PTuningModel(nn.Module):
    def __init__(self, model_name="bert-base-uncased", prompt_length=10, num_labels=2):
        super().__init__()
        # åŠ è½½é¢„è®­ç»ƒ BERT
        self.bert = BertModel.from_pretrained(model_name)
        for param in self.bert.parameters():
            param.requires_grad = False  # å†»ç»“é¢„è®­ç»ƒå‚æ•°
        
        # å®šä¹‰å¯å­¦ä¹ çš„ prompt å‘é‡
        self.prompt_length = prompt_length
        self.hidden_size = self.bert.config.hidden_size
        self.prompt_embeddings = nn.Parameter(torch.randn(prompt_length, self.hidden_size))
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(self.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        # åŸå§‹è¾“å…¥ embedding
        inputs_embeds = self.bert.embeddings.word_embeddings(input_ids)

        # æ‰©å±• batch ç»´åº¦çš„ prompt
        batch_size = input_ids.size(0)
        prompt = self.prompt_embeddings.unsqueeze(0).expand(batch_size, -1, -1)

        # æ‹¼æ¥ prompt + åŸå§‹ embedding
        inputs_embeds = torch.cat([prompt, inputs_embeds], dim=1)

        # æ³¨æ„åŠ› mask ä¹Ÿè¦æ‰©å±•
        prompt_mask = torch.ones(batch_size, self.prompt_length).to(attention_mask.device)
        attention_mask = torch.cat([prompt_mask, attention_mask], dim=1)

        # é€å…¥ BERT
        outputs = self.bert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token

        # åˆ†ç±»
        logits = self.classifier(pooled_output)
        return logits

# æµ‹è¯•
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
inputs = tokenizer("P-Tuning is amazing!", return_tensors="pt", padding=True, truncation=True)
model = PTuningModel()

logits = model(**inputs)
print(logits)
```


## ğŸ“– æ€»ç»“

* **å®šä¹‰**ï¼šåœ¨è¾“å…¥åºåˆ—å‰æ’å…¥è¿ç»­å¯å­¦ä¹ çš„æç¤ºå‘é‡ï¼Œå†»ç»“é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªä¼˜åŒ–æç¤ºå‚æ•°ã€‚
* **æ•°å­¦å½¢å¼**ï¼š$\mathcal{L}(P) = - \sum \log p(y \mid [P; X]; \theta)$ã€‚
* **ä»£ç **ï¼šé€šè¿‡ `nn.Parameter` å®šä¹‰æç¤º embeddingï¼Œå¹¶æ‹¼æ¥åˆ°è¾“å…¥ embedding å‰ã€‚


