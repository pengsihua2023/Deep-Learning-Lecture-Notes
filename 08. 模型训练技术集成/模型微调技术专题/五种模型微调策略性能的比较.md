
# 📊 五种模型微调策略性能的比较

| 方法                | 定义                                                        | 数学形式                                                      | 优点               | 缺点                        | 适用场景                          |
| ----------------- | --------------------------------------------------------- | --------------------------------------------------------- | ---------------- | ------------------------- | ----------------------------- |
| **Prompt Tuning** | 在输入层前添加一段 **可学习 soft prompt**，冻结模型参数                      | 输入为 $[P;X]$，仅优化 $P$                                       | 简单，参数少，易实现       | 表达能力有限，大模型更稳定，小模型效果差      | 大规模预训练模型（>1B 参数）的分类/生成任务      |
| **P-Tuning v1**   | 在输入层添加 soft prompt，并用 **LSTM/MHA** 学习提示                   | $[g(P);X]$，其中 $g$ 是 LSTM/MHA                              | 灵活，能建模复杂提示       | 训练不稳定，开销大                 | 小规模模型（如 BERT-base）下游任务        |
| **P-Tuning v2**   | 在 **Transformer 每层** 添加 prefix embedding，类似 Prefix Tuning | 在注意力 Q/K/V 拼接 prefix：$\text{Attn}([XW_Q;P_Q],[XW_K;P_K])$ | 表达能力强，性能接近全参数微调  | 参数比 Prompt Tuning 多，实现更复杂 | 各类模型（BERT、GPT、T5），分类、生成、抽取等任务 |
| **LoRA**          | 在权重矩阵 $W$ 上插入 **低秩分解更新**：$\Delta W = AB^T$                | $W' = W + AB^T$，仅优化 $A,B$                                 | 高效，可与其他方法结合，推理友好 | 对部分任务收敛稍慢                 | 大模型微调（尤其是生成任务，如 LLaMA、GPT）    |
| **Model Pruning** | 移除不重要的权重/通道，再对剩余参数微调                                      | $W' = W \odot M$，其中 $M \in \{0,1\}^n$                     | 减少模型大小，加速推理      | 性能下降风险大，需要 fine-tuning    | 部署优化（移动端/低算力设备），模型压缩          |

---

# ✅ 总结

* 如果目标是 **最少参数开销**：Prompt Tuning。
* 如果模型较小、需要灵活表达：P-Tuning v1。
* 如果想兼顾性能与参数高效：P-Tuning v2。
* 如果想要可扩展、推理友好的方案：LoRA。
* 如果重点在 **模型压缩与加速**：Model Pruning。

