# LoHAå¾®è°ƒæ–¹æ³• (Low-rank Hadamard Product Approximation) 

## ğŸ“– 1. å®šä¹‰

**LoHA** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT, Parameter-Efficient Fine-Tuning) æ–¹æ³•ï¼Œå®ƒå’Œ **LoRA** ç±»ä¼¼ï¼Œä½†åœ¨ä½ç§©åˆ†è§£æ—¶å¼•å…¥äº† **Hadamard é€å…ƒç´ ä¹˜ç§¯**ï¼Œä»è€Œåœ¨ä¿æŒä½ç§©æ›´æ–°çš„åŒæ—¶å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ã€‚  
<div align="center">
<img width="125" height="170" alt="image" src="https://github.com/user-attachments/assets/37e20da6-608b-4325-b3ff-3ab91549b5f0" />
</div>

LoHAï¼ˆLowâ€‘Rank Hadamard Productï¼‰å¾®è°ƒæ–¹æ³• é¦–æ¬¡ç”± Nam Hyeonâ€‘Wooã€Moon Yeâ€‘Bin å’Œ Taeâ€‘Hyun Oh æå‡ºï¼Œå¹¶åœ¨ 2021 å¹´å‘è¡¨äº ICLR 2022ï¼ˆä¼šè®®æ—¶é—´ï¼‰ä½œä¸ºã€ŠFedPara: Lowâ€‘Rank Hadamard Product for Communicationâ€‘Efficient Federated Learningã€‹ä¸€æ–‡ä¸­æå‡ºã€‚

* **LoRA**ï¼šå¯¹æƒé‡çŸ©é˜µ $W \in \mathbb{R}^{d \times k}$ï¼Œé‡‡ç”¨ä½ç§©åˆ†è§£

$$
\Delta W = B A, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, \; r \ll \min(d,k)
$$

* **LoHA**ï¼šåœ¨ä½ç§©åˆ†è§£çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥ Hadamard ä¹˜ç§¯ï¼ˆé€å…ƒç´ ä¹˜æ³•ï¼‰ï¼Œå¢å¼ºå‚æ•°åŒ–è¡¨è¾¾èƒ½åŠ›ï¼š

  $$
  \Delta W = (B A) \odot (D C)
  $$

  å…¶ä¸­ï¼š

  * $B, A$ æ˜¯ç¬¬ä¸€ç»„ä½ç§©åˆ†è§£å‚æ•°ï¼›
  * $D, C$ æ˜¯ç¬¬äºŒç»„ä½ç§©åˆ†è§£å‚æ•°ï¼›
  * $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³• (Hadamard product)ã€‚

è¿™æ ·ï¼ŒLoHA ç›¸æ¯” LoRA åœ¨ç›¸ä¼¼å‚æ•°è§„æ¨¡ä¸‹ï¼Œèƒ½è¡¨ç¤ºæ›´å¤æ‚çš„å˜åŒ–ã€‚


## ğŸ“– 2. æ•°å­¦å…¬å¼

è®¾åŸå§‹æƒé‡ä¸º $W$ï¼ŒLoHA è®­ç»ƒæ—¶å†»ç»“ $W$ï¼Œä»…è®­ç»ƒ $\Delta W$ï¼š

1. **æœ‰æ•ˆæƒé‡**ï¼š

$$
W^{\text{eff}} = W + \Delta W
$$

2. **LoHA ä½ç§©æ›´æ–°**ï¼š

$$
\Delta W = (B A) \odot (D C)
$$

å…¶ä¸­ï¼š

* $B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$
* $D \in \mathbb{R}^{d \times r}, C \in \mathbb{R}^{r \times k}$
* $\odot$ ä¸ºé€å…ƒç´ ä¹˜ç§¯ã€‚

è®­ç»ƒæ—¶åªæ›´æ–° $(A, B, C, D)$ï¼ŒåŸå§‹æƒé‡ $W$ ä¿æŒå†»ç»“ã€‚


## ğŸ“– 3. æœ€ç®€ä»£ç ä¾‹å­

ç”¨ **PyTorch** å®ç°ä¸€ä¸ªæç®€ LoHA çº¿æ€§å±‚ï¼š

```python
import torch
import torch.nn as nn

class LoHALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=4):
        super().__init__()
        # å†»ç»“çš„åŸå§‹æƒé‡
        self.weight = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)
        
        # LoHA å‚æ•° (ä¸¤ç»„ä½ç§©åˆ†è§£)
        self.A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.B = nn.Parameter(torch.randn(out_features, rank) * 0.01)
        self.C = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.D = nn.Parameter(torch.randn(out_features, rank) * 0.01)

    def forward(self, x):
        # LoHA ä½ç§©æ›´æ–°
        delta_W = (self.B @ self.A) * (self.D @ self.C)  # Hadamard product
        W_eff = self.weight + delta_W
        return x @ W_eff.T  # çº¿æ€§å±‚è®¡ç®—

# ===== æµ‹è¯• =====
x = torch.randn(2, 10)   # è¾“å…¥
layer = LoHALinear(10, 5, rank=4)
out = layer(x)
print("è¾“å‡ºå½¢çŠ¶:", out.shape)
```

è¿è¡Œç»“æœï¼š`è¾“å‡ºå½¢çŠ¶: torch.Size([2, 5])`ï¼Œè¯´æ˜ LoHA çº¿æ€§å±‚æ­£å¸¸å·¥ä½œã€‚



ğŸ“–  æ€»ç»“ï¼š

* **LoRA**ï¼šä½ç§©åŠ æ³•æ›´æ–° $\Delta W = BA$ã€‚
* **LoHA**ï¼šä½ç§© + Hadamard æ›´æ–° $\Delta W = (BA) \odot (DC)$ï¼Œè¡¨ç¤ºèƒ½åŠ›æ›´å¼ºã€‚


