
# å·®å¼‚åŒ–å‰ªæ (Diff Pruning)

## ğŸ“– 1. å®šä¹‰

**å·®å¼‚åŒ–å‰ªæï¼ˆDiff Pruningï¼‰** æ˜¯ä¸€ç§ **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFT, Parameter-Efficient Fine-Tuningï¼‰** æ–¹æ³•ï¼Œæœ€æ—©ç”¨äºå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„é«˜æ•ˆé€‚é…ï¼ˆGuo et al., 2021ï¼‰ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š

* ä¸ç›´æ¥ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•° $\theta$ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªå‚æ•°å¼•å…¥ä¸€ä¸ª **å·®åˆ†å‚æ•°ï¼ˆdiff parameterï¼‰** $\Delta \theta$ã€‚
* åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­åªå­¦ä¹ è¿™äº›å·®åˆ†å‚æ•°ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚
* é€šè¿‡ **ç¨€ç–åŒ–çº¦æŸï¼ˆå¦‚ L1 æ­£åˆ™æˆ–é—¨æ§æœºåˆ¶ï¼‰**ï¼Œä»…åœ¨å¿…è¦çš„å‚æ•°ä½ç½®å¼•å…¥å·®åˆ†æ›´æ–°ï¼Œè¾¾åˆ° **é«˜æ•ˆ + å¯è§£é‡Š** çš„æ•ˆæœã€‚

ğŸ‘‰ ç®€å•æ¥è¯´ï¼š
æ¨¡å‹å‚æ•°æ›´æ–°æ–¹å¼ä»ï¼š

$$
\theta' = \theta + \Delta \theta
$$

è½¬å˜ä¸ºï¼š

* **å¤šæ•°ä½ç½®**ï¼š $\Delta \theta = 0$ ï¼ˆå³å†»ç»“ï¼Œä¸æ›´æ–°ï¼‰ã€‚
* **å°‘æ•°ä½ç½®**ï¼š $\Delta \theta \neq 0$ ï¼ˆåªæ›´æ–°å¿…è¦çš„å‚æ•°ï¼‰ã€‚


## ğŸ“– 2. æ•°å­¦æè¿°

### 2.1 å‚æ•°è¡¨ç¤º

è®¾ï¼š

* é¢„è®­ç»ƒå‚æ•°ï¼š$\theta \in \mathbb{R}^d$
* å·®åˆ†å‚æ•°ï¼š$\Delta \theta \in \mathbb{R}^d$
* ä¸‹æ¸¸æ¨¡å‹å‚æ•°ï¼š

$$
\theta' = \theta + \Delta \theta
$$

### 2.2 æŸå¤±å‡½æ•°

è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–ä¸‹æ¸¸ä»»åŠ¡çš„æŸå¤±ï¼ŒåŒæ—¶è®©å·®åˆ†å‚æ•°ç¨€ç–ï¼š

$$
\mathcal{L}(\Delta \theta) = \mathcal{L}_{task}(f(x; \theta + \Delta \theta)) + \lambda \|\Delta \theta\|_1
$$

* $\mathcal{L}_{task}$ï¼šä¸‹æ¸¸ä»»åŠ¡æŸå¤±ï¼ˆå¦‚åˆ†ç±»äº¤å‰ç†µï¼‰ã€‚
* $\|\Delta \theta\|_1$ï¼šL1 æ­£åˆ™åŒ–ï¼Œé¼“åŠ±ç¨€ç–æ€§ã€‚
* $\lambda$ï¼šæ­£åˆ™ç³»æ•°ã€‚

### 2.3 å‰ªææœºåˆ¶

* åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¾ˆå¤š $\Delta \theta$ ä¼šæ”¶æ•›åˆ°æ¥è¿‘ 0ã€‚
* æœ€ç»ˆå¯ä»¥å°†è¿™äº›ä½ç½®å‰ªæï¼Œåªä¿ç•™å°‘é‡éé›¶å‚æ•°ã€‚

## ğŸ“– 3. ç®€å•ä»£ç ç¤ºä¾‹ï¼ˆPyTorchï¼‰

ä¸‹é¢ç”¨ PyTorch å®ç°ä¸€ä¸ª **çº¿æ€§å±‚çš„å·®å¼‚åŒ–å‰ªæç¤ºä¾‹**ï¼š

```python
import torch
import torch.nn as nn
import torch.optim as optim

# å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»æ¨¡å‹
class BaseModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(BaseModel, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim, bias=False)
        # å†»ç»“é¢„è®­ç»ƒå‚æ•°
        for p in self.fc.parameters():
            p.requires_grad = False
        # å·®åˆ†å‚æ•°ï¼ˆå¯è®­ç»ƒï¼‰
        self.delta = nn.Parameter(torch.zeros_like(self.fc.weight))

    def forward(self, x):
        # åŸå§‹å‚æ•° + å·®åˆ†å‚æ•°
        return (self.fc.weight + self.delta) @ x.T

# æ¨¡æ‹Ÿæ•°æ®
X = torch.randn(10, 5)   # batch=10, input_dim=5
y = torch.randint(0, 2, (10,))  # äºŒåˆ†ç±»æ ‡ç­¾

# åˆå§‹åŒ–æ¨¡å‹
model = BaseModel(input_dim=5, output_dim=2)

# æŸå¤±å‡½æ•° + ä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam([model.delta], lr=0.01)

# è®­ç»ƒ
for epoch in range(50):
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs.T, y)
    # L1 æ­£åˆ™åŒ–çº¦æŸå·®åˆ†å‚æ•°
    loss = loss + 0.01 * torch.norm(model.delta, p=1)
    loss.backward()
    optimizer.step()

print("Trained Î”Î¸ (sparse updates):")
print(model.delta)
```


## ğŸ“– 4. æ€»ç»“

* **å®šä¹‰**ï¼šDiff Pruning é€šè¿‡å­¦ä¹  **ç¨€ç–å·®åˆ†å‚æ•°** æ¥é«˜æ•ˆå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚
* **æ•°å­¦å…¬å¼**ï¼š

$$
\theta' = \theta + \Delta \theta, \quad 
\mathcal{L} = \mathcal{L}_{task} + \lambda \|\Delta \theta\|_1
$$
* **ç‰¹ç‚¹**ï¼š

  * èŠ‚çœæ˜¾å­˜å’Œè®¡ç®—é‡ï¼ˆåªæ›´æ–°å°‘é‡å‚æ•°ï¼‰ã€‚
  * ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
  * å‰ªæåå¾—åˆ°ç¨€ç–å¯è§£é‡Šçš„å·®åˆ†æ›´æ–°ã€‚
* **ä»£ç **ï¼šé€šè¿‡å†»ç»“åŸå§‹å‚æ•°ï¼Œä»…è®­ç»ƒ $\Delta \theta$ ï¼Œå¹¶åŠ ä¸Š L1 æ­£åˆ™å®ç°ç¨€ç–åŒ–ã€‚


