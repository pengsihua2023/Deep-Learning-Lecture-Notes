# Attention Visualization

## 1. å®šä¹‰

**Attention Visualization** æ˜¯ä¸€ç§ **æ¨¡å‹è§£é‡Šæ–¹æ³•**ï¼Œå®ƒé€šè¿‡å¯è§†åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„ **æ³¨æ„åŠ›æƒé‡ï¼ˆAttention Weightsï¼‰**ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£æ¨¡å‹åœ¨é¢„æµ‹æ—¶â€œå…³æ³¨äº†å“ªäº›è¾“å…¥éƒ¨åˆ†â€ã€‚

* åœ¨ **NLPï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰** ä¸­ï¼šå±•ç¤ºè¯ä¸è¯ä¹‹é—´çš„æ³¨æ„å…³ç³»ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘æ¨¡å‹åœ¨ç¿»è¯‘æŸä¸ªè¯æ—¶ä¸»è¦ä¾èµ–å“ªäº›ä¸Šä¸‹æ–‡è¯ã€‚
* åœ¨ **CVï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰** ä¸­ï¼šå±•ç¤ºå›¾åƒåŒºåŸŸçš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œä¾‹å¦‚è§†è§‰ Transformer åœ¨è¯†åˆ«â€œçŒ«â€æ—¶ï¼Œä¸»è¦å…³æ³¨çŒ«çš„è„¸éƒ¨åŒºåŸŸã€‚


## 2. æ•°å­¦æè¿°

æ³¨æ„åŠ›æƒé‡çš„è®¡ç®—å…¬å¼æ¥è‡ª **Scaled Dot-Product Attention**ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

å…¶ä¸­ï¼š

* $Q$ = Queries
* $K$ = Keys
* $V$ = Values
* $\frac{QK^T}{\sqrt{d_k}}$ = ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆç›¸å…³æ€§åˆ†æ•°ï¼‰
* $\text{softmax}(\cdot)$ = å°†åˆ†æ•°è½¬ä¸ºæ³¨æ„åŠ›æƒé‡ï¼ŒèŒƒå›´åœ¨ $[0,1]$ï¼Œä¸”å’Œä¸º 1

ğŸ‘‰ **å¯è§†åŒ–çš„æ ¸å¿ƒ**ï¼šæŠŠæ³¨æ„åŠ›æƒé‡çŸ©é˜µ

$$
A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)
$$

æ˜¾ç¤ºä¸º **çƒ­åŠ›å›¾ / ç®­å¤´å›¾ / å åŠ å›¾**ï¼Œç›´è§‚å±•ç¤ºæ¨¡å‹çš„å…³æ³¨æ¨¡å¼ã€‚



## 3. ä»£ç ç¤ºä¾‹

### 3.1 åœ¨ NLP ä¸­çš„ Attention å¯è§†åŒ–ï¼ˆçƒ­åŠ›å›¾ï¼‰

```python
import torch
import torch.nn as nn
import seaborn as sns
import matplotlib.pyplot as plt

# å®šä¹‰ Multi-Head Attention
mha = nn.MultiheadAttention(embed_dim=16, num_heads=2, batch_first=True)

# è¾“å…¥ (batch=1, seq_len=5, d_model=16)
x = torch.rand(1, 5, 16)

# è‡ªæ³¨æ„åŠ› (Q=K=V=x)
out, attn_weights = mha(x, x, x)

print("Attention weights shape:", attn_weights.shape)  # [1, num_heads, seq_len, seq_len]

# å¯è§†åŒ–ç¬¬ä¸€å¤´çš„æ³¨æ„åŠ›
sns.heatmap(attn_weights[0, 0].detach().numpy(), cmap="viridis", annot=True)
plt.xlabel("Key positions")
plt.ylabel("Query positions")
plt.title("Attention Heatmap (Head 1)")
plt.show()
```

ğŸ‘‰ è¿è¡Œåå¾—åˆ°ä¸€ä¸ª **çƒ­åŠ›å›¾**ï¼Œæ¨ªè½´è¡¨ç¤º **Keyï¼ˆè¢«å…³æ³¨çš„è¯ï¼‰**ï¼Œçºµè½´è¡¨ç¤º **Queryï¼ˆå½“å‰è¯ï¼‰**ï¼Œé¢œè‰²æ·±æµ…è¡¨ç¤ºæ³¨æ„åŠ›æƒé‡å¤§å°ã€‚



### 3.2 åœ¨ CV ä¸­çš„ Attention å¯è§†åŒ–ï¼ˆå åŠ å›¾ï¼‰

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡ (å‡è®¾å›¾åƒè¢«åˆ†æˆ 4x4 patch)
attn_weights = torch.rand(1, 1, 16, 16)  # [batch, head, patch_num, patch_num]
attn_map = attn_weights[0, 0].mean(0).reshape(4,4).detach().numpy()

# åŸå›¾ (éšæœºæ¨¡æ‹Ÿä¸€å¼  64x64 å›¾åƒ)
img = np.random.rand(64,64)

# å¯è§†åŒ–
plt.imshow(img, cmap="gray")
plt.imshow(attn_map, cmap="jet", alpha=0.5, extent=(0,64,64,0))  # å åŠ æ³¨æ„åŠ›
plt.title("Attention Overlay on Image")
plt.colorbar()
plt.show()
```

ğŸ‘‰ è¿è¡Œåå¾—åˆ°ä¸€ä¸ªå›¾åƒï¼Œå åŠ äº† **æ³¨æ„åŠ›çƒ­åŠ›å›¾**ï¼Œå±•ç¤ºæ¨¡å‹åœ¨å›¾åƒåˆ†ç±»æ—¶ä¸»è¦å…³æ³¨çš„åŒºåŸŸã€‚



## 4. æ€»ç»“

* **Attention Visualization å®šä¹‰**ï¼šé€šè¿‡å±•ç¤ºæ³¨æ„åŠ›æƒé‡ï¼Œè§£é‡Šæ¨¡å‹åœ¨é¢„æµ‹æ—¶â€œçœ‹å“ªé‡Œâ€ã€‚
* **æ•°å­¦æè¿°**ï¼šåŸºäº

$$
A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)
$$
* **ä»£ç ç¤ºä¾‹**ï¼š

  * NLP â†’ ç”¨çƒ­åŠ›å›¾å±•ç¤ºè¯ä¸è¯ä¹‹é—´çš„æ³¨æ„å…³ç³»ã€‚
  * CV â†’ ç”¨çƒ­åŠ›å›¾å åŠ åˆ°å›¾åƒä¸Šï¼Œå±•ç¤ºæ¨¡å‹çš„å…³æ³¨åŒºåŸŸã€‚



