## 深度学习模型训练技术集成
- 关键技术和诀窍
  - [自动混合精度（AMP）训练](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%EF%BC%88AMP%EF%BC%89%E8%AE%AD%E7%BB%83.md)
  - [Curriculum Learning](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/Curriculum%20Learning.md)
  - [Optuna超参数优化方法](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/Optuna%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.md)
  - [Ray Tune超参数优化方法](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/Ray%20Tune%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95.md)
  - [处理类别不平衡的两种方法：加权损失函数和过采样](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E5%A4%84%E7%90%86%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%EF%BC%9A%E5%8A%A0%E6%9D%83%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%BF%87%E9%87%87%E6%A0%B7.md)
  - [Min-Max 归一化](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/Min-Max%20%E5%BD%92%E4%B8%80%E5%8C%96.md)
  - [Z-score 标准化](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/Z-score%20%E6%A0%87%E5%87%86%E5%8C%96.md)
  - [混合精度训练Mixed Precision Training](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83Mixed%20Precision%20Training.md)
  - [多GPU并行训练 （Distributed Data Parallel (DDP)）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E5%A4%9AGPU%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%20%EF%BC%88Distributed%20Data%20Parallel%20(DDP)%EF%BC%89.md)
  - [梯度裁剪（clip_grad_norm_)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA.md)
  - [累积梯度 （Gradient Accumulation）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E7%B4%AF%E7%A7%AF%E6%A2%AF%E5%BA%A6%20%EF%BC%88Gradient%20Accumulation%EF%BC%89.md)
  - [高效注意力计算 （Flash Attention）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E9%AB%98%E6%95%88%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AE%A1%E7%AE%97%20%EF%BC%88Flash%20Attention%EF%BC%89.md)
  - [超参数搜索 （Bayesian Optimization）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2%20%EF%BC%88Bayesian%20Optimization%EF%BC%89.md)
  - [多个模型集成 （Ensemble Learning）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E5%92%8C%E8%AF%80%E7%AA%8D/%E5%A4%9A%E4%B8%AA%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90%20%EF%BC%88Ensemble%20Learning%EF%BC%89.md)
- [正则化技术概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E6%A6%82%E8%BF%B0.md)
  - [L1范数正则化 (L1 Regularization)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/L1%E8%8C%83%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96.md)
  - [L2范数正则化 （L2 Regularization (Weight Decay)）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/L2%E8%8C%83%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96.md)  - 
  - [标准化批次输入 (Batch Normalization)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E6%A0%87%E5%87%86%E5%8C%96%E6%89%B9%E6%AC%A1%E8%BE%93%E5%85%A5%EF%BC%88Batch%20Normalization%EF%BC%89.md)
  -  [标准化层输入 (Layer Normalization)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E6%A0%87%E5%87%86%E5%8C%96%E5%B1%82%E8%BE%93%E5%85%A5%20(Layer%20Normalization).md)
  -  [训练早停(Early Stopping)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E8%AE%AD%E7%BB%83%E6%97%A9%E5%81%9C.md)
  -  [添加噪声到输入/权重 （Noise Injection）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E6%B7%BB%E5%8A%A0%E5%99%AA%E5%A3%B0%E5%88%B0%E8%BE%93%E5%85%A5-%E6%9D%83%E9%87%8D%EF%BC%88Noise%20Injection%EF%BC%89.md)
  -  [随机丢弃神经元 (Dropout)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E9%9A%8F%E6%9C%BA%E4%B8%A2%E5%BC%83%E7%A5%9E%E7%BB%8F%E5%85%83.md)
- [学习率调整方法概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0.md)
  - [动态调整学习率 （Learning Rate Scheduling）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88Learning%20Rate%20Scheduling%EF%BC%89.md)
  - [根据损失监控自动降低学习率 （ReduceLROnPlateau）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E6%A0%B9%E6%8D%AE%E6%8D%9F%E5%A4%B1%E7%9B%91%E6%8E%A7%E8%87%AA%E5%8A%A8%E9%99%8D%E4%BD%8E%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88ReduceLROnPlateau%EF%BC%89.md)
  - [自适应学习率 （Adam Optimizer）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%20%EF%BC%88Adam%20Optimizer%EF%BC%89.md)
  - [自适应学习率 （RMSprop）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%20%EF%BC%88RMSprop%EF%BC%89.md)
- [优化器概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A6%82%E8%BF%B0.md)
  - [Adam优化器](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adam%20Optimizer.md)
  - [Adam变体 （AdamW）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adam%E5%8F%98%E4%BD%93%20%EF%BC%88AdamW%EF%BC%89.md)
  - [SGD优化器（随机梯度下降，Stochastic Gradient Descent）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/SGD%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%8CStochastic%20Gradient%20Descent%EF%BC%89.md)
  - [RMSProp优化器（Root Mean Square Propagation）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/RMSProp%20Optimizer.md)
  - [Adagrad优化器（Adaptive Gradient Algorithm）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adagrad%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Adaptive%20Gradient%20Algorithm%EF%BC%89.md)
  - [Adadelta优化器](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adadelta%E4%BC%98%E5%8C%96%E5%99%A8.md)  
  - [AMSGrad优化器](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/AMSGrad%E4%BC%98%E5%8C%96%E5%99%A8.md)
  - Nadam优化器
  - [L-BFGS优化器（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/L-BFGS%E4%BC%98%E5%8C%96%E5%99%A8.md)
  - Rprop优化器（Resilient Backpropagation）
  - SparseAdam优化器
  - ASGD优化器（Averaged Stochastic Gradient Descent）
- [初始化方法概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0.md)
  - [均匀/正态分布初始化 （Xavier/Glorot Initialization](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%9D%87%E5%8C%80-%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E5%88%9D%E5%A7%8B%E5%8C%96%20%EF%BC%88Xavier-Glorot%20Initialization%EF%BC%89.md)
  - 考虑ReLU的方差的初始化 （He Initialization）
  - 均匀分布初始化 （Uniform Initialization）
  - [正态分布初始化 （Normal Initialization）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E5%88%9D%E5%A7%8B%E5%8C%96%20%EF%BC%88Normal%20Initialization%EF%BC%89.md)
  - He初始化的均匀变体 （Kaiming Uniform）
  - 层级标准化初始化 （LSUV (Layer-Sequential Unit-Variance)）
  - 零初始化 （Zero Initialization）
- 模型微调技术概述
  - [PEFT 库简介](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/PEFT%20%E5%BA%93%E6%A6%82%E8%BF%B0.md)
  - [IA³微调](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/IA%C2%B3%E5%BE%AE%E8%B0%83.md)
  - 少参数微调 （LoRA）
  - 微调提示嵌入 （Prompt Tuning）
  - 重启LoRA （ReLoRA）
  - 量化+LoRA （QLoRA）
  - [差异化剪枝 （Diff Pruning）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E5%B7%AE%E5%BC%82%E5%8C%96%E5%89%AA%E6%9E%9D%20(Diff%20Pruning)z.md)
  - 添加小型适配器层 （Adapter Modules）
  - 模型剪枝（移除不重要权重, Model Pruning）
- [模型评估方法汇总](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB.md)
- [模型解释方法概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0.md)
  - [SHAP](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/SHAP%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95.md)
  - LIME
  - Saliency Maps
  - [Integrated Gradients](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/Integrated%20Gradients.md)
  - Grad-CAM
  - [DeepLIFT](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/DeepLIFT%20%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95.md)
  - t-SNE 
  - [Attention Visualization](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A8%A1%E5%9E%8B%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/Attention%20Visualization.md)

