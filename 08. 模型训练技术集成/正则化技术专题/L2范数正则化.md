## L2èŒƒæ•°æ­£åˆ™åŒ–
### ğŸ“– ä»€ä¹ˆæ˜¯L2èŒƒæ•°æ­£åˆ™åŒ–ï¼Ÿ

L2èŒƒæ•°æ­£åˆ™åŒ–ï¼ˆä¹Ÿç§°ä¸ºæƒé‡è¡°å‡ï¼ŒWeight Decayï¼‰æ˜¯ä¸€ç§åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç”¨äºé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œé™åˆ¶æ¨¡å‹å‚æ•°ï¼ˆæƒé‡ï¼‰çš„å¤§å°ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

#### æ ¸å¿ƒåŸç†
- **æŸå¤±å‡½æ•°ä¿®æ”¹**ï¼šåœ¨åŸå§‹æŸå¤±å‡½æ•°ï¼ˆå¦‚å‡æ–¹è¯¯å·®æˆ–äº¤å‰ç†µï¼‰ä¸Šæ·»åŠ L2èŒƒæ•°çš„æƒ©ç½šé¡¹ï¼š


$$
\text{Loss}_ {\text{regularized}} = \text{Loss}_{\text{original}} + \lambda \sum_i \|w_i\|_2^2
$$

å…¶ä¸­ï¼š

* $\text{Loss}_{\text{original}}$ æ˜¯åŸå§‹æŸå¤±ï¼ˆå¦‚åˆ†ç±»è¯¯å·®ï¼‰ã€‚
* $w_i$ æ˜¯æ¨¡å‹å‚æ•°ï¼ˆå¦‚æƒé‡ï¼‰ã€‚
* $\|w_i\|_2^2$ æ˜¯æƒé‡çš„ L2 èŒƒæ•°ï¼ˆå³æƒé‡å¹³æ–¹å’Œï¼‰ã€‚
*  $\lambda$ æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„è¶…å‚æ•°ï¼Œæ§åˆ¶æƒ©ç½šåŠ›åº¦ã€‚



- **æ•ˆæœ**ï¼š
  - L2æ­£åˆ™åŒ–å€¾å‘äºè®©æƒé‡å˜å°ï¼ˆä½†ä¸å®Œå…¨ä¸º0ï¼‰ï¼Œä»è€Œä½¿æ¨¡å‹æ›´å¹³æ»‘ï¼Œå‡å°‘å¯¹è®­ç»ƒæ•°æ®çš„è¿‡æ‹Ÿåˆã€‚
  - å®ƒé¼“åŠ±æ¨¡å‹å­¦ä¹ è¾ƒå°çš„æƒé‡ï¼Œé™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œé˜²æ­¢å¯¹å™ªå£°çš„è¿‡åº¦æ‹Ÿåˆã€‚

- **ä¸L1æ­£åˆ™åŒ–çš„åŒºåˆ«**ï¼š


* L1 æ­£åˆ™åŒ–ï¼ˆLassoï¼‰ä½¿ç”¨ $\sum |w_i|$ï¼Œå€¾å‘äºäº§ç”Ÿç¨€ç–æƒé‡ï¼ˆéƒ¨åˆ†æƒé‡ä¸º 0ï¼‰ã€‚
* L2 æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰ä½¿ç”¨ $\sum w_i^2$ï¼Œå€¾å‘äºåˆ†æ•£æƒé‡å€¼ï¼Œä¿æŒæ‰€æœ‰æƒé‡è¾ƒå°ã€‚



#### åº”ç”¨åœºæ™¯
- æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚CNNã€RNNã€Transformerï¼‰ä¸­ï¼ŒL2æ­£åˆ™åŒ–å¸¸ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- ä¸Dropoutã€Batch Normalizationç­‰å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ç»“åˆä½¿ç”¨ã€‚
- è¶…å‚æ•° $\lambda$ é€šå¸¸é€šè¿‡äº¤å‰éªŒè¯æˆ–è´å¶æ–¯ä¼˜åŒ–ï¼ˆå¦‚å‰è¿°é—®é¢˜ä¸­çš„æ–¹æ³•ï¼‰è°ƒä¼˜ã€‚

---

### ğŸ“– Pythonä»£ç ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨PyTorchå®ç°L2èŒƒæ•°æ­£åˆ™åŒ–çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­åº”ç”¨æƒé‡è¡°å‡ã€‚ç¤ºä¾‹åŸºäºä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç½‘ç»œï¼Œè®­ç»ƒæ‰‹å†™æ•°å­—åˆ†ç±»ä»»åŠ¡ï¼ˆMNISTæ•°æ®é›†ï¼‰ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# æ­¥éª¤1: å®šä¹‰ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥ï¼š28x28åƒç´ 
        self.fc2 = nn.Linear(128, 10)       # è¾“å‡ºï¼š10ç±»
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³è¾“å…¥
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# æ­¥éª¤2: åŠ è½½MNISTæ•°æ®é›†
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

# æ­¥éª¤3: åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼ˆå¸¦L2æ­£åˆ™åŒ–ï¼‰
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
weight_decay = 1e-4  # L2æ­£åˆ™åŒ–ç³»æ•°ï¼ˆå³\lambdaï¼‰
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=weight_decay)

# æ­¥éª¤4: è®­ç»ƒå‡½æ•°
def train(epoch):
    model.train()
    total_loss = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        output = model(data)
        loss = criterion(output, target)
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°å‚æ•°ï¼ˆåŒ…å«L2æ­£åˆ™åŒ–ï¼‰
        total_loss += loss.item()
    print(f'Epoch {epoch}, Average Loss: {total_loss / len(train_loader):.4f}')

# æ­¥éª¤5: æµ‹è¯•å‡½æ•°
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

# æ­¥éª¤6: è¿è¡Œè®­ç»ƒå’Œæµ‹è¯•
epochs = 5
for epoch in range(1, epochs + 1):
    train(epoch)
    test()
```



### ğŸ“– ä»£ç è¯´æ˜

1. **æ¨¡å‹å®šä¹‰**ï¼š
   - `SimpleNet` æ˜¯ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œç”¨äºMNISTæ‰‹å†™æ•°å­—åˆ†ç±»ï¼ˆ28x28åƒç´ è¾“å…¥ï¼Œ10ç±»è¾“å‡ºï¼‰ã€‚
   - åŒ…å«ä¸¤å±‚çº¿æ€§å˜æ¢å’ŒReLUæ¿€æ´»å‡½æ•°ã€‚

2. **æ•°æ®é›†**ï¼š
   - ä½¿ç”¨PyTorchçš„`torchvision`åŠ è½½MNISTæ•°æ®é›†ï¼Œæ‰¹é‡å¤§å°ä¸º64ã€‚
   - æ•°æ®é¢„å¤„ç†ä»…åŒ…æ‹¬è½¬æ¢ä¸ºå¼ é‡ï¼ˆ`ToTensor`ï¼‰ã€‚

3. **L2æ­£åˆ™åŒ–**ï¼š


* åœ¨ä¼˜åŒ–å™¨ä¸­é€šè¿‡ `weight_decay=1e-4` å®ç° L2 æ­£åˆ™åŒ–ã€‚
* PyTorch çš„ `weight_decay` å‚æ•°ä¼šè‡ªåŠ¨åœ¨ä¼˜åŒ–å™¨çš„æ›´æ–°æ­¥éª¤ä¸­æ·»åŠ  L2 èŒƒæ•°æƒ©ç½šï¼Œç­‰æ•ˆäºåœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥ï¼š

$$
\lambda \sum w_i^2
$$

* æ•°å­¦ä¸Šï¼ŒSGD çš„æƒé‡æ›´æ–°å˜ä¸ºï¼š

$$
w \leftarrow w - \eta \left( \frac{\partial \text{Loss}}{\partial w} + \lambda w \right)
$$

å…¶ä¸­ $\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\lambda$ æ˜¯ `weight_decay`ã€‚

4. **è®­ç»ƒä¸æµ‹è¯•**ï¼š
   - è®­ç»ƒ5ä¸ªepochï¼Œæ¯æ¬¡æ‰“å°å¹³å‡æŸå¤±ã€‚
   - æµ‹è¯•æ—¶è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

5. **è¾“å‡ºç¤ºä¾‹**ï¼š
   ```
   Epoch 1, Average Loss: 0.8923
   Test Accuracy: 92.15%
   Epoch 2, Average Loss: 0.4125
   Test Accuracy: 94.30%
   ...
   Epoch 5, Average Loss: 0.2987
   Test Accuracy: 95.80%
   ```
   å®é™…æŸå¤±å’Œå‡†ç¡®ç‡ä¼šå› éšæœºåˆå§‹åŒ–è€Œç•¥æœ‰å˜åŒ–ã€‚




### ğŸ“– å®é™…åº”ç”¨åœºæ™¯
- **æ·±åº¦å­¦ä¹ **ï¼šL2æ­£åˆ™åŒ–å¸¸ç”¨äºCNNã€RNNã€Transformerç­‰æ¨¡å‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **è¶…å‚æ•°é€‰æ‹©**ï¼š\( \lambda \)ï¼ˆå¦‚1e-4ï¼‰éœ€é€šè¿‡äº¤å‰éªŒè¯æˆ–è´å¶æ–¯ä¼˜åŒ–ï¼ˆå¦‚å‰è¿°é—®é¢˜ä¸­ä»‹ç»ï¼‰è°ƒä¼˜ï¼Œå…¸å‹èŒƒå›´ä¸º1e-5åˆ°1e-2ã€‚
- **ä¸å…¶ä»–æ­£åˆ™åŒ–ç»“åˆ**ï¼šL2æ­£åˆ™åŒ–å¸¸ä¸Dropoutã€BatchNormç­‰è”åˆä½¿ç”¨ï¼Œæ•ˆæœæ›´ä½³ã€‚
- **æƒé‡è¡°å‡çš„å±€é™**ï¼šå¯¹æŸäº›ä»»åŠ¡ï¼ˆå¦‚ç¨€ç–æ¨¡å‹ï¼‰ï¼ŒL1æ­£åˆ™åŒ–æˆ–Elastic Netï¼ˆL1+L2ï¼‰å¯èƒ½æ›´é€‚åˆã€‚


