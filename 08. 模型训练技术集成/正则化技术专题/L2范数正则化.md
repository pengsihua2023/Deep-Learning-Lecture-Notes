## L2范数正则化
### 什么是L2范数正则化？

L2范数正则化（也称为权重衰减，Weight Decay）是一种在机器学习和深度学习中常用的正则化技术，用于防止模型过拟合。它的核心思想是通过在损失函数中添加一个惩罚项，限制模型参数（权重）的大小，从而提高模型的泛化能力。

#### 核心原理
- **损失函数修改**：在原始损失函数（如均方误差或交叉熵）上添加L2范数的惩罚项：
 <img width="990" height="433" alt="image" src="https://github.com/user-attachments/assets/f0402296-985c-46c0-b126-27eb7b1a345e" />


$$
\text{Loss}_ {\text{regularized}} = \text{Loss}_{\text{original}} + \lambda \sum_i \|w_i\|_2^2
$$

其中：

* $\text{Loss}_{\text{original}}$ 是原始损失（如分类误差）。
* $w_i$ 是模型参数（如权重）。
* $\|w_i\|_2^2$ 是权重的 L2 范数（即权重平方和）。
* $\lambda$ 是正则化强度的超参数，控制惩罚力度。



- **效果**：
  - L2正则化倾向于让权重变小（但不完全为0），从而使模型更平滑，减少对训练数据的过拟合。
  - 它鼓励模型学习较小的权重，降低模型复杂度，防止对噪声的过度拟合。

- **与L1正则化的区别**：
<img width="1002" height="136" alt="image" src="https://github.com/user-attachments/assets/f7fbba9d-bb4d-4dfc-9e87-f13a25a27819" />


* L1 正则化（Lasso）使用 $\sum |w_i|$，倾向于产生稀疏权重（部分权重为 0）。
* L2 正则化（Ridge）使用 $\sum w_i^2$，倾向于分散权重值，保持所有权重较小。



#### 应用场景
- 深度神经网络（如CNN、RNN、Transformer）中，L2正则化常用于防止过拟合。
- 与Dropout、Batch Normalization等其他正则化方法结合使用。
- 超参数 \( \lambda \) 通常通过交叉验证或贝叶斯优化（如前述问题中的方法）调优。

---

### Python代码示例

以下是一个使用PyTorch实现L2范数正则化的示例，展示如何在神经网络训练中应用权重衰减。示例基于一个简单的全连接网络，训练手写数字分类任务（MNIST数据集）。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 步骤1: 定义简单的全连接神经网络
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # 输入：28x28像素
        self.fc2 = nn.Linear(128, 10)       # 输出：10类
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # 展平输入
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 步骤2: 加载MNIST数据集
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

# 步骤3: 初始化模型、损失函数和优化器（带L2正则化）
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
weight_decay = 1e-4  # L2正则化系数（即\lambda）
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=weight_decay)

# 步骤4: 训练函数
def train(epoch):
    model.train()
    total_loss = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()  # 清空梯度
        output = model(data)
        loss = criterion(output, target)
        loss.backward()  # 反向传播
        optimizer.step()  # 更新参数（包含L2正则化）
        total_loss += loss.item()
    print(f'Epoch {epoch}, Average Loss: {total_loss / len(train_loader):.4f}')

# 步骤5: 测试函数
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

# 步骤6: 运行训练和测试
epochs = 5
for epoch in range(1, epochs + 1):
    train(epoch)
    test()
```

---

### 代码说明

1. **模型定义**：
   - `SimpleNet` 是一个简单的全连接神经网络，用于MNIST手写数字分类（28x28像素输入，10类输出）。
   - 包含两层线性变换和ReLU激活函数。

2. **数据集**：
   - 使用PyTorch的`torchvision`加载MNIST数据集，批量大小为64。
   - 数据预处理仅包括转换为张量（`ToTensor`）。

3. **L2正则化**：
<img width="1317" height="426" alt="image" src="https://github.com/user-attachments/assets/d3459a57-5759-464b-85cb-6b24feb0f6a7" />


4. **训练与测试**：
   - 训练5个epoch，每次打印平均损失。
   - 测试时计算分类准确率，评估模型性能。

5. **输出示例**：
   ```
   Epoch 1, Average Loss: 0.8923
   Test Accuracy: 92.15%
   Epoch 2, Average Loss: 0.4125
   Test Accuracy: 94.30%
   ...
   Epoch 5, Average Loss: 0.2987
   Test Accuracy: 95.80%
   ```
   实际损失和准确率会因随机初始化而略有变化。

---


### 实际应用场景
- **深度学习**：L2正则化常用于CNN、RNN、Transformer等模型，防止过拟合。
- **超参数选择**：\( \lambda \)（如1e-4）需通过交叉验证或贝叶斯优化（如前述问题中介绍）调优，典型范围为1e-5到1e-2。
- **与其他正则化结合**：L2正则化常与Dropout、BatchNorm等联合使用，效果更佳。
- **权重衰减的局限**：对某些任务（如稀疏模型），L1正则化或Elastic Net（L1+L2）可能更适合。

---
