## æ ‡å‡†åŒ–å±‚è¾“å…¥
### ğŸ“– ä»€ä¹ˆæ˜¯æ ‡å‡†åŒ–å±‚è¾“å…¥ï¼ˆLayer Normalizationï¼‰ï¼Ÿ

å±‚æ ‡å‡†åŒ–ï¼ˆLayer Normalizationï¼Œç®€ç§°LNï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­ç”¨äºå½’ä¸€åŒ–ç¥ç»ç½‘ç»œè¾“å…¥çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚ç”¨äºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€Transformerç­‰æ¨¡å‹ã€‚ä¸æ‰¹é‡æ ‡å‡†åŒ–ï¼ˆBatch Normalizationï¼ŒBNï¼‰ä¸åŒï¼ŒLNåœ¨æ¯ä¸€å±‚çš„è¾“å…¥ä¸Šå¯¹**å•ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦**è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯è·¨æ‰¹é‡å½’ä¸€åŒ–ã€‚è¿™ä½¿å¾—LNå¯¹æ‰¹é‡å¤§å°ä¸æ•æ„Ÿï¼Œå°¤å…¶é€‚åˆå°æ‰¹é‡æˆ–åºåˆ—ä»»åŠ¡ã€‚

#### æ ¸å¿ƒåŸç†


å¯¹äºæ¯ä¸ªæ ·æœ¬çš„è¾“å…¥ $x \in \mathbb{R}^d$ï¼ˆ $d$ æ˜¯ç‰¹å¾ç»´åº¦ï¼Œå¦‚éšè—å±‚å¤§å°ï¼‰ï¼ŒLN æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

**1. è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼š**

* å‡å€¼ï¼š $\mu = \frac{1}{d} \sum_{i=1}^d x_i$

* æ–¹å·®ï¼š $\sigma^2 = \frac{1}{d} \sum_{i=1}^d (x_i - \mu)^2$

**2. å½’ä¸€åŒ–ï¼š**

$$
\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

* $\epsilon$ æ˜¯ä¸€ä¸ªå°å¸¸æ•°ï¼Œé˜²æ­¢é™¤é›¶ã€‚

**3. ç¼©æ”¾å’Œå¹³ç§»ï¼š**

$$
y_i = \gamma \hat{x}_i + \beta
$$

* $\gamma$ å’Œ $\beta$ æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œåˆ†åˆ«æ§åˆ¶ç¼©æ”¾å’Œå¹³ç§»ã€‚



- **è®­ç»ƒä¸æµ‹è¯•ä¸€è‡´**ï¼šLNçš„å½’ä¸€åŒ–åŸºäºå•ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦ï¼Œæ— éœ€åƒBNé‚£æ ·åœ¨æµ‹è¯•æ—¶ä½¿ç”¨å…¨å±€ç»Ÿè®¡é‡ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šLNå¯¹åºåˆ—æ¨¡å‹ï¼ˆå¦‚RNNã€Transformerï¼‰æˆ–å°æ‰¹é‡åœºæ™¯è¡¨ç°ä¼˜å¼‚ï¼Œå› ä¸ºå®ƒä¸ä¾èµ–æ‰¹é‡ç»Ÿè®¡é‡ã€‚

#### ä¸BatchNormçš„åŒºåˆ«
- **BN**ï¼šè·¨æ‰¹é‡å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦å½’ä¸€åŒ–ï¼Œä¾èµ–æ‰¹é‡å¤§å°ï¼Œé€‚åˆCNNã€‚
- **LN**ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–ï¼Œä¸ä¾èµ–æ‰¹é‡å¤§å°ï¼Œé€‚åˆRNNå’ŒTransformerã€‚
- **è®¡ç®—ç»´åº¦**ï¼šBNå½’ä¸€åŒ–è½´æ˜¯æ‰¹é‡ç»´åº¦ï¼ŒLNå½’ä¸€åŒ–è½´æ˜¯ç‰¹å¾ç»´åº¦ã€‚

#### ä¼˜åŠ¿
- **æ‰¹é‡å¤§å°æ— å…³**ï¼šé€‚åˆå°æ‰¹é‡æˆ–å•æ ·æœ¬æ¨ç†ï¼ˆå¦‚åœ¨çº¿å­¦ä¹ ï¼‰ã€‚
- **ç¨³å®šæ€§**ï¼šå‡å°‘å†…éƒ¨åå˜é‡åç§»ï¼ŒåŠ é€Ÿè®­ç»ƒï¼Œå…è®¸æ›´é«˜å­¦ä¹ ç‡ã€‚
- **é€‚ç”¨æ€§**ï¼šåœ¨Transformerï¼ˆå¦‚BERTã€GPTï¼‰ä¸­æ˜¯æ ‡å‡†ç»„ä»¶ã€‚

#### å±€é™æ€§
- **è®¡ç®—å¼€é”€**ï¼šå¯¹é«˜ç»´ç‰¹å¾å¯èƒ½ç•¥å¢åŠ è®¡ç®—é‡ã€‚
- **ä¸é€‚åˆæŸäº›ä»»åŠ¡**ï¼šåœ¨å·ç§¯ç½‘ç»œä¸­ï¼ŒBNé€šå¸¸ä¼˜äºLNã€‚

---

### ğŸ“– Pythonä»£ç ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨PyTorchå®ç°Layer Normalizationçš„ç®€å•ç¤ºä¾‹ï¼ŒåŸºäºMNISTæ‰‹å†™æ•°å­—åˆ†ç±»ä»»åŠ¡ã€‚ä»£ç åœ¨å…¨è¿æ¥ç¥ç»ç½‘ç»œä¸­æ·»åŠ LNå±‚ï¼Œå¹¶ç»“åˆAdamä¼˜åŒ–å™¨å’Œæ—©åœï¼ˆå‚è€ƒå‰è¿°é—®é¢˜ï¼‰ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# æ­¥éª¤1: å®šä¹‰å¸¦LayerNormçš„ç¥ç»ç½‘ç»œ
class LayerNormNet(nn.Module):
    def __init__(self):
        super(LayerNormNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥ï¼š28x28åƒç´ 
        self.ln1 = nn.LayerNorm(128)        # LayerNormå±‚ï¼Œå½’ä¸€åŒ–128ç»´ç‰¹å¾
        self.fc2 = nn.Linear(128, 10)       # è¾“å‡ºï¼š10ç±»
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³è¾“å…¥
        x = self.fc1(x)
        x = self.ln1(x)          # åº”ç”¨LayerNorm
        x = self.relu(x)
        x = self.fc2(x)
        return x

# æ­¥éª¤2: åŠ è½½MNISTæ•°æ®é›†å¹¶æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›†
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=64)
test_loader = DataLoader(test_dataset, batch_size=64)

# æ­¥éª¤3: åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = LayerNormNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# æ­¥éª¤4: æ—©åœç±»ï¼ˆå¤ç”¨å‰è¿°é€»è¾‘ï¼‰
class EarlyStopping:
    def __init__(self, patience=3, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_loss = float('inf')
        self.counter = 0
        self.best_model_state = None
        self.early_stop = False
    
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.best_model_state = model.state_dict()
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

# æ­¥éª¤5: è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train(epoch):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)

def validate():
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, target in val_loader:
            output = model(data)
            val_loss += criterion(output, target).item()
    return val_loss / len(val_loader)

# æ­¥éª¤6: æµ‹è¯•å‡½æ•°
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    return 100. * correct / total

# æ­¥éª¤7: è®­ç»ƒå¾ªç¯ä¸æ—©åœ
early_stopping = EarlyStopping(patience=3, delta=0.001)
epochs = 20
for epoch in range(1, epochs + 1):
    train_loss = train(epoch)
    val_loss = validate()
    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
    
    early_stopping(val_loss, model)
    if early_stopping.early_stop:
        print("Early stopping triggered!")
        break

# æ¢å¤æœ€ä½³æ¨¡å‹
if early_stopping.best_model_state:
    model.load_state_dict(early_stopping.best_model_state)
    print("Restored best model from early stopping.")

# æ­¥éª¤8: æµ‹è¯•æœ€ä½³æ¨¡å‹
test_accuracy = test()
print(f'Test Accuracy: {test_accuracy:.2f}%')
```

---

### ğŸ“– ä»£ç è¯´æ˜

1. **æ¨¡å‹å®šä¹‰**ï¼š
   - `LayerNormNet` æ˜¯ä¸€ä¸ªå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œè¾“å…¥ä¸ºMNISTçš„28x28åƒç´ å›¾åƒï¼Œè¾“å‡ºä¸º10ç±»åˆ†ç±»ã€‚
   - åœ¨ç¬¬ä¸€å±‚å…¨è¿æ¥ï¼ˆ`fc1`ï¼‰åæ·»åŠ `nn.LayerNorm(128)`ï¼Œå¯¹128ç»´ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ã€‚

2. **æ•°æ®é›†**ï¼š
   - ä½¿ç”¨`torchvision`åŠ è½½MNISTæ•°æ®é›†ï¼Œæ‹†åˆ†ä¸º80%è®­ç»ƒ+20%éªŒè¯ã€‚
   - æ‰¹é‡å¤§å°ä¸º64ï¼Œæ•°æ®é¢„å¤„ç†ä»…åŒ…æ‹¬è½¬æ¢ä¸ºå¼ é‡ã€‚

3. **LayerNormå±‚**ï¼š
   - `nn.LayerNorm(128)`ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„128ç»´ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ã€‚
   - è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µè¡Œä¸ºä¸€è‡´ï¼Œæ— éœ€åƒBNé‚£æ ·åˆ‡æ¢ç»Ÿè®¡é‡ï¼ˆå› LNä¸ä¾èµ–æ‰¹é‡ï¼‰ã€‚

4. **è®­ç»ƒä¸éªŒè¯**ï¼š
   - ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼ˆå‚è€ƒå‰è¿°é—®é¢˜ï¼‰è¿›è¡Œè®­ç»ƒã€‚
   - ç»“åˆæ—©åœï¼ˆ`EarlyStopping`ç±»ï¼‰ç›‘æ§éªŒè¯æŸå¤±ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹ã€‚
   - `model.train()`å’Œ`model.eval()`å¯¹LNæ— ç‰¹æ®Šå½±å“ï¼ˆå› LNä¸ä¾èµ–æ‰¹é‡ç»Ÿè®¡é‡ï¼‰ã€‚

5. **è¾“å‡ºç¤ºä¾‹**ï¼š
   ```
   Epoch 1, Train Loss: 0.3214, Val Loss: 0.1789
   Epoch 2, Train Loss: 0.1345, Val Loss: 0.1256
   Epoch 3, Train Loss: 0.1012, Val Loss: 0.1087
   Epoch 4, Train Loss: 0.0823, Val Loss: 0.1092
   Epoch 5, Train Loss: 0.0678, Val Loss: 0.1101
   Early stopping triggered!
   Restored best model from early stopping.
   Test Accuracy: 97.50%
   ```
   å®é™…å€¼å› éšæœºåˆå§‹åŒ–è€Œå¼‚ã€‚

---

### ğŸ“– å…³é”®ç‚¹
- **LNä½ç½®**ï¼šé€šå¸¸æ”¾åœ¨çº¿æ€§å±‚æˆ–å·ç§¯å±‚åï¼Œæ¿€æ´»å‡½æ•°å‰ã€‚
- **æ‰¹é‡æ— å…³**ï¼šLNå¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å½’ä¸€åŒ–ï¼Œé€‚åˆå°æ‰¹é‡æˆ–å•æ ·æœ¬æ¨ç†ã€‚
- **å¯å­¦ä¹ å‚æ•°**ï¼š`nn.LayerNorm`è‡ªåŠ¨ç»´æŠ¤`gamma`å’Œ`beta`ï¼Œé€šè¿‡ä¼˜åŒ–å™¨å­¦ä¹ ã€‚
- **ä¸BNå¯¹æ¯”**ï¼š
  - LNå½’ä¸€åŒ–ç‰¹å¾ç»´åº¦ï¼Œé€‚åˆRNN/Transformerã€‚
  - BNå½’ä¸€åŒ–æ‰¹é‡ç»´åº¦ï¼Œé€‚åˆCNNã€‚

---

### ğŸ“– å®é™…åº”ç”¨åœºæ™¯
- **Transformeræ¨¡å‹**ï¼šLNæ˜¯Transformerï¼ˆå¦‚BERTã€GPTï¼‰çš„æ ‡å‡†ç»„ä»¶ï¼Œé€šå¸¸ç”¨äºå¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰å’Œå‰é¦ˆç½‘ç»œï¼ˆFeed-Forwardï¼‰åã€‚
- **åºåˆ—ä»»åŠ¡**ï¼šåœ¨RNNã€LSTMç­‰æ¨¡å‹ä¸­ï¼ŒLNæ¯”BNæ›´ç¨³å®šã€‚
- **å°æ‰¹é‡åœºæ™¯**ï¼šLNé€‚åˆåœ¨çº¿å­¦ä¹ æˆ–æ‰¹é‡å¤§å°ä¸º1çš„æƒ…å†µã€‚

### ğŸ“– æ³¨æ„äº‹é¡¹
- **ç‰¹å¾ç»´åº¦**ï¼š`nn.LayerNorm`éœ€æŒ‡å®šå½’ä¸€åŒ–çš„ç»´åº¦ï¼ˆå¦‚128ï¼‰ï¼Œç¡®ä¿ä¸è¾“å…¥åŒ¹é…ã€‚
- **è®¡ç®—å¼€é”€**ï¼šLNå¯¹é«˜ç»´ç‰¹å¾çš„è®¡ç®—ç•¥é«˜äºBNï¼Œä½†é€šå¸¸å½±å“ä¸å¤§ã€‚
- **ä¸å…¶ä»–æ­£åˆ™åŒ–ç»“åˆ**ï¼šå¯ä¸Dropoutã€L1/L2æ­£åˆ™åŒ–ï¼ˆå¦‚å‰è¿°é—®é¢˜ï¼‰è”åˆä½¿ç”¨ã€‚
