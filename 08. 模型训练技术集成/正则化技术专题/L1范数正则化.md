## L1范数正则化
### 什么是L1范数正则化？

L1范数正则化（也称为Lasso正则化）是一种在机器学习和深度学习中常用的正则化技术，用于防止模型过拟合并促进参数稀疏性。与L2范数正则化（Ridge正则化）不同，L1正则化通过在损失函数中添加模型参数的L1范数（即权重的绝对值之和）作为惩罚项，倾向于将部分权重推向零，从而生成稀疏模型。

#### 核心原理
- **损失函数修改**：L1正则化在原始损失函数上添加L1范数惩罚项：


$$
\text{Loss}_ {\text{regularized}} = \text{Loss}_{\text{original}} + \lambda \sum_i |w_i|
$$

其中：

* $\text{Loss}_{\text{original}}$ 是原始损失（如均方误差或交叉熵）。
* $w_i$ 是模型参数（如权重）。
* $|w_i|$ 是权重的绝对值， $\sum |w_i|$ 是 L1 范数。
* $\lambda$ 是正则化强度的超参数，控制惩罚力度。




- **效果**：
  - L1正则化倾向于将不重要的权重设为零，生成稀疏模型（即特征选择效果）。
  - 稀疏性有助于减少模型复杂度、提高可解释性，并降低过拟合风险。
  - 相比L2正则化（使权重变小但非零），L1正则化更适合需要稀疏解的场景。

- **与L2正则化的区别**：
<img width="1094" height="197" alt="image" src="https://github.com/user-attachments/assets/341bcfd7-4e11-40ab-890b-35ea6fa21d35" />  


* L2 正则化 $\left(\sum w_i^2\right)$ 使权重趋向于小值，保持所有权重非零。
* L1 正则化 $\left(\sum |w_i|\right)$ 因其不可微的特性（在零点处），能将部分权重精确推向零。
* L1 正则化在高维数据（如特征选择）中更有效，而 L2 正则化更适合平滑权重分布。




#### 应用场景
- **特征选择**：在机器学习（如线性回归、逻辑回归）中，L1正则化可自动选择重要特征。
- **神经网络**：在深度学习中，L1正则化可用于稀疏化网络（如卷积层或全连接层）。
- **压缩模型**：稀疏模型更容易进行剪枝或量化，适用于资源受限的设备。

---

### Python代码示例

以下是一个使用PyTorch实现L1范数正则化的示例，基于MNIST手写数字分类任务。L1正则化通常需要手动添加到损失函数中，因为PyTorch的优化器（如SGD、Adam）内置支持`weight_decay`（对应L2正则化），但不支持直接的L1正则化。
```
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 步骤1: 定义简单的全连接神经网络
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # 输入：28x28像素
        self.fc2 = nn.Linear(128, 10)       # 输出：10类
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # 展平输入
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 步骤2: 加载MNIST数据集
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

# 步骤3: 初始化模型、损失函数和优化器
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
l1_lambda = 1e-4  # L1正则化系数

# 步骤4: 训练函数（手动添加L1正则化）
def train(epoch):
    model.train()
    total_loss = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()  # 清空梯度
        output = model(data)
        # 计算原始损失
        loss = criterion(output, target)
        # 添加L1正则化项
        l1_norm = sum(torch.abs(p).sum() for p in model.parameters())
        loss = loss + l1_lambda * l1_norm
        # 反向传播和优化
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f'Epoch {epoch}, Average Loss: {total_loss / len(train_loader):.4f}')

# 步骤5: 测试函数
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

# 步骤6: 运行训练和测试
epochs = 5
for epoch in range(1, epochs + 1):
    train(epoch)
    test()
```

### 代码说明

1. **模型定义**：
   - `SimpleNet` 是一个简单的全连接神经网络，输入为MNIST的28x28像素图像，输出为10类分类。

2. **数据集**：
   - 使用`torchvision`加载MNIST数据集，批量大小为64，数据预处理仅包括转换为张量。

3. **L1正则化实现**：
   - PyTorch优化器不支持直接的L1正则化（不像L2的`weight_decay`），因此手动计算L1范数。
   - 通过`sum(torch.abs(p).sum() for p in model.parameters())`计算所有参数的L1范数（绝对值和）。
   - 将L1范数乘以系数`l1_lambda`（如1e-4）加到损失函数中。

4. **训练与测试**：
   - 训练时，损失包括原始交叉熵损失和L1正则化项。
   - 每个epoch打印平均损失，测试时计算分类准确率。
   - L1正则化会推动部分权重趋向于零，可通过检查`model.parameters()`验证稀疏性。

5. **输出示例**：
   ```
   Epoch 1, Average Loss: 0.9123
   Test Accuracy: 91.50%
   Epoch 2, Average Loss: 0.4321
   Test Accuracy: 93.80%
   ...
   Epoch 5, Average Loss: 0.3124
   Test Accuracy: 95.20%
   ```
   实际值因随机初始化而异。

---

### 检查稀疏性
为了验证L1正则化的稀疏效果，可以在训练后检查模型权重：

```python
# 检查权重的稀疏性
def check_sparsity(model):
    total_params = 0
    zero_params = 0
    for param in model.parameters():
        total_params += param.numel()
        zero_params += (param == 0).sum().item()
    sparsity = 100. * zero_params / total_params
    print(f'模型稀疏性: {sparsity:.2f}% (零权重占比)')

# 在训练后调用
check_sparsity(model)
```

L1正则化会增加零权重的比例，尤其当`l1_lambda`较大时。

---

### L1与L2正则化的对比
- **L1正则化**：
  - 倾向于产生稀疏权重（部分权重为0）。
  - 适合特征选择或需要压缩的场景。
  - 梯度不连续（在零点不可微），优化可能更复杂。
- **L2正则化**：
  - 使权重变小但非零，保持平滑。
  - 适合需要稳定梯度更新的场景。
  - PyTorch通过`weight_decay`直接支持。

#### 结合L1和L2（Elastic Net）
可以同时使用L1和L2正则化，形成Elastic Net正则化：

```python
l1_lambda = 1e-4
l2_lambda = 1e-4
loss = criterion(output, target)
l1_norm = sum(torch.abs(p).sum() for p in model.parameters())
l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm
```

---

### 实际应用场景
- **特征选择**：在传统机器学习中（如逻辑回归），L1正则化用于选择重要特征。
- **神经网络压缩**：L1正则化可生成稀疏网络，便于剪枝或部署到资源受限设备。
- **高维数据**：在输入特征较多时，L1正则化能有效减少不相关权重。
- **超参数调优**：`l1_lambda`需通过交叉验证或贝叶斯优化（如前述问题）调整，典型范围为1e-5到1e-3。

---
