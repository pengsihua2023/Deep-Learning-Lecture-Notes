## è®­ç»ƒæ—©åœ
### ğŸ“– ä»€ä¹ˆæ˜¯è®­ç»ƒæ—©åœï¼ˆEarly Stoppingï¼‰ï¼Ÿ

è®­ç»ƒæ—©åœï¼ˆEarly Stoppingï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­å¸¸ç”¨çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç”¨äºé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆå¹¶èŠ‚çœè®¡ç®—èµ„æºã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç›‘æ§éªŒè¯é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚éªŒè¯æŸå¤±æˆ–å‡†ç¡®ç‡ï¼‰ï¼Œå½“æ€§èƒ½åœ¨ä¸€å®šè½®æ•°ï¼ˆç§°ä¸ºâ€œè€å¿ƒå€¼â€ï¼‰å†…ä¸å†æ”¹å–„æ—¶ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒï¼Œå¹¶é€šå¸¸ä¿å­˜éªŒè¯æ€§èƒ½æœ€å¥½çš„æ¨¡å‹å‚æ•°ã€‚

#### æ ¸å¿ƒåŸç†
- **ç›‘æ§æŒ‡æ ‡**ï¼šåœ¨æ¯ä¸ªè®­ç»ƒè½®æ¬¡ï¼ˆepochï¼‰åï¼Œè®¡ç®—éªŒè¯é›†ä¸Šçš„æŸå¤±æˆ–å‡†ç¡®ç‡ã€‚
- **æ—©åœæ¡ä»¶**ï¼šå¦‚æœéªŒè¯æ€§èƒ½è¿ç»­è‹¥å¹²è½®ï¼ˆpatienceï¼‰æœªæ”¹å–„ï¼ˆä¾‹å¦‚éªŒè¯æŸå¤±ä¸å†ä¸‹é™ï¼‰ï¼Œåˆ™åœæ­¢è®­ç»ƒã€‚
- **ä¿å­˜æœ€ä½³æ¨¡å‹**ï¼šè®°å½•éªŒè¯æ€§èƒ½æœ€ä½³çš„æ¨¡å‹æƒé‡ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒç»“æŸæ—¶æ¢å¤ã€‚

#### ä¼˜åŠ¿
- **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šé¿å…æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿‡åº¦ä¼˜åŒ–ï¼Œå¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸‹é™ã€‚
- **èŠ‚çœæ—¶é—´**ï¼šå‡å°‘ä¸å¿…è¦çš„è®­ç»ƒè½®æ¬¡ï¼Œæé«˜æ•ˆç‡ã€‚
- **ç®€å•æœ‰æ•ˆ**ï¼šæ— éœ€å¤æ‚è°ƒå‚ï¼Œæ˜“äºå®ç°ã€‚

#### å±€é™æ€§
- **è€å¿ƒå€¼é€‰æ‹©**ï¼šè¿‡å°çš„è€å¿ƒå€¼å¯èƒ½å¯¼è‡´è¿‡æ—©åœæ­¢ï¼Œè¿‡å¤§åˆ™å¯èƒ½æµªè´¹è®¡ç®—èµ„æºã€‚
- **éªŒè¯é›†ä¾èµ–**ï¼šéœ€è¦å¯é çš„éªŒè¯é›†ï¼Œå¦åˆ™å¯èƒ½è¯¯åˆ¤åœæ­¢æ—¶æœºã€‚

---

### ğŸ“– Pythonä»£ç ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨PyTorchå®ç°æ—©åœçš„ç®€å•ç¤ºä¾‹ï¼ŒåŸºäºMNISTæ‰‹å†™æ•°å­—åˆ†ç±»ä»»åŠ¡ã€‚ä»£ç å±•ç¤ºå¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§éªŒè¯æŸå¤±ï¼Œå¹¶åœ¨æ€§èƒ½ä¸å†æ”¹å–„æ—¶åœæ­¢è®­ç»ƒã€‚

```
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# æ­¥éª¤1: å®šä¹‰ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥ï¼š28x28åƒç´ 
        self.fc2 = nn.Linear(128, 10)       # è¾“å‡ºï¼š10ç±»
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³è¾“å…¥
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# æ­¥éª¤2: åŠ è½½MNISTæ•°æ®é›†å¹¶æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›†
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)

# æ‹†åˆ†è®­ç»ƒé›†ä¸ºè®­ç»ƒ+éªŒè¯ï¼ˆ80%è®­ç»ƒï¼Œ20%éªŒè¯ï¼‰
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=64)
test_loader = DataLoader(test_dataset, batch_size=64)

# æ­¥éª¤3: åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# æ­¥éª¤4: æ—©åœç±»
class EarlyStopping:
    def __init__(self, patience=3, delta=0):
        self.patience = patience  # è€å¿ƒå€¼ï¼šè¿ç»­å¤šå°‘è½®æ— æ”¹è¿›
        self.delta = delta        # æœ€å°æ”¹è¿›é˜ˆå€¼
        self.best_loss = float('inf')  # æœ€ä½³éªŒè¯æŸå¤±
        self.counter = 0          # æ— æ”¹è¿›è½®æ•°è®¡æ•°å™¨
        self.best_model_state = None  # æœ€ä½³æ¨¡å‹å‚æ•°
        self.early_stop = False   # æ˜¯å¦åœæ­¢
    
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.delta:
            # éªŒè¯æŸå¤±æ”¹å–„ï¼Œæ›´æ–°æœ€ä½³æŸå¤±å’Œæ¨¡å‹
            self.best_loss = val_loss
            self.best_model_state = model.state_dict()
            self.counter = 0
        else:
            # æ— æ”¹è¿›ï¼Œè®¡æ•°å™¨åŠ 1
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

# æ­¥éª¤5: è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train(epoch):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)

def validate():
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, target in val_loader:
            output = model(data)
            val_loss += criterion(output, target).item()
    return val_loss / len(val_loader)

# æ­¥éª¤6: æµ‹è¯•å‡½æ•°
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    return 100. * correct / total

# æ­¥éª¤7: è®­ç»ƒå¾ªç¯ä¸æ—©åœ
early_stopping = EarlyStopping(patience=3, delta=0.001)
epochs = 20
for epoch in range(1, epochs + 1):
    train_loss = train(epoch)
    val_loss = validate()
    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')
    
    # æ£€æŸ¥æ—©åœ
    early_stopping(val_loss, model)
    if early_stopping.early_stop:
        print("Early stopping triggered!")
        break

# æ¢å¤æœ€ä½³æ¨¡å‹
if early_stopping.best_model_state:
    model.load_state_dict(early_stopping.best_model_state)
    print("Restored best model from early stopping.")

# æ­¥éª¤8: æµ‹è¯•æœ€ä½³æ¨¡å‹
test_accuracy = test()
print(f'Test Accuracy: {test_accuracy:.2f}%')
```

---

### ğŸ“– ä»£ç è¯´æ˜

1. **æ¨¡å‹å®šä¹‰**ï¼š
   - `SimpleNet` æ˜¯ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œè¾“å…¥ä¸ºMNISTçš„28x28åƒç´ å›¾åƒï¼Œè¾“å‡ºä¸º10ç±»åˆ†ç±»ã€‚

2. **æ•°æ®é›†**ï¼š
   - ä½¿ç”¨`torchvision`åŠ è½½MNISTæ•°æ®é›†ï¼Œå°†è®­ç»ƒé›†æ‹†åˆ†ä¸º80%è®­ç»ƒ+20%éªŒè¯ã€‚
   - æ‰¹é‡å¤§å°ä¸º64ï¼Œæ•°æ®é¢„å¤„ç†ä»…åŒ…æ‹¬è½¬æ¢ä¸ºå¼ é‡ã€‚

3. **æ—©åœç±»**ï¼š
   - `EarlyStopping`ç±»è·Ÿè¸ªéªŒè¯æŸå¤±ï¼š
     - `patience=3`ï¼šè¿ç»­3è½®éªŒè¯æŸå¤±æ— æ”¹è¿›åˆ™åœæ­¢ã€‚
     - `delta=0.001`ï¼šæŸå¤±éœ€è‡³å°‘æ”¹å–„0.001æ‰ç®—æœ‰æ•ˆã€‚
     - ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼ˆ`best_model_state`ï¼‰ä»¥ä¾¿æ¢å¤ã€‚

4. **è®­ç»ƒä¸éªŒè¯**ï¼š
   - `train`å‡½æ•°è®¡ç®—è®­ç»ƒæŸå¤±ï¼Œ`validate`å‡½æ•°è®¡ç®—éªŒè¯æŸå¤±ã€‚
   - æ¯ä¸ªepochåè°ƒç”¨`early_stopping`ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢ã€‚
   - å¦‚æœè§¦å‘æ—©åœï¼Œæ¢å¤æœ€ä½³æ¨¡å‹æƒé‡ã€‚

5. **æµ‹è¯•**ï¼š
   - ä½¿ç”¨æ¢å¤çš„æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡ã€‚

6. **è¾“å‡ºç¤ºä¾‹**ï¼š
   ```
   Epoch 1, Train Loss: 0.4123, Val Loss: 0.1987
   Epoch 2, Train Loss: 0.1854, Val Loss: 0.1432
   Epoch 3, Train Loss: 0.1321, Val Loss: 0.1205
   Epoch 4, Train Loss: 0.0987, Val Loss: 0.1210
   Epoch 5, Train Loss: 0.0765, Val Loss: 0.1223
   Early stopping triggered!
   Restored best model from early stopping.
   Test Accuracy: 96.50%
   ```
   å®é™…å€¼å› éšæœºåˆå§‹åŒ–è€Œå¼‚ã€‚

---

### ğŸ“– å…³é”®ç‚¹
- **éªŒè¯é›†**ï¼šæ—©åœä¾èµ–éªŒè¯é›†æ€§èƒ½ï¼Œéœ€ç¡®ä¿éªŒè¯é›†ä»£è¡¨æ€§å¼ºã€‚
- **è€å¿ƒå€¼**ï¼š`patience=3`è¡¨ç¤ºå…è®¸3è½®æ— æ”¹è¿›ï¼Œå€¼è¶Šå¤§è¶Šä¿å®ˆã€‚
- **æœ€ä½³æ¨¡å‹æ¢å¤**ï¼šé€šè¿‡`model.load_state_dict`æ¢å¤éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹ã€‚
- **æŒ‡æ ‡é€‰æ‹©**ï¼šç¤ºä¾‹ä¸­ç”¨éªŒè¯æŸå¤±ï¼Œä¹Ÿå¯ç”¨éªŒè¯å‡†ç¡®ç‡ï¼ˆéœ€æ”¹é€»è¾‘ä¸ºæœ€å¤§åŒ–ï¼‰ã€‚

---

### ğŸ“– å®é™…åº”ç”¨åœºæ™¯
- **æ·±åº¦å­¦ä¹ **ï¼šæ—©åœå¹¿æ³›ç”¨äºCNNã€RNNã€Transformerç­‰æ¨¡å‹çš„è®­ç»ƒã€‚
- **èµ„æºä¼˜åŒ–**ï¼šåœ¨å¤§å‹æ¨¡å‹ï¼ˆå¦‚BERTï¼‰è®­ç»ƒä¸­èŠ‚çœæ—¶é—´å’Œè®¡ç®—èµ„æºã€‚
- **ä¸å…¶ä»–æ­£åˆ™åŒ–ç»“åˆ**ï¼šå¯ä¸Dropoutï¼ˆå¦‚å‰è¿°é—®é¢˜ï¼‰ã€L1/L2æ­£åˆ™åŒ–ã€æ¢¯åº¦è£å‰ªç­‰è”åˆä½¿ç”¨ã€‚

### ğŸ“– æ³¨æ„äº‹é¡¹
- **éªŒè¯é›†åˆ’åˆ†**ï¼šéœ€åˆç†åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼Œé¿å…æ•°æ®æ³„æ¼ã€‚
- **è€å¿ƒå€¼è°ƒä¼˜**ï¼šè¿‡å°å¯èƒ½æ¬ æ‹Ÿåˆï¼Œè¿‡å¤§å¯èƒ½è¿‡æ‹Ÿåˆã€‚
- **æŒ‡æ ‡é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©æŸå¤±æˆ–å‡†ç¡®ç‡ï¼Œæ³¨æ„æ–¹å‘ï¼ˆæœ€å°åŒ–æŸå¤±æˆ–æœ€å¤§åŒ–å‡†ç¡®ç‡ï¼‰ã€‚
