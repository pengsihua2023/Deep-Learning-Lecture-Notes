## åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
### ğŸ“– ä»€ä¹ˆæ˜¯åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLearning Rate Schedulingï¼‰ï¼Ÿ

åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLearning Rate Schedulingï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä¸€ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®ä¸€å®šç­–ç•¥åŠ¨æ€æ”¹å˜å­¦ä¹ ç‡ï¼Œä»¥åŠ é€Ÿæ”¶æ•›ã€é¿å…éœ‡è¡å¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚å­¦ä¹ ç‡å†³å®šäº†å‚æ•°æ›´æ–°çš„æ­¥é•¿ï¼Œåˆé€‚çš„è°ƒåº¦ç­–ç•¥å¯ä»¥åœ¨è®­ç»ƒåˆæœŸå¿«é€Ÿä¸‹é™æŸå¤±ï¼ŒåæœŸç²¾ç»†è°ƒæ•´ä»¥æ¥è¿‘æœ€ä¼˜è§£ã€‚

### ğŸ“– æ ¸å¿ƒåŸç†
- **åˆå§‹å­¦ä¹ ç‡**ï¼šé€šå¸¸è¾ƒé«˜ï¼ˆå¦‚0.01æˆ–0.001ï¼‰ï¼Œä»¥å¿«é€Ÿä¼˜åŒ–ã€‚
- **è°ƒæ•´ç­–ç•¥**ï¼šæ ¹æ®è®­ç»ƒè½®æ•°ï¼ˆepochï¼‰ã€æŸå¤±æˆ–æ€§èƒ½æŒ‡æ ‡è°ƒæ•´å­¦ä¹ ç‡ã€‚
- **å¸¸è§è°ƒåº¦æ–¹æ³•**ï¼š
  - **æ—¶é—´è¡°å‡ï¼ˆStep Decayï¼‰**ï¼šæ¯éš”å›ºå®šè½®æ•°é™ä½å­¦ä¹ ç‡ï¼ˆå¦‚æ¯5ä¸ªepochä¹˜ä»¥0.1ï¼‰ã€‚
  - **æŒ‡æ•°è¡°å‡ï¼ˆExponential Decayï¼‰**ï¼šå­¦ä¹ ç‡æŒ‰æŒ‡æ•°é€æ¸å‡å°ã€‚
  - **ä½™å¼¦é€€ç«ï¼ˆCosine Annealingï¼‰**ï¼šå­¦ä¹ ç‡æŒ‰ä½™å¼¦å‡½æ•°å¹³æ»‘å˜åŒ–ã€‚
  - **è‡ªé€‚åº”è°ƒæ•´**ï¼šå¦‚åŸºäºéªŒè¯æŸå¤±çš„ReduceLROnPlateauã€‚

### ğŸ“– ä¼˜åŠ¿
- **åŠ é€Ÿæ”¶æ•›**ï¼šåˆæœŸé«˜å­¦ä¹ ç‡å¿«é€Ÿä¸‹é™ï¼Œæœ«æœŸä½å­¦ä¹ ç‡ç²¾ç»†è°ƒæ•´ã€‚
- **é˜²æ­¢éœ‡è¡**ï¼šé¿å…å­¦ä¹ ç‡è¿‡é«˜å¯¼è‡´æŸå¤±å‡½æ•°åœ¨æœ€ä¼˜è§£é™„è¿‘éœ‡è¡ã€‚
- **æé«˜æ€§èƒ½**ï¼šåŠ¨æ€è°ƒæ•´å¯å¸®åŠ©æ¨¡å‹æ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚

### ğŸ“– å±€é™æ€§
- **è¶…å‚æ•°é€‰æ‹©**ï¼šè°ƒåº¦ç­–ç•¥ï¼ˆå¦‚æ­¥é•¿ã€è¡°å‡ç‡ï¼‰éœ€è°ƒä¼˜ã€‚
- **ä»»åŠ¡ä¾èµ–**ï¼šä¸åŒä»»åŠ¡å¯èƒ½éœ€è¦ä¸åŒè°ƒåº¦æ–¹æ³•ã€‚

---

### ğŸ“– Pythonä»£ç ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„PyTorchç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨MNISTæ‰‹å†™æ•°å­—åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨**StepLR**ï¼ˆæ—¶é—´è¡°å‡ï¼‰è°ƒåº¦å™¨åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ã€‚ä»£ç ä¿æŒæç®€ï¼Œç»“åˆAdamä¼˜åŒ–å™¨ã€‚

```
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR

# æ­¥éª¤1: å®šä¹‰ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc = nn.Linear(28 * 28, 10)  # è¾“å…¥ï¼š28x28åƒç´ ï¼Œè¾“å‡ºï¼š10ç±»
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³è¾“å…¥
        x = self.fc(x)
        return x

# æ­¥éª¤2: åŠ è½½MNISTæ•°æ®é›†
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

# æ­¥éª¤3: åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # æ¯2ä¸ªepochå­¦ä¹ ç‡ä¹˜ä»¥0.1

# æ­¥éª¤4: è®­ç»ƒå‡½æ•°
def train(epoch):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f'Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}, LR: {optimizer.param_groups[0]["lr"]:.6f}')

# æ­¥éª¤5: æµ‹è¯•å‡½æ•°
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    accuracy = 100. * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

# æ­¥éª¤6: è®­ç»ƒå¾ªç¯
epochs = 5
for epoch in range(1, epochs + 1):
    train(epoch)
    test()
    scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
```

---

### ğŸ“– ä»£ç è¯´æ˜

1. **æ¨¡å‹å®šä¹‰**ï¼š
   - `SimpleNet` æ˜¯ä¸€ä¸ªæç®€çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œè¾“å…¥ä¸ºMNISTçš„28x28åƒç´ å›¾åƒï¼Œè¾“å‡ºä¸º10ç±»åˆ†ç±»ã€‚

2. **æ•°æ®é›†**ï¼š
   - ä½¿ç”¨`torchvision`åŠ è½½MNISTæ•°æ®é›†ï¼Œæ‰¹é‡å¤§å°ä¸º64ï¼Œæ•°æ®é¢„å¤„ç†ä»…åŒ…æ‹¬è½¬æ¢ä¸ºå¼ é‡ã€‚

3. **å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼š
   - ä½¿ç”¨`StepLR`è°ƒåº¦å™¨ï¼š`step_size=2`ï¼ˆæ¯2ä¸ªepochè°ƒæ•´ä¸€æ¬¡ï¼‰ï¼Œ`gamma=0.1`ï¼ˆå­¦ä¹ ç‡ä¹˜ä»¥0.1ï¼‰ã€‚
   - åˆå§‹å­¦ä¹ ç‡`lr=0.001`ï¼Œç¬¬3ä¸ªepoché™ä¸º0.0001ï¼Œç¬¬5ä¸ªepoché™ä¸º0.00001ã€‚

4. **è®­ç»ƒä¸æµ‹è¯•**ï¼š
   - è®­ç»ƒæ—¶æ‰“å°æŸå¤±å’Œå½“å‰å­¦ä¹ ç‡ï¼ˆ`optimizer.param_groups[0]["lr"]`ï¼‰ã€‚
   - æ¯ä¸ªepochæœ«è°ƒç”¨`scheduler.step()`æ›´æ–°å­¦ä¹ ç‡ã€‚
   - æµ‹è¯•æ—¶è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ã€‚

5. **è¾“å‡ºç¤ºä¾‹**ï¼š
   ```
   Epoch 1, Loss: 0.4321, LR: 0.001000
   Test Accuracy: 92.50%
   Epoch 2, Loss: 0.2876, LR: 0.001000
   Test Accuracy: 93.80%
   Epoch 3, Loss: 0.2345, LR: 0.000100
   Test Accuracy: 94.20%
   Epoch 4, Loss: 0.2234, LR: 0.000100
   Test Accuracy: 94.30%
   Epoch 5, Loss: 0.2109, LR: 0.000010
   Test Accuracy: 94.50%
   ```
   å®é™…å€¼å› éšæœºåˆå§‹åŒ–è€Œå¼‚ã€‚

---

### ğŸ“– å…³é”®ç‚¹
- **è°ƒåº¦å™¨è°ƒç”¨**ï¼š`scheduler.step()`é€šå¸¸åœ¨æ¯ä¸ªepochæœ«è°ƒç”¨ï¼Œæ›´æ–°å­¦ä¹ ç‡ã€‚
- **StepLR**ï¼šç®€å•æœ‰æ•ˆï¼Œæ¯éš”å›ºå®šè½®æ•°é™ä½å­¦ä¹ ç‡ï¼Œé€‚åˆå¿«é€Ÿå®éªŒã€‚
- **å­¦ä¹ ç‡å˜åŒ–**ï¼šè¾“å‡ºæ˜¾ç¤ºå­¦ä¹ ç‡ä»0.001é™åˆ°0.00001ï¼ŒæŸå¤±é€æ¸ç¨³å®šã€‚

---

### ğŸ“– å®é™…åº”ç”¨åœºæ™¯
- **æ·±åº¦å­¦ä¹ **ï¼šå­¦ä¹ ç‡è°ƒåº¦å¹¿æ³›ç”¨äºCNNã€RNNã€Transformerï¼ˆå¦‚BERTï¼‰ã€‚
- **å¤æ‚æ¨¡å‹**ï¼šç»“åˆAdamä¼˜åŒ–å™¨ï¼ˆå¦‚å‰è¿°é—®é¢˜ï¼‰ï¼Œè°ƒåº¦å™¨å¯æé«˜æ”¶æ•›ç¨³å®šæ€§ã€‚
- **ä¸å…¶ä»–æ­£åˆ™åŒ–ç»“åˆ**ï¼šå¯ä¸Dropoutã€BatchNormã€L2æ­£åˆ™åŒ–ç­‰è”åˆä½¿ç”¨ã€‚

#### æ³¨æ„äº‹é¡¹
- **è°ƒåº¦ç­–ç•¥é€‰æ‹©**ï¼šStepLRç®€å•ï¼Œä½™å¼¦é€€ç«æˆ–ReduceLROnPlateauæ›´çµæ´»ã€‚
- **æ­¥é•¿å’Œè¡°å‡ç‡**ï¼š`step_size`å’Œ`gamma`éœ€æ ¹æ®ä»»åŠ¡è°ƒä¼˜ã€‚
- **è°ƒç”¨é¡ºåº**ï¼š`scheduler.step()`é€šå¸¸åœ¨`optimizer.step()`åè°ƒç”¨ã€‚
