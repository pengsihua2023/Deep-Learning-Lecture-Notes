## æ ¹æ®æŸå¤±ç›‘æ§è‡ªåŠ¨é™ä½å­¦ä¹ ç‡
### ğŸ“– ä»€ä¹ˆæ˜¯æ ¹æ®æŸå¤±ç›‘æ§è‡ªåŠ¨é™ä½å­¦ä¹ ç‡ï¼ˆReduceLROnPlateauï¼‰ï¼Ÿ

`ReduceLROnPlateau` æ˜¯ä¸€ç§åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡çš„è°ƒåº¦ç­–ç•¥ï¼Œé€šè¿‡ç›‘æ§éªŒè¯é›†ä¸Šçš„æŸå¤±ï¼ˆæˆ–å…¶ä»–æŒ‡æ ‡ï¼‰æ¥å†³å®šæ˜¯å¦é™ä½å­¦ä¹ ç‡ã€‚å¦‚æœéªŒè¯æŸå¤±åœ¨ä¸€å®šè½®æ•°ï¼ˆè€å¿ƒå€¼ï¼Œpatienceï¼‰å†…æ²¡æœ‰æ”¹å–„ï¼Œå­¦ä¹ ç‡ä¼šä¹˜ä»¥ä¸€ä¸ªè¡°å‡å› å­ï¼ˆfactorï¼‰ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹æ›´ç²¾ç»†åœ°ä¼˜åŒ–ï¼Œé˜²æ­¢éœ‡è¡æˆ–è¿‡æ—©é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

### ğŸ“– æ ¸å¿ƒåŸç†
- **ç›‘æ§æŒ‡æ ‡**ï¼šé€šå¸¸æ˜¯éªŒè¯æŸå¤±ï¼ˆä¹Ÿå¯ä¸ºå‡†ç¡®ç‡ç­‰ï¼‰ã€‚
- **æ¡ä»¶è§¦å‘**ï¼šå¦‚æœéªŒè¯æŸå¤±è¿ç»­`patience`è½®æœªä¸‹é™ï¼ˆæˆ–æœªè¾¾åˆ°æœ€å°æ”¹è¿›`min_delta`ï¼‰ï¼Œåˆ™é™ä½å­¦ä¹ ç‡ã€‚
- **å­¦ä¹ ç‡è°ƒæ•´**ï¼šæ–°å­¦ä¹ ç‡ = å½“å‰å­¦ä¹ ç‡ Ã— `factor`ï¼ˆå¦‚0.1ï¼‰ã€‚
- **åœæ­¢æ¡ä»¶**ï¼šå¯é€‰è®¾ç½®æœ€å°å­¦ä¹ ç‡`min_lr`ï¼Œé¿å…è¿‡ä½ã€‚

### ğŸ“– ä¼˜åŠ¿
- **è‡ªé€‚åº”è°ƒæ•´**ï¼šæ ¹æ®æ¨¡å‹æ€§èƒ½åŠ¨æ€é™ä½å­¦ä¹ ç‡ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡ã€‚
- **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šå¸®åŠ©æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šæ‰¾åˆ°æ›´å¥½çš„è§£ã€‚
- **çµæ´»æ€§**ï¼šå¯ç›‘æ§ä»»ä½•æŒ‡æ ‡ï¼ˆå¦‚æŸå¤±ã€å‡†ç¡®ç‡ï¼‰ã€‚

### ğŸ“– å±€é™æ€§
- **éªŒè¯é›†ä¾èµ–**ï¼šéœ€è¦å¯é çš„éªŒè¯é›†æ•°æ®ã€‚
- **è¶…å‚æ•°è°ƒä¼˜**ï¼š`patience`ã€`factor`å’Œ`min_delta`éœ€åˆç†è®¾ç½®ã€‚

---

### ğŸ“– Pythonä»£ç ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„PyTorchç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨MNISTæ‰‹å†™æ•°å­—åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨`ReduceLROnPlateau`è°ƒåº¦å™¨ï¼Œæ ¹æ®éªŒè¯æŸå¤±è‡ªåŠ¨é™ä½å­¦ä¹ ç‡ã€‚ä»£ç ä¿æŒæç®€ï¼Œç»“åˆAdamä¼˜åŒ–å™¨ã€‚

```
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau

# æ­¥éª¤1: å®šä¹‰ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc = nn.Linear(28 * 28, 10)  # è¾“å…¥ï¼š28x28åƒç´ ï¼Œè¾“å‡ºï¼š10ç±»
    
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³è¾“å…¥
        x = self.fc(x)
        return x

# æ­¥éª¤2: åŠ è½½MNISTæ•°æ®é›†å¹¶æ‹†åˆ†è®­ç»ƒ/éªŒè¯é›†
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=64)
test_loader = DataLoader(test_dataset, batch_size=64)

# æ­¥éª¤3: åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
model = SimpleNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, min_lr=1e-6)

# æ­¥éª¤4: è®­ç»ƒå‡½æ•°
def train(epoch):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)

# æ­¥éª¤5: éªŒè¯å‡½æ•°
def validate():
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, target in val_loader:
            output = model(data)
            val_loss += criterion(output, target).item()
    return val_loss / len(val_loader)

# æ­¥éª¤6: æµ‹è¯•å‡½æ•°
def test():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    return 100. * correct / total

# æ­¥éª¤7: è®­ç»ƒå¾ªç¯
epochs = 10
for epoch in range(1, epochs + 1):
    train_loss = train(epoch)
    val_loss = validate()
    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]["lr"]:.6f}')
    scheduler.step(val_loss)  # æ ¹æ®éªŒè¯æŸå¤±æ›´æ–°å­¦ä¹ ç‡

# æ­¥éª¤8: æµ‹è¯•æ¨¡å‹
test_accuracy = test()
print(f'Test Accuracy: {test_accuracy:.2f}%')
```

---

### ğŸ“– ä»£ç è¯´æ˜

1. **æ¨¡å‹å®šä¹‰**ï¼š
   - `SimpleNet` æ˜¯ä¸€ä¸ªæç®€çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œè¾“å…¥ä¸ºMNISTçš„28x28åƒç´ å›¾åƒï¼Œè¾“å‡ºä¸º10ç±»åˆ†ç±»ã€‚

2. **æ•°æ®é›†**ï¼š
   - ä½¿ç”¨`torchvision`åŠ è½½MNISTæ•°æ®é›†ï¼Œæ‹†åˆ†ä¸º80%è®­ç»ƒ+20%éªŒè¯ã€‚
   - æ‰¹é‡å¤§å°ä¸º64ï¼Œæ•°æ®é¢„å¤„ç†ä»…åŒ…æ‹¬è½¬æ¢ä¸ºå¼ é‡ã€‚

3. **è°ƒåº¦å™¨**ï¼š
   - `ReduceLROnPlateau`é…ç½®ï¼š
     - `mode='min'`ï¼šç›‘æ§éªŒè¯æŸå¤±ï¼Œæœ€å°åŒ–ã€‚
     - `factor=0.1`ï¼šæŸå¤±ä¸æ”¹å–„æ—¶ï¼Œå­¦ä¹ ç‡ä¹˜ä»¥0.1ã€‚
     - `patience=2`ï¼šè¿ç»­2è½®éªŒè¯æŸå¤±æ— æ”¹è¿›åˆ™é™ä½å­¦ä¹ ç‡ã€‚
     - `min_lr=1e-6`ï¼šå­¦ä¹ ç‡æœ€å°å€¼ã€‚
   - `scheduler.step(val_loss)`ï¼šåœ¨æ¯ä¸ªepochæœ«æ ¹æ®éªŒè¯æŸå¤±æ›´æ–°å­¦ä¹ ç‡ã€‚

4. **è®­ç»ƒä¸æµ‹è¯•**ï¼š
   - è®­ç»ƒæ—¶æ‰“å°è®­ç»ƒæŸå¤±ã€éªŒè¯æŸå¤±å’Œå½“å‰å­¦ä¹ ç‡ã€‚
   - æµ‹è¯•æ—¶è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ã€‚

5. **è¾“å‡ºç¤ºä¾‹**ï¼š
   ```
   Epoch 1, Train Loss: 0.4567, Val Loss: 0.2876, LR: 0.001000
   Epoch 2, Train Loss: 0.2987, Val Loss: 0.2345, LR: 0.001000
   Epoch 3, Train Loss: 0.2564, Val Loss: 0.2109, LR: 0.001000
   Epoch 4, Train Loss: 0.2345, Val Loss: 0.2112, LR: 0.001000
   Epoch 5, Train Loss: 0.2213, Val Loss: 0.2123, LR: 0.000100
   Epoch 6, Train Loss: 0.1987, Val Loss: 0.2014, LR: 0.000100
   ...
   Test Accuracy: 94.80%
   ```
   å®é™…å€¼å› éšæœºåˆå§‹åŒ–è€Œå¼‚ã€‚æ³¨æ„ç¬¬5ä¸ªepochå­¦ä¹ ç‡é™ä¸º0.0001ï¼Œå› éªŒè¯æŸå¤±è¿ç»­2è½®æœªæ”¹å–„ã€‚

---

### ğŸ“– å…³é”®ç‚¹
- **åŠ¨æ€è°ƒæ•´**ï¼š`ReduceLROnPlateau`æ ¹æ®éªŒè¯æŸå¤±è‡ªåŠ¨é™ä½å­¦ä¹ ç‡ï¼Œçµæ´»é€‚åº”è®­ç»ƒè¿›ç¨‹ã€‚
- **è°ƒåº¦å™¨è°ƒç”¨**ï¼š`scheduler.step(val_loss)`éœ€ä¼ å…¥éªŒè¯æŸå¤±ï¼Œæ”¾åœ¨epochæœ«ã€‚
- **å‚æ•°è®¾ç½®**ï¼š
   - `patience=2`ï¼šç­‰å¾…2è½®æ— æ”¹è¿›ã€‚
   - `factor=0.1`ï¼šå­¦ä¹ ç‡è¡°å‡åˆ°åŸæ¥çš„1/10ã€‚
   - `min_lr`ï¼šé˜²æ­¢å­¦ä¹ ç‡è¿‡ä½ã€‚

---

### ğŸ“– å®é™…åº”ç”¨åœºæ™¯
- **å¤æ‚æ¨¡å‹**ï¼šå¦‚Transformerã€ResNetï¼ŒéªŒè¯æŸå¤±æ³¢åŠ¨å¤§æ—¶ï¼Œ`ReduceLROnPlateau`èƒ½åŠ¨æ€è°ƒæ•´ã€‚
- **ä¸å…¶ä»–æ­£åˆ™åŒ–ç»“åˆ**ï¼šå¯ä¸Dropoutã€BatchNormã€L2æ­£åˆ™åŒ–ï¼ˆå¦‚å‰è¿°é—®é¢˜ï¼‰è”åˆä½¿ç”¨ã€‚
- **ä¸ç¨³å®šè®­ç»ƒ**ï¼šå½“æŸå¤±æ›²çº¿ä¸å¹³æ»‘æ—¶ï¼Œ`ReduceLROnPlateau`æ¯”å›ºå®šè°ƒåº¦ï¼ˆå¦‚StepLRï¼‰æ›´æœ‰æ•ˆã€‚

### ğŸ“– æ³¨æ„äº‹é¡¹
- **éªŒè¯é›†è´¨é‡**ï¼šéœ€ç¡®ä¿éªŒè¯é›†ä»£è¡¨æ€§å¼ºï¼Œå¦åˆ™å¯èƒ½è¯¯è§¦å‘ã€‚
- **è¶…å‚æ•°è°ƒä¼˜**ï¼š`patience`å’Œ`factor`éœ€æ ¹æ®ä»»åŠ¡è°ƒæ•´ã€‚
- **æŒ‡æ ‡é€‰æ‹©**ï¼šå¯ç›‘æ§å‡†ç¡®ç‡ï¼ˆè®¾`mode='max'`ï¼‰æˆ–å…¶ä»–æŒ‡æ ‡ã€‚
