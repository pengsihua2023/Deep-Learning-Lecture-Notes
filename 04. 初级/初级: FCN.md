# FCN
å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFully Connected Neural Network, FCNNï¼‰ï¼Œä¹Ÿç§°ä¸ºå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼‰ï¼Œæ˜¯ä¸€ç±»æœ€åŸºæœ¬çš„äººå·¥ç¥ç»ç½‘ç»œç»“æ„ã€‚å…¶å®šä¹‰å¦‚ä¸‹ï¼š

### åŸºæœ¬å®šä¹‰

åœ¨å…¨è¿æ¥ç¥ç»ç½‘ç»œä¸­ï¼Œç›¸é‚»å±‚ä¹‹é—´çš„**æ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½ä¸ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€å±‚çš„è¾“å‡ºä¼šä½œä¸ºåŠ æƒè¾“å…¥ä¼ é€’ç»™ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ã€‚
<div align="center">
<img width="600" height="180" alt="image" src="https://github.com/user-attachments/assets/4f07aa2a-dd72-4e95-8543-7f71810d8023" />
</div>
<div align="center">
(æ­¤å›¾å¼•è‡ªInternetã€‚)
</div>

### ç»“æ„ç‰¹ç‚¹

1. **è¾“å…¥å±‚ï¼ˆInput Layerï¼‰**ï¼šæ¥æ”¶åŸå§‹æ•°æ®ï¼ˆå¦‚ç‰¹å¾å‘é‡ï¼‰ã€‚
2. **éšè—å±‚ï¼ˆHidden Layersï¼‰**ï¼šç”±å¤šä¸ªç¥ç»å…ƒç»„æˆï¼Œé€šè¿‡åŠ æƒæ±‚å’Œå’Œæ¿€æ´»å‡½æ•°è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œä»è€Œæå–å’Œç»„åˆç‰¹å¾ã€‚
3. **è¾“å‡ºå±‚ï¼ˆOutput Layerï¼‰**ï¼šç»™å‡ºé¢„æµ‹ç»“æœæˆ–åˆ†ç±»ç»“æœã€‚
4. **æƒé‡ä¸åç½®ï¼ˆWeights & Biasesï¼‰**ï¼šæ¯ä¸€æ¡è¿æ¥çº¿éƒ½æœ‰å¯¹åº”çš„æƒé‡å‚æ•°ï¼Œæ¯ä¸ªç¥ç»å…ƒé€šå¸¸è¿˜å¸¦æœ‰åç½®é¡¹ã€‚
5. **æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰**ï¼šåœ¨éšè—å±‚æˆ–è¾“å‡ºå±‚ä¸­å¼•å…¥éçº¿æ€§ï¼Œä½¿ç½‘ç»œå…·å¤‡æ‹Ÿåˆå¤æ‚å‡½æ•°çš„èƒ½åŠ›ã€‚

### ç‰¹ç‚¹ä¸åº”ç”¨

* **ä¼˜ç‚¹**ï¼šç»“æ„ç®€å•ï¼Œé€šç”¨æ€§å¼ºï¼Œèƒ½é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•°ï¼ˆé€šç”¨è¿‘ä¼¼å®šç†ï¼‰ã€‚
* **ç¼ºç‚¹**ï¼šå‚æ•°é‡å¤§ï¼ˆå°¤å…¶åœ¨è¾“å…¥ç»´åº¦å¾ˆé«˜æ—¶ï¼‰ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè®­ç»ƒæ•ˆç‡è¾ƒä½ã€‚
* **åº”ç”¨**ï¼šæ—©æœŸå¤šç”¨äºç®€å•åˆ†ç±»ä¸å›å½’é—®é¢˜ï¼Œå¦‚æ‰‹å†™æ•°å­—è¯†åˆ«ï¼ˆMNISTï¼‰ã€è¡¨æ ¼æ•°æ®é¢„æµ‹ç­‰ã€‚

 
## ğŸ“– æ•°å­¦æè¿°

## 1. ç½‘ç»œç»“æ„

ä¸€ä¸ªå…¸å‹çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œç”±è‹¥å¹² **å±‚ (layers)** æ„æˆï¼š

* è¾“å…¥å±‚ï¼ˆinput layerï¼‰
* ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ï¼ˆhidden layersï¼‰
* è¾“å‡ºå±‚ï¼ˆoutput layerï¼‰

åœ¨å…¨è¿æ¥ç»“æ„ä¸­ï¼Œ**æ¯ä¸€å±‚çš„æ¯ä¸ªç¥ç»å…ƒéƒ½ä¸ä¸Šä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿**ã€‚



## 2. æ•°å­¦ç¬¦å·

è®¾ï¼š

* è¾“å…¥å‘é‡ä¸º


$$
\mathbf{x} \in \mathbb{R}^{d}
$$


* ç¬¬ $l$ å±‚æœ‰ $n_l$ ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºè®°ä¸º

$$
\mathbf{h}^{(l)} \in \mathbb{R}^{n_l}
$$

* æƒé‡çŸ©é˜µä¸åç½®ä¸º

$$
\mathbf{W}^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}, \quad \mathbf{b}^{(l)} \in \mathbb{R}^{n_l}
$$

* æ¿€æ´»å‡½æ•°ä¸º

$$
\sigma(\cdot)
$$



## 3. å‰å‘ä¼ æ’­ (Forward Propagation)

è¾“å…¥å±‚è®°ä¸º

$$
\mathbf{h}^{(0)} = \mathbf{x}
$$

å¯¹äºç¬¬ $l$ å±‚ ($l=1,2,\dots,L$)ï¼Œæœ‰ï¼š

1. **çº¿æ€§å˜æ¢ï¼š**

$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)}
$$

2. **éçº¿æ€§æ¿€æ´»ï¼š**

$$
\mathbf{h}^{(l)} = \sigma\left(\mathbf{z}^{(l)}\right)
$$

æœ€ç»ˆï¼Œè¾“å‡ºå±‚ç»“æœä¸ºï¼š

$$
\hat{\mathbf{y}} = \mathbf{h}^{(L)}
$$



## 4. æŸå¤±å‡½æ•° (Loss Function)

è®­ç»ƒæ—¶ï¼Œç»™å®šç›®æ ‡è¾“å‡º $\mathbf{y}$ï¼Œå¸¸ç”¨æŸå¤±å‡½æ•°åŒ…æ‹¬ï¼š

* **å›å½’é—®é¢˜ï¼š** å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰

$$
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N \|\hat{\mathbf{y}}^{(i)} - \mathbf{y}^{(i)}\|^2
$$

* **åˆ†ç±»é—®é¢˜ï¼š** äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropyï¼‰

$$
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = - \sum_{k=1}^K y_k \log \hat{y}_k
$$



## 5. å‚æ•°æ›´æ–° (Backpropagation + Gradient Descent)

é€šè¿‡åå‘ä¼ æ’­ (Backpropagation) è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}, \quad \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}}
$$

å†ç”¨æ¢¯åº¦ä¸‹é™æˆ–å…¶å˜ç§ï¼ˆå¦‚ Adam, SGD, RMSPropï¼‰æ›´æ–°ï¼š

$$
\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}
$$

$$
\mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}}
$$

å…¶ä¸­ $\eta$ ä¸ºå­¦ä¹ ç‡ã€‚



### æ€»ç»“æ¥è¯´ï¼Œå…¨è¿æ¥ç¥ç»ç½‘ç»œå¯ä»¥æŠ½è±¡ä¸ºï¼š

$$
\hat{\mathbf{y}} = f(\mathbf{x}; \Theta) = \sigma^{(L)}\Big(\mathbf{W}^{(L)} \sigma^{(L-1)}(\cdots \sigma^{(1)}(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}) \cdots ) + \mathbf{b}^{(L)}\Big)
$$

å…¶ä¸­ $\Theta = \{\mathbf{W}^{(l)}, \mathbf{b}^{(l)} \mid l=1,\dots,L\}$ ä¸ºæ¨¡å‹å‚æ•°ã€‚



## ğŸ“– ä»£ç ï¼ˆpytorchï¼‰
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
torch.manual_seed(42)

# 1. æ•°æ®å‡†å¤‡
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# åŠ è½½Fashion MNISTæ•°æ®é›†
trainset = torchvision.datasets.FashionMNIST(
    root='./data', 
    train=True,
    download=True, 
    transform=transform
)
trainloader = DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.FashionMNIST(
    root='./data', 
    train=False,
    download=True, 
    transform=transform
)
testloader = DataLoader(testset, batch_size=64, shuffle=False)

# 2. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹
class FashionMNISTClassifier(nn.Module):
    def __init__(self):
        super(FashionMNISTClassifier, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥å±‚åˆ°éšè—å±‚
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)      # éšè—å±‚åˆ°è¾“å‡ºå±‚ï¼ˆ10ä¸ªç±»åˆ«ï¼‰
        
    def forward(self, x):
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FashionMNISTClassifier().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. è®­ç»ƒæ¨¡å‹
def train_model(num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, (images, labels) in enumerate(trainloader):
            images, labels = images.to(device), labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            if (i + 1) % 100 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/100:.4f}')
                running_loss = 0.0

# 5. æµ‹è¯•æ¨¡å‹
def test_model():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

# 6. æ‰§è¡Œè®­ç»ƒå’Œæµ‹è¯•
if __name__ == "__main__":
    print("Training started...")
    train_model(num_epochs=5)
    print("\nTesting started...")
    test_model()

```
## ğŸ“– è®­ç»ƒç»“æœ
Epoch [5/5], Step [800], Loss: 0.3124   
Epoch [5/5], Step [900], Loss: 0.2941   

Testing started...   
Test Accuracy: 87.24%   
