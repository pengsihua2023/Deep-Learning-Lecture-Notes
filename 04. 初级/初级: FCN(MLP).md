## FCN(MLP)
å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼ˆFully Connected Neural Network / MLPï¼‰  
- é‡è¦æ€§ï¼šè¿™æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¨¡å‹ï¼Œç†è§£å®ƒæœ‰åŠ©äºæŒæ¡ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ¦‚å¿µï¼ˆå¦‚ç¥ç»å…ƒã€æƒé‡ã€æ¿€æ´»å‡½æ•°ã€æ¢¯åº¦ä¸‹é™ï¼‰ã€‚  
- æ ¸å¿ƒæ¦‚å¿µï¼š  
ç¥ç»ç½‘ç»œç”±è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆï¼Œåƒå¤§è„‘çš„ç¥ç»å…ƒä¸€æ ·å·¥ä½œã€‚  
æ¯ä¸ªç¥ç»å…ƒæ¥æ”¶è¾“å…¥ï¼Œè®¡ç®—åŠ æƒå’Œï¼Œç»è¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ sigmoid æˆ– ReLUï¼‰è¾“å‡ºã€‚  
è®­ç»ƒæ˜¯é€šè¿‡â€œè¯•é”™â€è°ƒæ•´æƒé‡ï¼Œä½¿é¢„æµ‹æ›´æ¥è¿‘çœŸå®å€¼ï¼ˆç”¨æ¢¯åº¦ä¸‹é™ï¼‰ã€‚  
- åº”ç”¨ï¼šæˆ¿ä»·é¢„æµ‹ã€åˆ†ç±»ä»»åŠ¡ï¼ˆå¦‚åˆ¤æ–­å›¾ç‰‡æ˜¯çŒ«è¿˜æ˜¯ç‹—ï¼‰ã€‚
- ä¸ºä»€ä¹ˆæ•™ï¼šMLP æ˜¯æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºç¡€ï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæœºåˆ¶ã€‚  
<div align="center">
<img width="600" height="180" alt="image" src="https://github.com/user-attachments/assets/4f07aa2a-dd72-4e95-8543-7f71810d8023" />
</div>
<div align="center">
(æ­¤å›¾å¼•è‡ªInternetã€‚)
</div>

## ğŸ“– æ•°å­¦æè¿°

## 1. ç½‘ç»œç»“æ„

ä¸€ä¸ªå…¸å‹çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œç”±è‹¥å¹² **å±‚ (layers)** æ„æˆï¼š

* è¾“å…¥å±‚ï¼ˆinput layerï¼‰
* ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ï¼ˆhidden layersï¼‰
* è¾“å‡ºå±‚ï¼ˆoutput layerï¼‰

åœ¨å…¨è¿æ¥ç»“æ„ä¸­ï¼Œ**æ¯ä¸€å±‚çš„æ¯ä¸ªç¥ç»å…ƒéƒ½ä¸ä¸Šä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿**ã€‚



## 2. æ•°å­¦ç¬¦å·

è®¾ï¼š

* è¾“å…¥å‘é‡ä¸º


$$
\mathbf{x} \in \mathbb{R}^{d}
$$


* ç¬¬ $l$ å±‚æœ‰ $n_l$ ä¸ªç¥ç»å…ƒï¼Œè¾“å‡ºè®°ä¸º

$$
\mathbf{h}^{(l)} \in \mathbb{R}^{n_l}
$$

* æƒé‡çŸ©é˜µä¸åç½®ä¸º

$$
\mathbf{W}^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}, \quad \mathbf{b}^{(l)} \in \mathbb{R}^{n_l}
$$

* æ¿€æ´»å‡½æ•°ä¸º

$$
\sigma(\cdot)
$$



## 3. å‰å‘ä¼ æ’­ (Forward Propagation)

è¾“å…¥å±‚è®°ä¸º

$$
\mathbf{h}^{(0)} = \mathbf{x}
$$

å¯¹äºç¬¬ $l$ å±‚ ($l=1,2,\dots,L$)ï¼Œæœ‰ï¼š

1. **çº¿æ€§å˜æ¢ï¼š**

$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)}
$$

2. **éçº¿æ€§æ¿€æ´»ï¼š**

$$
\mathbf{h}^{(l)} = \sigma\left(\mathbf{z}^{(l)}\right)
$$

æœ€ç»ˆï¼Œè¾“å‡ºå±‚ç»“æœä¸ºï¼š

$$
\hat{\mathbf{y}} = \mathbf{h}^{(L)}
$$



## 4. æŸå¤±å‡½æ•° (Loss Function)

è®­ç»ƒæ—¶ï¼Œç»™å®šç›®æ ‡è¾“å‡º $\mathbf{y}$ï¼Œå¸¸ç”¨æŸå¤±å‡½æ•°åŒ…æ‹¬ï¼š

* **å›å½’é—®é¢˜ï¼š** å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰

$$
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = \frac{1}{N}\sum_{i=1}^N \|\hat{\mathbf{y}}^{(i)} - \mathbf{y}^{(i)}\|^2
$$

* **åˆ†ç±»é—®é¢˜ï¼š** äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropyï¼‰

$$
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = - \sum_{k=1}^K y_k \log \hat{y}_k
$$



## 5. å‚æ•°æ›´æ–° (Backpropagation + Gradient Descent)

é€šè¿‡åå‘ä¼ æ’­ (Backpropagation) è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ï¼š

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}, \quad \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}}
$$

å†ç”¨æ¢¯åº¦ä¸‹é™æˆ–å…¶å˜ç§ï¼ˆå¦‚ Adam, SGD, RMSPropï¼‰æ›´æ–°ï¼š

$$
\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}
$$

$$
\mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}}
$$

å…¶ä¸­ $\eta$ ä¸ºå­¦ä¹ ç‡ã€‚



### æ€»ç»“æ¥è¯´ï¼Œå…¨è¿æ¥ç¥ç»ç½‘ç»œå¯ä»¥æŠ½è±¡ä¸ºï¼š

$$
\hat{\mathbf{y}} = f(\mathbf{x}; \Theta) = \sigma^{(L)}\Big(\mathbf{W}^{(L)} \sigma^{(L-1)}(\cdots \sigma^{(1)}(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}) \cdots ) + \mathbf{b}^{(L)}\Big)
$$

å…¶ä¸­ $\Theta = \{\mathbf{W}^{(l)}, \mathbf{b}^{(l)} \mid l=1,\dots,L\}$ ä¸ºæ¨¡å‹å‚æ•°ã€‚



## ğŸ“– ä»£ç ï¼ˆpytorchï¼‰
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤
torch.manual_seed(42)

# 1. æ•°æ®å‡†å¤‡
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# åŠ è½½Fashion MNISTæ•°æ®é›†
trainset = torchvision.datasets.FashionMNIST(
    root='./data', 
    train=True,
    download=True, 
    transform=transform
)
trainloader = DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.FashionMNIST(
    root='./data', 
    train=False,
    download=True, 
    transform=transform
)
testloader = DataLoader(testset, batch_size=64, shuffle=False)

# 2. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹
class FashionMNISTClassifier(nn.Module):
    def __init__(self):
        super(FashionMNISTClassifier, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥å±‚åˆ°éšè—å±‚
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)      # éšè—å±‚åˆ°è¾“å‡ºå±‚ï¼ˆ10ä¸ªç±»åˆ«ï¼‰
        
    def forward(self, x):
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FashionMNISTClassifier().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. è®­ç»ƒæ¨¡å‹
def train_model(num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, (images, labels) in enumerate(trainloader):
            images, labels = images.to(device), labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            if (i + 1) % 100 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/100:.4f}')
                running_loss = 0.0

# 5. æµ‹è¯•æ¨¡å‹
def test_model():
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

# 6. æ‰§è¡Œè®­ç»ƒå’Œæµ‹è¯•
if __name__ == "__main__":
    print("Training started...")
    train_model(num_epochs=5)
    print("\nTesting started...")
    test_model()

```
## ğŸ“– è®­ç»ƒç»“æœ
Epoch [5/5], Step [800], Loss: 0.3124   
Epoch [5/5], Step [900], Loss: 0.2941   

Testing started...   
Test Accuracy: 87.24%   
