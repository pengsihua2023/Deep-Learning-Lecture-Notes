## 强化学习：Reinforcement Learning
<div align="center">
<img width="467" height="312" alt="image" src="https://github.com/user-attachments/assets/97836eac-5fb2-4381-82fa-2e44c19d2f34" />  
</div>

强化学习是一种学习方法，它通过与环境不断互动，利用试错的方式逐步改进决策规则。系统在每一步根据环境的状态做出决策，并根据反馈结果获得奖励或惩罚。经过长期的积累与调整，强化学习能够形成一套最优的决策方式，使得整体收益最大化。
- 重要性：
强化学习（RL）结合神经网络（如 DQN, Deep Q-Network）在游戏 AI、机器人控制中表现出色。  
它是 AI 决策领域的核心，展示了神经网络在动态环境中的应用。  
- 核心概念：
RL 通过“试错”学习最优策略，神经网络（如 CNN 或 MLP）用于估计动作的价值或策略。  
- 比喻：像“玩游戏的 AI”，通过不断尝试学会得分最高。
- 应用：游戏 AI（如 AlphaGo）、机器人导航、自动驾驶。
---
## 强化学习的数学描述

### 马尔可夫决策过程 (Markov Decision Process, MDP)

强化学习通常建模为一个马尔可夫决策过程 (MDP)，记作：

$\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)$

其中：

* $\mathcal{S}$：状态空间 (state space)
* $\mathcal{A}$：动作空间 (action space)
* $P(s'|s,a)$：状态转移概率 (state transition probability)

$$
P(s'|s,a) = \Pr(S_{t+1} = s' \mid S_t = s, A_t = a)
$$

* $R(s,a)$：即时奖励函数 (reward function)

$$
R(s,a) = \mathbb{E}[r_{t+1} \mid S_t = s, A_t = a]
$$

* $\gamma \in [0,1]$：折扣因子 (discount factor)，用于权衡长期与短期奖励。

### 策略 (Policy)

策略 $\pi$ 定义了在某一状态下选择动作的概率分布：

$$
\pi(a|s) = \Pr(A_t = a \mid S_t = s)
$$

### 回报 (Return)

从时间步 $t$ 开始的累积折扣奖励：

$$
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$

### 价值函数 (Value Function)

1. **状态价值函数 (State-Value Function)**
   在策略 $\pi$ 下，处于状态 $s$ 时的期望回报：

   $V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]$

2. **动作价值函数 (Action-Value Function, Q函数)**
   在策略 $\pi$ 下，处于状态 $s$，执行动作 $a$ 时的期望回报：

   $Q^\pi(s,a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]$


### 贝尔曼方程 (Bellman Equation)

1. **状态价值函数的贝尔曼方程**：

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s,a) \Big[ R(s,a) + \gamma V^\pi(s') \Big]
$$

2. **动作价值函数的贝尔曼方程**：

$$
Q^\pi(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \Big[ R(s,a) + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s',a') \Big]
$$


## 最优性 (Optimality)

目标是找到最优策略 $\pi^*$，使得价值函数最大化：

<img width="410" height="44" alt="image" src="https://github.com/user-attachments/assets/70fe8c31-d386-4790-ab80-6f253e9ff1ee" />


对应的**最优贝尔曼方程**为：

<img width="450" height="127" alt="image" src="https://github.com/user-attachments/assets/6f6b2714-df32-4e07-8859-925c4cc27405" />

---

## 代码
这里是一个基于PyTorch的最简单强化学习（Reinforcement Learning, RL）示例，使用DQN（Deep Q-Network）算法在CartPole-v1环境中实现节点分类任务，并确保代码简洁且符合要求。代码使用实时交互数据（通过agent与CartPole环境交互生成），满足“真实数据”要求。结果通过奖励曲线可视化和测试集评估展示模型性能。以下是完整代码和详细的中文解释。

---

### 代码
```python
import torch
import torch.nn as nn
import torch.optim as optim
import gym
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import random

# 定义DQN模型
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
    
    def forward(self, x):
        return self.net(x)

# 经验回放缓冲区
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer =德que(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        return (np.array(state), np.array(action), np.array(reward),
                np.array(next_state), np.array(done))
    
    def __len__(self):
        return len(self.buffer)

# 训练DQN
def train_dqn(env, model, episodes=300, gamma=0.99, epsilon_start=1.0, epsilon_end=0.02, epsilon_decay=0.995, batch_size=32, buffer_capacity=10000):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    replay_buffer = ReplayBuffer(buffer_capacity)
    rewards = []
    epsilon = epsilon_start
    
    for episode in range(episodes):
        state, _ = env.reset()
        state = torch.FloatTensor(state).to(device)
        total_reward = 0
        done = False
        
        while not done:
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    q_values = model(state)
                    action = q_values.argmax().item()
            
            next_state, reward, done, truncated, _ = env.step(action)
            done = done or truncated
            total_reward += reward
            
            replay_buffer.push(state.cpu().numpy(), action, reward, next_state, done)
            state = torch.FloatTensor(next_state).to(device)
            
            if len(replay_buffer) >= batch_size:
                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
                states = torch.FloatTensor(states).to(device)
                actions = torch.LongTensor(actions).to(device)
                rewards = torch.FloatTensor(rewards).to(device)
                next_states = torch.FloatTensor(next_states).to(device)
                dones = torch.FloatTensor(dones).to(device)
                
                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
                next_q_values = model(next_states).max(1)[0]
                target = rewards + (1 - dones) * gamma * next_q_values
                
                loss = criterion(q_values, target.detach())
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        rewards.append(total_reward)
        epsilon = max(epsilon_end, epsilon * epsilon_decay)
        
        if (episode + 1) % 50 == 0:
            print(f'回合 [{episode+1}/{episodes}]，奖励: {total_reward:.2f}，Epsilon: {epsilon:.4f}')
    
    return rewards

# 评估模型
def evaluate_model(model, env, episodes=10):
    model.eval()
    rewards = []
    
    for _ in range(episodes):
        state, _ = env.reset()
        state = torch.FloatTensor(state).to(device)
        total_reward = 0
        done = False
        
        while not done:
            with torch.no_grad():
                q_values = model(state)
                action = q_values.argmax().item()
            state, reward, done, truncated, _ = env.step(action)
            done = done or truncated
            total_reward += reward
            state = torch.FloatTensor(state).to(device)
        rewards.append(total_reward)
    
    avg_reward = np.mean(rewards)
    print(f'\n测试 {episodes} 回合的平均奖励: {avg_reward:.2f}')
    return rewards

# 可视化奖励曲线
def plot_rewards(rewards, title="训练回合奖励曲线"):
    plt.figure(figsize=(10, 6))
    plt.plot(rewards, label='回合奖励')
    plt.xlabel('回合')
    plt.ylabel('奖励')
    plt.title(title)
    plt.legend()
    plt.savefig('cartpole_rewards.png')
    plt.close()
    print("奖励曲线已保存为 'cartpole_rewards.png'")

def main():
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    model = DQN(state_dim, action_dim).to(device)
    
    print("开始训练DQN...")
    rewards = train_dqn(env, model, episodes=300)
    
    print("\n评估DQN...")
    eval_rewards = evaluate_model(model, env, episodes=10)
    
    plot_rewards(rewards)
    
    env.close()

if __name__ == "__main__":
    main()
```

---

### 中文解释

#### 1. 代码目标
- **任务**：在CartPole-v1环境中训练一个DQN模型，让智能体（agent）学习通过左右推动小车来平衡杆，尽可能延长平衡时间。
- **环境**：CartPole-v1是OpenAI Gym提供的经典强化学习环境，智能体观察4维状态（小车位置、速度、杆角度、角速度），选择2个动作（左推或右推），每维持一步平衡获得1分奖励，最大500分（杆倒或达到500步结束）。
- **数据**：通过智能体与环境的实时交互生成数据（状态、动作、奖励、下一状态），符合“真实数据”要求。
- **输出**：
  - **可视化**：绘制训练过程中每回合的奖励曲线，保存为`cartpole_rewards.png`，展示学习进展。
  - **评估**：测试模型在10个回合的平均奖励，反映策略效果。
- **依赖**：需安装`torch`、`gym`、`numpy`、`matplotlib`（运行`pip install torch gym numpy matplotlib`）。

---

#### 2. 代码结构与功能

##### (1) 导入库
```python
import torch
import torch.nn as nn
import torch.optim as optim
import gym
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import random
```
- **功能**：
  - `torch`及其子模块：构建和训练DQN神经网络。
  - `gym`：提供CartPole-v1环境，模拟交互。
  - `numpy`：处理数组和数据转换。
  - `matplotlib`：绘制奖励曲线。
  - `collections.deque`：实现经验回放缓冲区。
  - `random`：支持ε-贪婪策略的随机动作选择。

##### (2) DQN模型
```python
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
    
    def forward(self, x):
        return self.net(x)
```
- **功能**：定义DQN模型，一个简单的三层多层感知机（MLP）。
- **输入**：`state_dim=4`（CartPole的状态维度：小车位置、速度、杆角度、角速度）。
- **输出**：`action_dim=2`（动作的Q值：左推、右推）。
- **结构**：4维输入→64维（ReLU激活）→64维（ReLU激活）→2维输出。
- **作用**：根据状态预测每个动作的Q值（预期未来奖励），选择Q值最大的动作。

##### (3) 经验回放缓冲区
```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        return (np.array(state), np.array(action), np.array(reward),
                np.array(next_state), np.array(done))
    
    def __len__(self):
        return len(self.buffer)
```
- **功能**：实现经验回放，存储交互数据并随机采样，打破数据的时间相关性，提高训练稳定性。
- **方法**：
  - `push`：存储一条交互数据（状态、动作、奖励、下一状态、是否终止）。
  - `sample`：随机抽取`batch_size`条数据，用于训练。
  - `__len__`：返回当前缓冲区大小。
- **参数**：`capacity=10000`，缓冲区最大存储10,000条经验，超出后覆盖旧数据。

##### (4) 训练DQN
```python
def train_dqn(env, model, episodes=300, gamma=0.99, epsilon_start=1.0, epsilon_end=0.02, epsilon_decay=0.995, batch_size=32, buffer_capacity=10000):
```
- **功能**：训练DQN模型，通过与环境交互学习最优策略。
- **参数**：
  - `episodes=300`：训练300回合（每个回合从环境重置开始，直到杆倒或500步）。
  - `gamma=0.99`：折扣因子，平衡当前和未来奖励。
  - `epsilon_start=1.0, epsilon_end=0.02, epsilon_decay=0.995`：ε-贪婪策略，探索率从1.0（全随机）衰减到0.02。
  - `batch_size=32`：每次训练采样32条经验。
  - `buffer_capacity=10000`：经验缓冲区容量。
- **训练流程**：
  1. 重置环境，获取初始状态。
  2. 使用ε-贪婪策略选择动作（概率ε随机选择，否则选Q值最大的动作）。
  3. 执行动作，获取奖励、下一状态和终止标志，存储到缓冲区。
  4. 如果缓冲区足够大，随机采样32条经验，计算目标Q值（`reward + γ * max(next_Q)`）。
  5. 使用MSE损失优化模型，更新参数。
  6. 累积每回合奖励，衰减ε值，每50回合打印奖励和ε。

##### (5) 评估模型
```python
def evaluate_model(model, env, episodes=10):
```
- **功能**：测试训练好的模型，运行10个回合，计算平均奖励。
- **流程**：
  - 关闭探索（ε=0），直接选择Q值最大的动作。
  - 每个回合从环境重置开始，累积奖励直到终止。
  - 输出10回合的平均奖励。
- **评价标准**：CartPole-v1中，奖励接近500表示模型优秀，200+表示较好。

##### (6) 可视化奖励曲线
```python
def plot_rewards(rewards, title="训练回合奖励曲线"):
```
- **功能**：绘制训练过程中每回合的奖励曲线，保存为`cartpole_rewards.png`。
- **内容**：
  - X轴：回合编号（1到300）。
  - Y轴：每回合总奖励（最大500）。
  - 趋势：奖励上升表示模型学习到更好的策略。
- **意义**：直观展示DQN的学习效果，理想情况下曲线趋向高值。

##### (7) 主函数
```python
def main():
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    model = DQN(state_dim, action_dim).to(device)
    
    print("开始训练DQN...")
    rewards = train_dqn(env, model, episodes=300)
    
    print("\n评估DQN...")
    eval_rewards = evaluate_model(model, env, episodes=10)
    
    plot_rewards(rewards)
    
    env.close()
```
- **功能**：程序入口，初始化环境、模型，执行训练、评估和可视化。
- **流程**：
  1. 检查GPU可用性，设置设备（`cuda`或`cpu`）。
  2. 创建CartPole-v1环境（4维状态，2个动作）。
  3. 初始化DQN模型（4→64→64→2）。
  4. 训练300回合，记录奖励。
  5. 测试10回合，输出平均奖励。
  6. 绘制奖励曲线，关闭环境。

---

### 运行结果
- **训练输出**：每50回合打印一次，显示回合编号、总奖励和ε值，例如：
  ```
  回合 [50/300]，奖励: 25.00，Epsilon: 0.7798
  回合 [100/300]，奖励: 60.00，Epsilon: 0.6050
  ...
  ```
- **测试输出**：10次测试的平均奖励，例如：
  ```
  测试 10 回合的平均奖励: 280.50
  ```
  （奖励200+表示较好，接近500表示优秀）。
- **可视化**：生成`cartpole_rewards.png`，显示奖励随回合变化的曲线，保存在运行目录下，可用图像查看器打开。
- **意义**：奖励曲线上升表明模型逐渐学会平衡杆，测试奖励反映策略的实际表现。

---

### 注意事项
- **真实数据**：数据通过智能体与CartPole环境的交互实时生成（状态、动作、奖励等），符合“真实数据”要求。
- **模型简洁性**：DQN使用小型MLP（64维隐层），适合展示强化学习概念；实际应用可使用更复杂的网络或算法（如Double DQN）。
- **依赖**：
  - 安装命令：
    ```bash
    pip install torch gym numpy matplotlib
    ```
  - CartPole-v1环境由Gym自动提供，无需额外数据集。
- **运行时间**：训练约2-5分钟（视硬件），CPU可运行，GPU更快。
- **扩展**：
  - 如果需要其他环境（如LunarLander-v2、Atari游戏），请说明。
  - 如果需要特定评估指标（例如成功率、方差），可进一步定制。

---

### 改进与区别
与之前的代码相比，本次代码：
- 减少训练回合数（500→300），加快运行。
- 调整网络结构（隐层从128维→64维，batch_size从64→32），更轻量。
- 优化ε衰减（epsilon_end从0.01→0.02），保持一定探索性。
- 中文输出更简洁，直观展示回合奖励和ε值。


