## 深度学习基础知识：常用激活函数简介

---

# 1. Sigmoid激活函数
Sigmoid激活函数是一种常用的非线性激活函数，广泛应用于神经网络中，尤其是在二分类问题中。以下是其简介：

### 定义

Sigmoid函数的数学表达式为：

$$
f(x) = \frac{1}{1 + e^{-x}}
$$

<img width="400" height="260" alt="image" src="https://github.com/user-attachments/assets/08a9e5b0-5222-489c-834a-537005512ee3" />  

其中，x 是输入，e是自然对数的底（约2.718）。

### 特点
1. **输出范围**：Sigmoid函数将任意实数输入映射到 (0, 1) 区间，适合表示概率或用于二分类任务。
2. **非线性**：通过引入非线性，Sigmoid帮助神经网络建模复杂的非线性关系。
3. **平滑性**：函数连续且可导，适合梯度下降优化。

### 优点
- 输出值在0到1之间，适合需要概率输出的场景，如逻辑回归或二分类神经网络。

* 导数简单，易于计算： $f'(x) = f(x)(1 - f(x))$ 

### 缺点
1. **梯度消失**：在输入值较大或较小时，Sigmoid的梯度接近于0，导致深层网络训练时梯度消失问题，影响学习效率。
2. **非零中心**：输出始终为正（0到1），可能导致梯度更新不平衡，减慢收敛。
3. **计算开销**：指数运算相对复杂，计算成本较高。

### 应用场景
- **二分类问题**：常用于输出层，将模型输出转化为概率。
- **传统RNN/LSTM**：早期常用于门控机制（如LSTM的遗忘门、输入门等），不过现代网络常使用其他激活函数（如tanh或ReLU）。
- **概率输出**：当需要将输出解释为概率时，Sigmoid是自然选择。

### 替代方案
由于梯度消失等问题，现代神经网络中，ReLU、Leaky ReLU或tanh等激活函数在某些场景下更受欢迎，尤其是在深层网络中。

总结来说，Sigmoid函数因其简单性和概率解释在特定场景下仍有价值，但在深层网络中需谨慎使用，以避免梯度消失等问题。

---

# 2. ReLU激活函数
ReLU（Rectified Linear Unit）激活函数是现代神经网络中最常用的非线性激活函数之一，因其简单性和高效性被广泛应用于深度学习模型。以下是其简介：
<img width="1387" height="793" alt="image" src="https://github.com/user-attachments/assets/47a21dee-26c4-4278-9178-3f355503b7fa" />  
 

### 定义
ReLU函数的数学表达式为：
f(x) = max(0, x)
即：如果输入x大于0，则输出x；否则输出0。

### 特点
1. **输出范围**：ReLU将负输入映射为0，正输入保持不变，输出范围为 [0, +infty)。
2. **非线性**：通过将负值置零，ReLU引入非线性，便于神经网络建模复杂关系。
3. **稀疏性**：ReLU会使负输入的输出为0，导致部分神经元不激活，产生稀疏表示，有助于提高计算效率和泛化能力。

### 优点
1. **缓解梯度消失**：相比Sigmoid和tanh，ReLU在正输入区域的梯度恒为1，避免了梯度消失问题，加速梯度下降收敛。
2. **计算简单**：仅涉及取最大值操作，计算效率高，适合大规模网络。
3. **稀疏激活**：负输入置零导致部分神经元不激活，减少计算量并降低过拟合风险。

### 缺点
1. **死神经元问题**（Dying ReLU）：当输入始终为负时，神经元的输出和梯度均为0，导致该神经元无法更新，可能永久“死亡”。
2. **非零中心**：输出始终非负，可能导致梯度更新偏向某一方向，影响收敛速度。
3. **非光滑**：在 x=0 处不可导（数学上），尽管在实践中通过定义子梯度（如设为0）解决。

### 应用场景
- **深层神经网络**：ReLU是卷积神经网络（CNN）和全连接网络的默认激活函数，广泛用于计算机视觉和自然语言处理任务。
- **隐藏层**：常用于隐藏层，因其高效性和对梯度消失的鲁棒性。
- **加速训练**：在深层网络（如ResNet、Transformer）中，ReLU显著提升训练速度。

### 改进变体
为解决ReLU的缺点，出现了一些变体：
- **Leaky ReLU**：f(x) = max(alpha x, x)，其中 alpha 是一个小的正数（如0.01），允许负输入有小梯度，缓解死神经元问题。
- **Parametric ReLU（PReLU）**：alpha 作为可学习的参数。
- **ELU（Exponential Linear Unit）**：对负输入使用指数函数，平滑过渡并具有零中心特性。
- **GELU（Gaussian Error Linear Unit）**：结合高斯分布特性，用于Transformer模型。

### 总结
ReLU因其简单、高效和缓解梯度消失的能力，成为深度学习中的首选激活函数，尤其适合隐藏层。尽管存在死神经元等局限性，其变体（如Leaky ReLU）可进一步优化性能。在实际应用中，ReLU通常是构建深层网络的起点选择。

---

# 3. Leaky ReLU激活函数
### Leaky ReLU激活函数简介

Leaky ReLU（Leaky Rectified Linear Unit）是ReLU激活函数的一种变体，旨在解决ReLU的“死神经元”问题，同时保留其简单性和高效性。以下是其简介：  
  
<img width="807" height="590" alt="image" src="https://github.com/user-attachments/assets/d74f9e07-ac63-4c29-8005-7941914576bb" />   
  

### 定义

Leaky ReLU 函数的数学表达式为：

$$
f(x) =
\begin{cases} 
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0
\end{cases}
$$

其中， $x$ 是输入， $\alpha$ 是一个小的正数（通常为 0.01 或 0.1），称为泄漏系数，控制负输入区域的斜率。


### 特点

1. **输出范围**：正输入区域输出为 $x$（与 ReLU 相同），负输入区域输出为 $\alpha x$，整体输出范围为 $(-\infty, +\infty)$。

2. **非线性**：通过分段线性函数引入非线性，支持复杂关系建模。

3. **部分稀疏性**：负输入区域输出非零，但幅度较小，仍保留一定稀疏性。

4. **可导性**：Leaky ReLU 几乎处处可导（在 $x = 0$ 处定义子梯度），导数为：

$$
f'(x) =
\begin{cases} 
1 & \text{if } x > 0 \\
\alpha & \text{if } x \leq 0
\end{cases}
$$
 

### 优点
1. **缓解死神经元问题**：相比ReLU（负输入输出为0，梯度为0），Leaky ReLU为负输入提供非零梯度（alpha），避免神经元“死亡”，使更多神经元参与训练。
2. **保留ReLU优点**：在正输入区域与ReLU相同，计算简单，梯度恒为1，缓解梯度消失问题，适合深层网络。
3. **简单高效**：相比tanh或Sigmoid，Leaky ReLU仍以线性运算为主，计算开销低。

### 缺点
1. **超参数选择**：泄漏系数 alpha 需手动设置（如0.01），不合适的 alpha 可能影响性能。
2. **非零中心**：输出仍然偏向正值，可能导致梯度更新不完全平衡（尽管比ReLU略好）。
3. **负区域梯度较小**：当 alpha 很小时，负输入区域的梯度可能仍不足以有效更新权重。

### 应用场景
- **深层神经网络**：Leaky ReLU常用于卷积神经网络（CNN）和全连接网络的隐藏层，作为ReLU的替代，提升训练稳定性。
- **计算机视觉**：在图像分类、目标检测等任务中，Leaky ReLU因其鲁棒性被广泛使用。
- **替代ReLU**：在ReLU导致死神经元问题显著的场景（如某些数据集或深层网络），Leaky ReLU是首选改进方案。

### 与其他激活函数的对比
- **ReLU**：ReLU负输入输出为0，可能导致死神经元；Leaky ReLU通过非零负梯度缓解此问题。
- **Sigmoid/tanh**：Sigmoid和tanh易引发梯度消失，且计算复杂；Leaky ReLU计算简单，适合深层网络。
- **Parametric ReLU (PReLU)**：PReLU将 alpha 作为可学习参数，增加灵活性但引入额外参数；Leaky ReLU使用固定 alpha，更简单。
- **ELU**：ELU在负输入区域使用指数函数，输出平滑且接近零中心，但计算成本高于Leaky ReLU。

### 变体
- **Parametric ReLU (PReLU)**：alpha 为可训练参数，逐通道或逐神经元学习，增加模型表达力。
- **Randomized Leaky ReLU**：训练时随机选择 \(\alpha\)，测试时固定，提升鲁棒性。

### 注意事项
- **alpha 的选择**：通常设为0.01或0.1，需根据任务实验调整；过大的 alpha 可能削弱稀疏性，过小则接近ReLU。
- **数值稳定性**：Leaky ReLU计算简单，无指数运算，数值稳定性较高。
- **适用性**：Leaky ReLU更适合隐藏层，不常用于输出层（输出层通常使用Softmax或Sigmoid）。

### 总结
Leaky ReLU通过为负输入引入小斜率，解决了ReLU的死神经元问题，同时保留了ReLU的计算效率和梯度消失缓解能力。它在深层神经网络中是ReLU的常见替代方案，尤其在计算机视觉等任务中表现优异。尽管需要手动设置泄漏系数，其简单性和鲁棒性使其成为现代深度学习中的重要激活函数。


# 4. softmax激活函数
Softmax激活函数是一种常用的非线性激活函数，广泛应用于多分类问题中，尤其是在神经网络的输出层。以下是其简介：  
<img width="384" height="300" alt="image" src="https://github.com/user-attachments/assets/58e52eff-9749-4337-bfeb-de294c741e1a" />  

### 定义

Softmax函数将输入向量 **x**（长度为 *n*）的每个元素映射为一个概率分布，输出值在 $[0,1]$ 区间，且所有输出之和为 $1$。其数学表达式为：

$$
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}, \quad i = 1, 2, \ldots, n
$$

其中， $x_i$ 是输入向量的第 $i$ 个元素， $e^{x_i}$ 是其指数化结果， $\sum_{j=1}^n e^{x_j}$ 是所有指数化输入的和，用于归一化。

### 特点
1. **输出范围**：每个输出值在 [0, 1]，且总和为1，适合表示多分类任务中的概率分布。
2. **非线性**：通过指数运算引入非线性，放大输入之间的差异。
3. **相对性**：Softmax的输出不仅依赖于某个输入值，还受其他输入值的影响，强调最大值的突出性。

### 优点
1. **概率解释**：输出可以直接解释为每个类别的概率，便于多分类任务的预测和评估。
2. **与交叉熵损失兼容**：Softmax常与交叉熵损失函数结合使用，优化多分类问题的训练。
3. **放大差异**：指数运算使较大的输入值在输出中占据更大比例，增强模型对最可能类别的信心。

### 缺点
1. **计算开销**：指数运算和归一化步骤相比ReLU等简单激活函数计算成本较高。
2. **梯度消失风险**：当输入值差异较大时，较小的输入对应的输出接近0，梯度可能变得极小，影响训练。
3. **不适合隐藏层**：由于其归一化特性，Softmax通常仅用于输出层，而不适合隐藏层（隐藏层更常用ReLU或tanh）。

### 应用场景
- **多分类问题**：Softmax常用于神经网络输出层，如图像分类（e.g., MNIST、ImageNet）或自然语言处理中的文本分类。
- **概率输出**：当需要模型输出每个类别的概率时，Softmax是标准选择。
- **结合交叉熵损失**：在深度学习框架中，Softmax通常与交叉熵损失集成，简化多分类任务的训练。

### 与其他激活函数的对比
- **Sigmoid**：Sigmoid用于二分类或独立概率输出（多标签分类），而Softmax用于多分类，输出归一化为概率分布。
- **ReLU**：ReLU适合隐藏层，计算简单但不提供概率输出；Softmax专为输出层设计。
- **tanh**：tanh输出范围为 [-1, 1]，不适合直接表示概率，而Softmax专为概率分布设计。

### 注意事项


* **数值稳定性**：在实现中，为避免指数运算导致的溢出，常对输入减去最大值（即 $e^{x_i - \max(x)}$ ），不改变 Softmax 的结果但提高稳定性。

* **损失函数**：Softmax 通常搭配交叉熵损失，而非均方误差，因其概率分布特性更适合分类任务。




### 总结
Softmax激活函数是多分类任务中的核心组件，通过将输入转化为概率分布，为模型提供直观的类别预测。其与交叉熵损失的结合使其在深度学习中广泛应用，尤其在输出层。尽管计算成本较高且不适合隐藏层，Softmax在多分类场景中的表现使其成为不可或缺的激活函数。

---

# 5. tanh激活函数
**tanh激活函数简介**
<img width="640" height="480" alt="image" src="https://github.com/user-attachments/assets/cfa1be3e-ff04-45cc-8509-af8e41be255a" />


tanh（双曲正切）激活函数是一种常用的非线性激活函数，广泛应用于神经网络中，尤其在早期循环神经网络（如RNN、LSTM）中常见。以下是其简介：

### 定义



tanh 函数的数学表达式为：

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

等价于：

$$
\tanh(x) = \frac{2}{1 + e^{-2x}} - 1
$$

其中， $x$ 是输入， $e$ 是自然对数的底（约 2.718）。



### 特点
1. **输出范围**：tanh将任意实数输入映射到 (-1, 1) 区间，输出以0为中心。
2. **非线性**：通过双曲正切运算引入非线性，帮助神经网络建模复杂关系。
3. **平滑性**：函数连续且可导，适合基于梯度的优化方法。
4. **导数**：tanh 的导数为 $\tanh'(x) = 1 - \tanh^2(x),$ 计算简单且基于自身输出。

### 优点
1. **零中心输出**：相比Sigmoid（输出范围[0, 1]，tanh的输出以0为中心，有助于平衡梯度更新，加速收敛。
2. **强非线性**：tanh的S形曲线比ReLU更平滑，适合需要平滑过渡的场景。
3. **适合RNN**：在传统循环神经网络（如LSTM、GRU）中，tanh常用于门控机制或隐藏状态，因其输出范围和梯度特性适合序列建模。

### 缺点
1. **梯度消失**：当输入值较大或较小时，tanh的梯度接近0，导致深层网络或长序列训练时梯度消失，影响学习效率。
2. **计算复杂性**：相比ReLU，tanh涉及指数运算，计算成本较高。
3. **不适合深层网络隐藏层**：由于梯度消失问题，tanh在现代深层网络（如CNN）中通常被ReLU及其变体取代。

### 应用场景
- **循环神经网络（RNN/LSTM/GRU）**：tanh常用于生成隐藏状态或门控机制（如LSTM的输出门），因其零中心特性适合序列建模。
- **浅层网络**：在较小的网络中，tanh可作为隐藏层的激活函数，提供非线性。
- **回归任务**：当输出需要归一化到 [-1, 1] 范围时，tanh是合适选择。

### 与其他激活函数的对比
- **Sigmoid**：Sigmoid输出范围为 [0, 1]，非零中心，可能导致梯度更新不平衡；tanh输出 [-1, 1]，更适合隐藏层。
- **ReLU**：ReLU计算简单，缓解梯度消失，但在负输入区域输出为0，可能导致死神经元；tanh输出连续但易梯度消失。
- **Softmax**：Softmax用于多分类输出层，生成概率分布；tanh更适合隐藏层或回归任务。

### 注意事项
- **梯度消失**：在深层网络或长序列任务中，tanh可能导致训练困难，需结合梯度裁剪或更现代的激活函数（如ReLU）。
- **数值稳定性**：tanh的指数运算可能引发数值溢出问题，但现代框架通常优化了实现。

### 总结
tanh激活函数因其零中心输出和平滑非线性特性，在早期神经网络和循环网络中广泛使用。尽管在深层网络中因梯度消失问题逐渐被ReLU等替代，但在特定场景（如RNN或需要 \([-1, 1]\) 输出的任务）中仍具有重要价值。其简单导数和对称性使其在某些架构中依然是可靠选择。
